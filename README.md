# ECCV-2024-Papers
![Alt text](%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20240319161853.png)
## 官网链接：https://eccv.ecva.net/

### 主会 :bell:：9 月 29 日（周日）至 10 月 4 日

## 历年综述论文分类汇总戳这里↘️[CV-Surveys](https://github.com/52CV/CV-Surveys)施工中~~~~~~~~~~

## 2024 年论文分类汇总戳这里
↘️[WACV-2024-Papers](https://github.com/52CV/WACV-2024-Papers)
↘️[CVPR-2024-Papers](https://github.com/52CV/CVPR-2024-Papers)
↘️[ECCV-2024-Papers](https://github.com/52CV/ECCV-2024-Papers)
## 2023 年论文分类汇总戳这里
↘️[CVPR-2023-Papers](https://github.com/52CV/CVPR-2023-Papers)
↘️[WACV-2023-Papers](https://github.com/52CV/WACV-2023-Papers)
↘️[ICCV-2023-Papers](https://github.com/52CV/ICCV-2023-Papers)

## [2022 年论文分类汇总戳这里](#000)
## [2021 年论文分类汇总戳这里](#00)
## [2020 年论文分类汇总戳这里](#0)

## 💥💥💥
<br>:thumbsup:[ECCV 2024奖项公布，哥大摘得最佳论文奖桂冠](https://mp.weixin.qq.com/s/2uFlMQUW1TVrNOIC01U8Pg)

## 🏆Best Paper Award(最佳论文奖)
* [Minimalist Vision with Freeform Pixels](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08113.pdf)<br>:house:[project](https://cave.cs.columbia.edu/projects/categories/project?cid=Computational+Imaging&pid=Minimalist+Vision+with+Freeform+Pixels)

## 🏅Best Paper Honorable Mention(最佳论文荣誉提名奖)
* [Rasterized Edge Gradients: Handling Discontinuities Differentiably](https://arxiv.org/abs/2405.02508)
* [Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models](https://arxiv.org/abs/2404.13706)<br>:house:[project](https://cs-people.bu.edu/vpetsiuk/arc/)

11月14日更新 28 篇，共计 3114+28 篇。
* [AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models](https://arxiv.org/abs/2307.12499)<br>:star:[code](https://github.com/EricDai0/advdiff)
* [Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks](https://arxiv.org/abs/2402.17976)
* [ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images](https://arxiv.org/abs/2403.10004)图像编辑
* [DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors](https://arxiv.org/abs/2310.12190)<br>:star:[code](https://github.com/Doubiiu/DynamiCrafter)
* [Text-Anchored Score Composition: Tackling Condition Misalignment in Text-to-Image Diffusion Models](https://arxiv.org/abs/2306.14408)<br>:star:[code](https://github.com/EnVision-Research/Decompose-and-Realign)
* [ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06174.pdf)<br>:house:[project](https://gen-l-2.github.io/)
* [Restore Anything with Masks: Leveraging Mask Image Modeling for Blind All-in-One Image Restoration](https://arxiv.org/abs/2409.19403v1)<br>:star:[code](https://github.com/Dragonisss/RAM)
* [When Fast Fourier Transform Meets Transformer for Image Restoration](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06190.pdf)<br>:star:[code](https://github.com/deng-ai-lab/SFHformer)
* [Motion Aware Event Representation-driven Image Deblurring](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06299.pdf)(https://github.com/ZhijingS/DA_event_deblur)
* [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)<br>:house:[project](https://vlm-driver.github.io/)
* [Personalized Federated Domain-Incremental Learning based on Adaptive Knowledge Matching](https://arxiv.org/abs/2407.05005)
* [Placing Objects in Context via Inpainting for Out-of-distribution Segmentation](https://arxiv.org/abs/2402.16392)<br>:star:[code](https://github.com/naver/poc)
* [Rethinking and Improving Visual Prompt Selection for In-Context Learning Segmentation Framework](https://arxiv.org/abs/2407.10233v1)<br>:star:[code](https://github.com/LanqingL/SCS)
* [Region-Adaptive Transform with Segmentation Prior for Image Compression](https://arxiv.org/abs/2403.00628)<br>:star:[code](https://github.com/GityuxiLiu/SegPIC-for-Image-Compression)
* [Teddy: Efficient Large-Scale Dataset Distillation via Taylor-Approximated Matching](https://arxiv.org/abs/2410.07579)<br>:star:[code](https://github.com/Lexie-YU/Teddy)
* [X-Pose: Detecting Any Keypoints](https://arxiv.org/abs/2310.08530)<br>:star:[code](https://github.com/IDEA-Research/X-Pose)
* [MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering](https://arxiv.org/abs/2403.18820)<br>:star:[code](https://github.com/sunshinnnn/metacap)
* [DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction](https://arxiv.org/abs/2312.03298)<br>:star:[code](https://github.com/TyraelDLee/DiffPMAE)
* [Rethinking Weakly-supervised Video Temporal Grounding From a Game Perspective](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06164.pdf)
* [RCS-Prompt: Learning Prompt to Rearrange Class Space for Prompt-based Continual Learning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06307.pdf)<br>:star:[code](https://github.com/longrongyang/RCS-Prompt)
* [UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection](https://arxiv.org/abs/2404.04933)<br>:star:[code](https://github.com/yingsen1/UniMD)
* [M2Depth: Self-supervised Two-Frame Multi-camera Metric Depth Estimation](https://arxiv.org/abs/2405.02004)<br>:house:[project](https://heiheishuang.xyz/M2Depth)
* [WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language](https://arxiv.org/abs/2304.05645)<br>:star:[code](https://github.com/4DVLab/WildRefer)
* [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)<br>:star:[code](https://github.com/IDEA-Research/GroundingDINO)
* [OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation](https://arxiv.org/abs/2403.19580)<br>:star:[code](https://github.com/zhenyuw16/Uni3DETR)
* [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043)<br>:star:[code](https://github.com/dvlab-research/LLaMA-VID)
* [Textual Grounding for Open-vocabulary Visual Information Extraction in Layout-diversified Documents](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06219.pdf)
* [Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset](https://arxiv.org/abs/2407.12330)<br>:star:[code](https://github.com/mijoo308/Energy-Calibration)

<br>:sunflower:[dataset]
<br>:star:[code]()<br>:house:[project]()
<br>:thumbsup:<br>🤗[huggingface]
ASDF


## 全家桶
* [X-InstructBLIP: A Framework for Aligning Image, 3D, Audio, Video to LLMs and its Emergent Cross-modal Reasoning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06140.pdf)<br>:star:[code](https://github.com/salesforce/LAVIS/tree/main/projects/xinstructbl)

## Dense Prediction
* [Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild](https://arxiv.org/abs/2404.18459)(https://github.com/GitGyun/chameleon)密集视觉预测
* [Unsupervised Dense Prediction using Differentiable Normalized Cuts](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05675.pdf)
* [Three Things We Need to Know About Transferring Stable Diffusion to Visual Dense Prediciton Tasks](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05837.pdf)

## Information Security(信息安全)
* 图像水印
  * [Certifiably Robust Image Watermark](http://arxiv.org/abs/2407.04086v1)<br>:star:[code](https://github.com/zhengyuan-jiang/Watermark-Library)
  * [A Secure Image Watermarking Framework with Statistical Guarantees via Adversarial Attacks on Secret Key Networks](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05695.pdf)图像水印

## Deepfake Detection
* [Enhancing Tampered Text Detection through Frequency Feature Fusion and Decomposition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04834.pdf)<br>:thumbsup:[文档图像篡改检测 (DITD) 方法——特征融合与分解网络 (FFDN)](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
* [Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities](http://arxiv.org/abs/2407.20337v1)<br>:star:[code](https://github.com/aimagelab/CoDE)
* [Fake It till You Make It: Curricular Dynamic Forgery Augmentations towards General Deepfake Detection](http://arxiv.org/abs/2409.14444v1)
* 图像伪造检测和定位
  * [Noise-assisted Prompt Learning for Image Forgery Detection and Localization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01688.pdf)
  * [AdaIFL: Adaptive Image Forgery Localization via a Dynamic and Importance-aware Transformer Network](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06023.pdf)<br>:star:[code](https://github.com/LMIAPC/AdaIFL)

## Keypoint Detection(关键点检测)
* [OpenKD: Opening Prompt Diversity for Zero- and Few-shot Keypoint Detection](https://arxiv.org/abs/2409.19899)<br>:star:[code](https://github.com/AlanLuSun/OpenKD)

## Visual Entity Recognition(视觉实体识别)
* [Grounding Language Models for Visual Entity Recognition](https://arxiv.org/abs/2402.18695)视觉实体识别

## Feature Matching 
* [Raising the Ceiling: Conflict-Free Local Feature Matching with Dynamic View Switching](http://arxiv.org/abs/2407.07789v1)
* 图像匹配
  * [CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint Matching](https://arxiv.org/abs/2404.16972)<br>:star:[code](https://github.com/Samia067/CriSp)

## Sketches(草图)
* [Do Generalised Classifiers really work on Human Drawn Sketches?](http://arxiv.org/abs/2407.03893v1)

## Light-Field(光场)
* [Deep Polarization Cues for Single-shot Shape and Subsurface Scattering Estimation](http://arxiv.org/abs/2407.08149v1)
* 相机重定位
  * [Differentiable Product Quantization for Memory Efficient Camera Relocalization](http://arxiv.org/abs/2407.15540v1)

## Computer Graphics(计算机图形学)
* 高动态范围成像
  * [SAFNet: Selective Alignment Fusion Network for Efficient HDR Imaging](http://arxiv.org/abs/2407.16308v1)<br>:star:[code](https://github.com/ltkong218/SAFNet)

## Animal
* [Ponymation: Learning Articulated 3D Animal Motions from Unlabeled Online Videos](https://arxiv.org/abs/2312.13604)<br>:house:[project](https://keqiangsun.github.io/projects/ponymation)3D动物运动
* [Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification](https://arxiv.org/abs/2410.06977)<br>:star:[code](https://github.com/JigglypuffStitch/AdaFreq.git)

## Rendering(渲染)
* [City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web](https://arxiv.org/abs/2312.16457)<br>:star:[code](https://github.com/USTC3DV/MERFStudio)<br>:house:[project](https://ustc3dv.github.io/City-on-Web/)
* [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05407.pdf)<br>:house:[project](https://merl.com/research/highlights/ps-neus)渲染
* [TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering](https://arxiv.org/abs/2311.16465)<br>:house:[project](https://aka.ms/textdiffuser-2)
* [AnyLens: A Generative Diffusion Model with Any Rendering Lens](https://arxiv.org/abs/2311.17609)(https://anylens-diffusion.github.io/)
* [CityGaussian: Real-time High-quality Large-Scale Scene Rendering with Gaussians](http://arxiv.org/abs/2404.01133)<br>:star:[code](https://github.com/DekuLiuTesla/CityGaussian)<br>:house:[project](https://dekuliutesla.github.io/citygs/)
* [METACAP: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering](https://arxiv.org/pdf/2403.18820.pdf)<br>:house:[project](https://vcai.mpi-inf.mpg.de/projects/MetaCap/)
* [GAURA: Generalizable Approach for Unified Restoration and Rendering of Arbitrary Views](http://arxiv.org/abs/2407.08221v1)
* [MaRINeR: Enhancing Novel Views by Matching Rendered Images with Nearby References](http://arxiv.org/abs/2407.13745v1)<br>:star:[code](https://boelukas.github.io/mariner/)
* [Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors](http://arxiv.org/abs/2407.16396v1)<br>:star:[code](https://wen-yuan-zhang.github.io/VolumeRenderingPriors/)
* [CaesarNeRF: Calibrated Semantic Representation for Few-Shot Generalizable Neural Rendering](https://arxiv.org/abs/2311.15510)<br>:house:[project](https://haidongz-usc.github.io/project/caesarnerf)
* [VersatileGaussian: Real-time Neural Rendering for Versatile Tasks using Gaussian Splatting](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03032.pdf)神经渲染
* [UniVoxel: Fast Inverse Rendering by Unified Voxelization of Scene Representation](http://arxiv.org/abs/2407.19542v1)<br>:star:[code](https://github.com/freemantom/UniVoxel)
* [Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering](http://arxiv.org/abs/2408.09702v1)<br>:house:[project](https://research.nvidia.com/labs/toronto-ai/DiPIR/)
* [GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering](https://arxiv.org/abs/2403.11324)<br>:star:[code](https://github.com/yanyan-li/GeoGaussian)场景渲染

## Neural Radiance Fields
* [Invertible Neural Warp for NeRF](http://arxiv.org/abs/2407.12354v1)<br>:star:[code](https://sfchng.github.io/ineurowarping-github.io/)
* [Regularizing Dynamic Radiance Fields with Kinematic Fields](http://arxiv.org/abs/2407.14059v1)
* [KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter](http://arxiv.org/abs/2407.13185v1)<br>:star:[code](https://github.com/Yifever20002/KFD-NeRF)
* [Dynamic Neural Radiance Field From Defocused Monocular Video](http://arxiv.org/abs/2407.05586v1)
* [Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering](https://arxiv.org/abs/2409.05867)<br>:house:[project](https://benattal.github.io/flash-cache/)
* [Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model](http://arxiv.org/abs/2407.07735v1)<br>:house:[project](https://qsong2001.github.io/NeRFProtector)
* [GeometrySticker: Enabling Ownership Claim of Recolorized Neural Radiance Fields](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01453.pdf)<br>:star:[code](https://github.com/kevinhuangxf/GeometrySticker)<br>:house:[project](https://kevinhuangxf.github.io/GeometrySticker/)
* [Efficient NeRF Optimization - Not All Samples Remain Equally Hard](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05300.pdf)
* [MeshFeat: Multi-Resolution Features for Neural Fields on Meshes](http://arxiv.org/abs/2407.13592v1)<br>:house:[project](https://maharajamihir.github.io/MeshFeat/)
* [Acquisition of Spatially-Varying Reflectance and Surface Normals via Polarized Reflectance Fields]<br>:house:[project](https://jingyangcarl.github.io/PolarizedReflectanceField/)
* [DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced Images](https://arxiv.org/abs/2403.13199)<br>:house:[project](https://zaidtas.github.io/decentnerfs/index.html)
* [TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature Tracks](http://arxiv.org/abs/2408.10739v1)<br>:star:[code](https://tracknerf.github.io/)
* [BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event Stream](http://arxiv.org/abs/2407.02174v1)<br>:star:[code](https://github.com/WU-CVGL/BeNeRF)
* [TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation](https://arxiv.org/abs/2401.06191)<br>:house:[project](https://rajaeekh.github.io/trinerflet-web)
* [RS-NeRF: Neural Radiance Fields from Rolling Shutter Images](http://arxiv.org/abs/2407.10267v1)<br>:star:[code](https://github.com/MyNiuuu/RS-NeRF)
* [Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling](http://arxiv.org/abs/2407.11962v1)<br>:star:[code](https://github.com/stevejaehyeok/MoCo-NeRF)<br>:house:[project](https://stevejaehyeok.github.io/publications/moco-nerf)
* [BeNeRF:Neural Radiance Fields from a Single Blurry Image and Event Stream](https://arxiv.org/abs/2407.02174)<br>:star:[code](https://github.com/wu-cvgl/BeNeRF)
* [Taming Latent Diffusion Model for Neural Radiance Field Inpainting](https://arxiv.org/abs/2404.09995)<br>:house:[project](https://hubert0527.github.io/MALD-NeRF)
* [Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation](https://arxiv.org/abs/2403.19319)<br>:house:[project](https://terencecyj.github.io/projects/Mesh2NeRF/)<br>🤗[huggingface](https://huggingface.co/papers/2403.19319)
* [SlotLifter: Slot-guided Feature Lifting for Learning Object-Centric Radiance Fields](https://www.arxiv.org/abs/2408.06697)<br>:house:[project](https://slotlifter.github.io/)
* [FisherRF: Active View Selection and Mapping with Radiance Fields using Fisher Information](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02130.pdf)<br>:star:[code](https://github.com/JiangWenPL/FisherRF)
* [Content-Aware Radiance Fields: Aligning Model Complexity with Scene Intricacy Through Learned Bitwidth Quantization](https://arxiv.org/abs/2410.19483)<br>:star:[code](https://github.com/WeihangLiu2024/Content_Aware_NeRF)
* [Physically Plausible Color Correction for Neural Radiance Fields](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06042.pdf)
* [Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions](https://arxiv.org/abs/2403.14053)NeRF
* [PointNeRF++: A multi-scale, point-based Neural Radiance Field](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05521.pdf)<br>:house:[project](https://pointnerfpp.github.io/)
* [Omni-Recon: Harnessing Image-based Rendering for General-Purpose Neural Radiance Fields](https://arxiv.org/abs/2403.11131)
* [High-Fidelity and Transferable NeRF Editing by Frequency Decomposition](https://arxiv.org/abs/2404.02514)<br>:house:[project](https://aigc3d.github.io/freditor)
* [Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction](https://arxiv.org/abs/2305.15171)<br>:house:[project](https://xinhangliu.com/deceptive-nerf-3dgs)
* 新视图合成
  * [Fast View Synthesis of Casual Videos](https://arxiv.org/abs/2312.02135)<br>:house:[project](https://casual-fvs.github.io/)
  * [PolyOculus: Simultaneous Multi-view Image-based Novel View Synthesis](https://arxiv.org/abs/2402.17986)
  * [RING-NeRF : Rethinking Inductive Biases for Versatile and Efficient Neural Fields](https://arxiv.org/abs/2312.03357)
  * [Structured-NeRF: Hierarchical Scene Graph with Neural Representation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05154.pdf)
  * [URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields](https://arxiv.org/abs/2403.10119)
  * [A Compact Dynamic 3D Gaussian Representation for Real-Time Dynamic View Synthesis](https://arxiv.org/abs/2311.12897)<br>:star:[code](https://github.com/raven38/EfficientDynamic3DGaussian/)<br>:house:[project](https://compactdynamic3dgaussian.github.io/)
  * [High-Resolution and Few-shot View Synthesis from Asymmetric Dual-lens Inputs](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00368.pdf)<br>:star:[code](https://github.com/XrKang/DL-GS)
  * [FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting](https://arxiv.org/abs/2312.00451)<br>:star:[code](https://github.com/VITA-Group/FSGS)
  * [Fast View Synthesis of Casual Videos with Soup-of-Planes](https://arxiv.org/abs/2312.02135)<br>:house:[project](https://casual-fvs.github.io/)
  * [CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians](https://arxiv.org/abs/2403.19495)<br>:house:[project](https://people.engr.tamu.edu/nimak/Papers/CoherentGS)
  * [MegaScenes: Scene-Level View Synthesis at Scale](https://arxiv.org/abs/2406.11819)<br>:star:[code](https://github.com/MegaScenes/nvs)
  * [Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis](https://arxiv.org/abs/2403.04116)<br>:star:[code](https://github.com/caiyuanhao1998/X-Gaussian)视图合成
  * [NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for Real-Time Novel View Synthesis](http://arxiv.org/abs/2407.10482v1)
  * [Efficient Depth-Guided Urban View Synthesis](http://arxiv.org/abs/2407.12395v1)<br>:star:[code](https://xdimlab.github.io/EDUS/)
  * [Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis](https://arxiv.org/abs/2405.14868)<br>:star:[code](https://github.com/basilevh/gcd)
  * [Generalizable Human Gaussians for Sparse View Synthesis](https://arxiv.org/abs/2407.12777)<br>:house:[project](https://humansensinglab.github.io/Generalizable-Human-Gaussians/)
  * [Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis](http://arxiv.org/abs/2409.08042v1)<br>:star:[code](https://github.com/mzzcdf/Thermal3DGS)

## Dataset/Benchmark(数据集/基准)
* [FYI: Flip Your Images for Dataset Distillation](http://arxiv.org/abs/2407.08113v1)
* [Neural Spectral Decomposition for Dataset Distillation](http://arxiv.org/abs/2408.16236v1)<br>:star:[code](https://github.com/slyang2021/NSD)
* [Teddy: Efficient Large-Scale Dataset Distillation via Taylor-Approximated Matching](https://arxiv.org/abs/2410.07579)<br>:star:[code](https://github.com/Lexie-YU/Teddy)
* [Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient Dataset Distillation](https://arxiv.org/abs/2305.18381)<br>:star:[code](https://github.com/silicx/GoldFromOres-BiLP)
* [COM Kitchens: An Unedited Overhead-view Video Dataset as a Vision-Language Benchmark](https://arxiv.org/abs/2408.02272)
* 基准
  * [MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models](https://arxiv.org/abs/2311.17600)<br>:star:[code](https://github.com/isXinLiu/MM-SafetyBench)
  * [DailyDVS-200: A Comprehensive Benchmark Dataset for Event-Based Action Recognition](http://arxiv.org/abs/2407.05106v1)<br>:star:[code](https://github.com/QiWang233/DailyDVS-200)
  * [Urban Waterlogging Detection: A Challenging Benchmark and Large-Small Model Co-Adapter](http://arxiv.org/abs/2407.08109v1)<br>:star:[code](https://github.com/zhang-chenxu/LSM-Adapter)
  * [MSD: A Benchmark Dataset for Floor Plan Generation of Building Complexes](http://arxiv.org/abs/2407.10121v1)
  * [HyTAS: A Hyperspectral Image Transformer Architecture Search Benchmark and Analysis](http://arxiv.org/abs/2407.16269v1)
  * [OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding](https://arxiv.org/abs/2406.07471)<br>:house:[project](https://minghu0830.github.io/OphNet-benchmark/)
  * [Cross-Platform Video Person ReID: A New Benchmark Dataset and Adaptation Approach](https://arxiv.org/abs/2408.07500)<br>:star:[code](https://github.com/FHR-L/VSLA-CLIP)
  * [R^2-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations](https://arxiv.org/abs/2403.04924)<br>:star:[code](https://github.com/lxa9867/r2bench)
  * [m&m’s: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01486.pdf)<br>:star:[code](https://github.com/RAIVNLab/mms)<br>🤗[huggingface](https://huggingface.co/datasets/zixianma/mms)
  * [LayeredFlow: A Real-World Benchmark for Non-Lambertian Multi-Layer Optical Flow](http://arxiv.org/abs/2409.05688v1)<br>:house:[project](https://layeredflow.cs.princeton.edu)
  * [HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects](http://arxiv.org/abs/2407.12371v1)<br>:star:[code](https://lvxintao.github.io/himo)
  * [When Pedestrian Detection Meets Multi-Modal Learning: Generalist Model and Benchmark Dataset](http://arxiv.org/abs/2407.10125v1)<br>:star:[code](https://github.com/BubblyYi/MMPedestron)
* 数据集
  * [VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models](https://arxiv.org/abs/2311.17404)<br>:star:[code](https://github.com/lscpku/VITATECS)
  * [OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web](https://arxiv.org/abs/2402.17553)
  * [SemTrack: A Large-scale Dataset for Semantic Tracking in the Wild](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03555.pdf)<br>:sunflower:[dataset](https://github.com/sutdcv/SemTrack)
  * [WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing](https://arxiv.org/abs/2402.09430)<br>:star:[code](https://github.com/huangshk/WiMANS)
  * [BugNIST - a Large Volumetric Dataset for Detection under Domain Shift](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04613.pdf)
  * [Defect Spectrum: A Granular Look of Large-scale Defect Datasets with Rich Semantics](https://arxiv.org/abs/2310.17316)<br>:star:[code](https://github.com/EnVision-Research/Defect_Spectrum)<br>:house:[project](https://envision-research.github.io/Defect_Spectrum/)大规模缺陷数据集
  * [Raindrop Clarity: A Dual-Focused Dataset for Day and Night Raindrop Removal](http://arxiv.org/abs/2407.16957v1)<br>:star:[code](https://github.com/jinyeying/RaindropClarity)
  * [PartImageNet++ Dataset: Scaling up Part-based Models for Robust Recognition](http://arxiv.org/abs/2407.10918v1)
  * [WTS: A Pedestrian-Centric Traffic Video Dataset for Fine-grained Spatial-Temporal Understanding](http://arxiv.org/abs/2407.15350v1)<br>:star:[code](https://woven-visionai.github.io/wts-dataset-homepage/)
  * [H-V2X: A Large Scale Highway Dataset for BEV Perception](https://eccv.ecva.net/virtual/2024/poster/126)
  * [PetFace: A Large-Scale Dataset and Benchmark for Animal Identification](http://arxiv.org/abs/2407.13555v1)<br>:star:[code](https://dahlian00.github.io/PetFacePage/)
  * [Long-range Turbulence Mitigation: A Large-scale Dataset and A Coarse-to-fine Framework](http://arxiv.org/abs/2407.08377v1)
  * [OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects](http://arxiv.org/abs/2407.08711v1)<br>:star:[code](https://omninocs.github.io)
  * [SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark](https://arxiv.org/abs/2310.20436)<br>:star:[code](https://github.com/ZhengdiYu/SignAvatars)<br>:house:[project](https://signavatars.github.io/)
  * [Insect Identification in the Wild: The AMI Dataset](https://arxiv.org/abs/2406.12452)<br>:star:[code](https://github.com/RolnickLab/ami-dataset)野外昆虫识别：AMI 数据集
  * [RoScenes: A Large-scale Multi-view 3D Dataset for Roadside Perception](https://arxiv.org/abs/2405.09883)<br>:sunflower:[dataset](https://github.com/xiaosu-zhu/RoScenes)
* 数据增强
  * [SUMix: Mixup with Semantic and Uncertain Information](http://arxiv.org/abs/2407.07805v1)<br>:star:[code](https://github.com/JinXins/SUMix)
  * [Data Augmentation via Latent Diffusion for Saliency Prediction](http://arxiv.org/abs/2409.07307v1)
  * [FreeAugment: Data Augmentation Search Across All Degrees of Freedom](http://arxiv.org/abs/2409.04820v1)<br>:star:[code](https://tombekor.github.io/FreeAugment-web)

## Sound
* [Audio-Synchronized Visual Animation](https://arxiv.org/abs/2403.05659)<br>:star:[code](https://github.com/lzhangbj/ASVA)<br>:house:[project](https://lzhangbj.github.io/projects/asva/asva.html)
* [Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation](https://arxiv.org/pdf/2305.03907.pdf)<br>:house:[project](https://bolinlai.github.io/CSTS-EgoGazeAnticipation/)
* [Label-anticipated Event Disentanglement for Audio-Visual Video Parsing](http://arxiv.org/abs/2407.08126v1)
* [Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity](http://arxiv.org/abs/2407.10387v1)<br>:star:[code](https://maskvat.github.io)
* [Spherical World-Locking for Audio-Visual Localization in Egocentric Videos](https://arxiv.org/abs/2408.05364)
* [Self-Supervised Audio-Visual Soundscape Stylization](http://arxiv.org/abs/2409.14340v1)<br>:house:[project](https://tinglok.netlify.app/files/avsoundscape/)
* [CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios](https://arxiv.org/abs/2403.04640)<br>:star:[code](https://github.com/rikeilong/Bay-CAT)视听场景
* [Siamese Vision Transformers are Scalable Audio-visual Learners](https://arxiv.org/abs/2403.19638)<br>:star:[code](https://github.com/GenjiB/AVSiam)视听学习器
* 视听分割
  * [Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes](http://arxiv.org/abs/2407.10957v1)<br>:star:[code](https://gewu-lab.github.io/Ref-AVS)
  * [Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation](http://arxiv.org/abs/2407.11820v1)<br>:star:[code](https://gewu-lab.github.io/stepping_stones)<br>:star:[code](https://gewu-lab.github.io/stepping_stones/)
  * [CPM: Class-conditional Prompting Machine for Audio-visual Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01634.pdf)视听分割

bs/2408.16426v1)

## Optical Flow Estimation(光流估计)
* [SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow](https://arxiv.org/abs/2405.14793)<br>:star:[code](https://github.com/princeton-vl/SEA-RAFT)

## Biomedical(生物特征识别)
* [Open-Set Biometrics: Beyond Good Closed-Set Models](http://arxiv.org/abs/2407.16133v1)<br>:star:[code](https://github.com/prevso1088/open-set-biometrics)

## Object Pose Estimation(物体姿态估计)
* [SCAPE: A Simple and Strong Category-Agnostic Pose Estimator](http://arxiv.org/abs/2407.13483v1)<br>:star:[code](https://github.com/tiny-smart/SCAPE)
* [SRPose: Two-view Relative Pose Estimation with Sparse Keypoints](http://arxiv.org/abs/2407.08199v1)
* [FAFA: Frequency-Aware Flow-Aided Self-Supervision for Underwater Object Pose Estimation](http://arxiv.org/abs/2409.16600v1)
* [GS-Pose: Category-Level Object Pose Estimation via Geometric and Semantic Correspondence](https://arxiv.org/abs/2311.13777)
* [OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547v1)
* [FoundPose: Unseen Object Pose Estimation with Foundation Features](https://arxiv.org/abs/2311.18809)<br>:house:[project](http://evinpinar.github.io/foundpose)
* [LaPose: Laplacian Mixture Shape Modeling for RGB-Based Category-Level Object Pose Estimation](http://arxiv.org/abs/2409.15727v1)<br>:star:[code](https://github.com/lolrudy/LaPose)
* [U-COPE: Taking a Further Step to Universal 9D Category-level Object Pose Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01566.pdf)
* 6-DoF
  * [An Economic Framework for 6-DoF Grasp Detection](http://arxiv.org/abs/2407.08366v1)<br>:star:[code](https://github.com/iSEE-Laboratory/EconomicGrasp)
  * [Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance](http://arxiv.org/abs/2407.13842v1)<br>:star:[code](https://airvlab.github.io/grasp-anything)
  * [Omni6D: Large-Vocabulary 3D Object Dataset for Category-Level 6D Object Pose Estimation](https://arxiv.org/abs/2409.18261)<br>:star:[code](https://github.com/3dtopia/omni6d)
  * [6DGS: 6D Pose Estimation from a Single Image and a 3D Gaussian Splatting Model](http://arxiv.org/abs/2407.15484v1)<br>:star:[code](https://mbortolon97.github.io/6dgs/)
* 相机姿态估计
  * [ADen: Adaptive Density Representations for Sparse-view Camera Pose Estimation](http://arxiv.org/abs/2408.09042v1)
* 计数
  * [AFreeCA: Annotation-Free Counting for All](https://arxiv.org/abs/2403.04943)计数
  * [Zero-shot Object Counting with Good Exemplars](https://arxiv.org/abs/2407.04948)
  * [ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting](https://arxiv.org/abs/2309.04820)<br>:star:[code](https://github.com/ActiveVisionLab/ABC123)<br>:house:[project](https://abc123.active.vision/)计数
  * [Shifted Autoencoders for Point Annotation Restoration in Object Counting](https://arxiv.org/abs/2312.07190)

## Human–Computer Interaction(人机交互)
* [Look Hear: Gaze Prediction for Speech-directed Human Attention](http://arxiv.org/abs/2407.19605v1)<br>:star:[code](https://github.com/cvlab-stonybrook/ART)
* [Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model](http://arxiv.org/abs/2408.01044v1)

## Robots(机器人)
* [See and Think: Embodied Agent in Virtual Environment](https://arxiv.org/abs/2311.15209)<br>:house:[project](https://rese1f.github.io/STEVE/)
* [SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs](https://arxiv.org/abs/2404.00469)
* [V-IRL: Grounding Virtual Intelligence in Real Life](https://arxiv.org/abs/2402.03310)<br>:star:[code](https://github.com/VIRL-Platform/VIRL)
* 机器人
  * [Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation](https://arxiv.org/abs/2401.07487)<br>:house:[project](https://tea-lab.github.io/Robo-ABC/)
  * [Learning Cross-hand Policies of High-DOF Reaching and Grasping](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04377.pdf)机器人
  * [DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control](http://arxiv.org/abs/2407.14758v1)<br>:star:[code](https://github.com/AllenXuuu/DISCO)
  * [ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation](https://arxiv.org/abs/2403.08321)<br>:star:[code](https://github.com/GuanxingLu/ManiGaussian)<br>:house:[project](https://guanxinglu.github.io/ManiGaussian/)
  * [Adapt2Reward: Adapting Video-Language Models to Generalizable Robotic Rewards via Failure Prompts](http://arxiv.org/abs/2407.14872v1)
  * [GraspXL: Generating Grasping Motions for Diverse Objects at Scale](https://arxiv.org/pdf/2403.19649.pdf)<br>:star:[code](https://github.com/zdchan/graspxl)<br>:house:[project](https://eth-ait.github.io/graspxl/)
  * [Decomposed Vector-Quantized Variational Autoencoder for Human Grasp Generation](http://arxiv.org/abs/2407.14062v1)<br>:star:[code](https://github.com/florasion/D-VQVAE)
* 导航
  * [NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models](http://arxiv.org/abs/2407.12366v1)<br>:star:[code](https://github.com/GengzeZhou/NavGPT-2) 
  * [Prioritized Semantic Learning for Zero-shot Instance Navigation](https://arxiv.org/abs/2403.11650)<br>:star:[code](https://github.com/XinyuSun/PSL-InstanceNav)导航 
* VPR
  * [Close, But Not There: Boosting Geographic Distance Sensitivity in Visual Place Recognition](https://arxiv.org/abs/2407.02422)<br>:star:[code](https://github.com/serizba/cliquemining)
  * [Navigation Instruction Generation with BEV Perception and Large Language Models](http://arxiv.org/abs/2407.15087v1)<br>:star:[code](https://github.com/FanScy/BEVInstructor)
  * [Revisit Anything: Visual Place Recognition via Image Segment Retrieval](http://arxiv.org/abs/2409.18049v1)<br>:star:[code](https://github.com/AnyLoc/Revisit-Anything)
  * [VLAD-BuFF: Burst-aware Fast Feature Aggregation for Visual Place Recognition](https://arxiv.org/abs/2409.19293)<br>:star:[code](https://github.com/Ahmedest61/VLAD-BuFF/)
* SLAM
  * [Deep Patch Visual SLAM](https://arxiv.org/abs/2408.01654)<br>:star:[code](https://github.com/princeton-vl/DPVO)
  * [RGBD GS-ICP SLAM](https://arxiv.org/abs/2403.12550)<br>:star:[code](https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM)
  * [I2-SLAM: Inverting Imaging Process for Robust Photorealistic Dense SLAM](https://arxiv.org/abs/2407.11347)
  * [Hyperion - A fast, versatile symbolic Gaussian Belief Propagation framework for Continuous-Time SLAM](http://arxiv.org/abs/2407.07074v1)<br>:star:[code](https://github.com/VIS4ROB-lab/hyperion)
  * [SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM](https://arxiv.org/abs/2402.03246)
  * [I$^2$-SLAM: Inverting Imaging Process for Robust Photorealistic Dense SLAM](http://arxiv.org/abs/2407.11347v1)
  * [CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field](https://arxiv.org/abs/2403.16095)<br>:star:[code](https://github.com/hjr37/CG-SLAM)
* Try-On
  * [Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of Altered Diffusion Models](https://arxiv.org/abs/2403.07371)
  * [Improving Virtual Try-On with Garment-focused Diffusion Models](http://arxiv.org/abs/2409.08258v1)<br>:star:[code](https://github.com/siqi0905/GarDiff/tree/master)
  * [Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment](https://arxiv.org/abs/2403.12965)<br>:star:[code](https://github.com/mengtingchen/wear-any-way-page)<br>:house:[project](https://mengtingchen.github.io/wear-any-way-page/)
  * [D4-VTON: Dynamic Semantics Disentangling for Differential Diffusion based Virtual Try-On](https://arxiv.org/abs/2407.15111)<br>:star:[code](https://github.com/Jerome-Young/D4-VTON)
  * [WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models](https://arxiv.org/abs/2407.10625)<br>:star:[code](https://github.com/scnuhealthy/video_try_on)
* 交叉地理定位
  * [GAReT: Cross-view Video Geolocalization with Adapters and Auto-Regressive Transformers](http://arxiv.org/abs/2408.02840v1)<br>:star:[code](https://github.com/manupillai308/GAReT)
  * [Cross-view image geo-localization with Panorama-BEV Co-Retrieval Network](https://arxiv.org/abs/2408.05475)<br>:star:[code](https://github.com/yejy53/EP-BEV)
  * [ConGeo: Robust Cross-view Geo-localization across Ground View Variations](https://arxiv.org/abs/2403.13965)<br>:star:[code](https://github.com/eceo-epfl/ConGeo)<br>:house:[project](https://eceo-epfl.github.io/ConGeo/)交叉视角地理定位 
* 地理定位
  * [Statewide Visual Geolocalization in the Wild](https://arxiv.org/abs/2409.16763)<br>:star:[code](https://github.com/fferflo/statewide-visual-geolocalization)
* Avatars(虚拟人)
  * [CanonicalFusion: Generating Drivable 3D Human Avatars from Multiple Images](http://arxiv.org/abs/2407.04345v1)<br>:star:[code](https://github.com/jsshin98/CanonicalFusion)
  * [RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models](http://arxiv.org/abs/2407.06938v1)<br>:star:[code](https://rodinhd.github.io/)
  * [PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations](https://arxiv.org/abs/2404.04421)<br>:house:[project](https://qingqing-zhao.github.io/PhysAvatar)
  * [iHuman: Instant Animatable Digital Humans From Monocular Videos](http://arxiv.org/abs/2407.11174v1)
  * [PAV: Personalized Head Avatar from Unstructured Video Collection](https://arxiv.org/abs/2407.21047)<br>:house:[project](https://akincaliskan3d.github.io/PAV)

## Human-Object Interaction
* [Controllable Human-Object Interaction Synthesis](https://arxiv.org/pdf/2312.03913.pdf)<br>:house:[project](https://lijiaman.github.io/projects/chois/)
* [F-HOI: Toward Fine-grained Semantic-Aligned 3D Human-Object Interactions](http://arxiv.org/abs/2407.12435v1)
* [Interaction-centric Spatio-Temporal Context Reasoning for Multi-Person Video HOI Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04769.pdf)<br>:star:[code](https://github.com/southnx/IcH-Vid-HOI)
* 手-物
  * [NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model](http://arxiv.org/abs/2407.12727v1)
  * [Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics](http://arxiv.org/abs/2409.04033v1)<br>:star:[code](https://hograspnet2024.github.io/)

## Style Transfer(风格迁移)
* 运动迁移
  * [Towards High-Quality 3D Motion Transfer with Realistic Apparel Animation](http://arxiv.org/abs/2407.11266v1)<br>:star:[code](https://github.com/rongakowang/MMDMC)

## Gaze Estimation
* [De-confounded Gaze Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03367.pdf)
* [3DGazeNet: Generalizing Gaze Estimation with Weak Supervision from Synthetic Views](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03191.pdf)<br>:star:[code](https://github.com/eververas/3DGazeNet)
* [Gaze Target Detection Based on Head-Local-Global Coordination](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03933.pdf)

## Action Detection(动作检测)
* [LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning](https://arxiv.org/pdf/2312.03849.pdf)<br>:star:[code](https://github.com/BolinLai/LEGO)<br>:house:[project](https://bolinlai.github.io/Lego_EgoActGen/)
* [ActionSwitch: Class-agnostic Detection of Simultaneous Actions in Streaming Videos](http://arxiv.org/abs/2407.12987v1)
* [Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity Recognition](https://arxiv.org/abs/2403.14113)
* 基于骨架的动作识别
  * [SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by Disentangled Variational Autoencoders](http://arxiv.org/abs/2407.13460v1)<br>:star:[code](https://github.com/pha123661/SA-DVAE)
  * [S-JEPA: A Joint Embedding Predictive Architecture for Skeletal Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04755.pdf)<br>:house:[project](https://sjepa.github.io)
  * [Idempotent Unsupervised Representation Learning for Skeleton-Based Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03717.pdf)<br>:star:[code](https://github.com/LanglandsLin/IGM)
  * [CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner](https://arxiv.org/abs/2403.10082)
* 小样本动作识别
  * [Trajectory-aligned Space-time Tokens for Few-shot Action Recognition](http://arxiv.org/abs/2407.18249v1)<br>:house:[project](https://www.cs.umd.edu/~pulkit/tats)
  * [Efficient Few-Shot Action Recognition via Multi-Level Post-Reasoning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00305.pdf)<br>:star:[code](https://github.com/cong-wu/EMP-Net)
* 时序动作检测
  * [DyFADet: Dynamic Feature Aggregation for Temporal Action Detection](http://arxiv.org/abs/2407.03197v1)<br>:star:[code](https://github.com/yangle15/DyFADet-pytorch)
  * [UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection](https://arxiv.org/abs/2404.04933)<br>:star:[code](https://github.com/yingsen1/UniMD)
* 时序动作定位
  * [HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization](https://arxiv.org/abs/2408.06437)<br>:star:[code](https://github.com/sakibreza/ECCV24-HAT)
  * [Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization](http://arxiv.org/abs/2407.07673v1)
  * [Online Temporal Action Localization with Memory-Augmented Transformer](http://arxiv.org/abs/2408.02957v1)<br>:house:[project](https://cvlab.postech.ac.kr/research/MATR/)
  * [Stepwise Multi-grained Boundary Detector for Point-supervised Temporal Action Localization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01159.pdf)
* 时序动作分割
  * [Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit Adjustment](http://arxiv.org/abs/2408.09919v1)<br>:star:[code](https://github.com/pangzhan27/GTLA)
* 动作质量评估
  * [Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment](http://arxiv.org/abs/2407.19675v1)
  * [RICA^2: Rubric-Informed, Calibrated Assessment of Actions](https://arxiv.org/abs/2408.02138)
  * [Vision-Language Action Knowledge Learning for Semantic-Aware Action Quality Assessment](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05909.pdf)动作质量评估
  * [MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment](https://arxiv.org/abs/2403.04398)<br>:star:[code](https://github.com/ZhouKanglei/MAGR_CAQA)
* 动作预测 
  * [Semantically Guided Representation Learning For Action Anticipation](http://arxiv.org/abs/2407.02309v1)<br>:star:[code](https://github.com/ADiko1997/S-GEAR)
* 动作识别
  * [Referring Atomic Video Action Recognition](https://arxiv.org/abs/2407.01872)<br>:star:[code](https://github.com/KPeng9510/RAVAR)
  * [DEAR: Depth-Enhanced Action Recognition](https://arxiv.org/abs/2408.15679)
  * [Bayesian Evidential Deep Learning for Online Action Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02475.pdf)
  * [C2C: Component-to-Composition Learning for Zero-Shot Compositional Action Recognition](http://arxiv.org/abs/2407.06113v1)<br>:star:[code](https://github.com/RongchangLi/ZSCAR_C2C)
  * [Masked Video and Body-worn IMU Autoencoder for Egocentric Action Recognition](http://arxiv.org/abs/2407.06628v1)
  * [Classification Matters: Improving Video Action Detection with Class-Specific Attention](http://arxiv.org/abs/2407.19698v1)
  * [FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition](http://arxiv.org/abs/2409.01448v1)<br>:house:[project](https://daveishan.github.io/finepsuedo-webpage/)
  * [Multimodal Cross-Domain Few-Shot Learning for Egocentric Action Recognition](https://arxiv.org/abs/2405.19917)<br>:house:[project](https://masashi-hatano.github.io/MM-CDFSL/)
  * [On the Utility of 3D Hand Poses for Action Recognition](https://arxiv.org/abs/2403.09805)<br>:house:[project](https://s-shamil.github.io/HandFormer/)
  * [Occluded Gait Recognition with Mixture of Experts: An Action Detection Perspective](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01016.pdf)<br>:star:[code](https://github.com/BNU-IVC/OccGait)
  * [Leveraging temporal contextualization for video action recognition](https://arxiv.org/abs/2404.09490)<br>:star:[code](https://github.com/naver-ai/tc-clip)
  * [Optimizing Factorized Encoder Models: Time and Memory Reduction for Scalable and Efficient Action Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01635.pdf)
  * [SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition](https://arxiv.org/abs/2403.09508)<br>:house:[project](https://kaist-viclab.github.io/SkateFormer_site/)
* 动作理解  
  * [EgoExo-Fitness: Towards Egocentric and Exocentric Full-Body Action Understanding](https://arxiv.org/abs/2406.08877)<br>:star:[code](https://github.com/iSEE-Laboratory/EgoExo-Fitness/tree/main)

## Visual Question Answering(视觉问答)
* [Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following](https://arxiv.org/abs/2406.02774)
* [WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering](http://arxiv.org/abs/2407.05603v1)<br>:star:[code](https://github.com/cpystan/WSI-VQA)
* [GRACE: Graph-Based Contextual Debiasing for Fair Visual Question Answering](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02569.pdf)
* [Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge](https://arxiv.org/abs/2401.10712)<br>:star:[code](https://github.com/WHB139426/QA-Prompts)
* 音视频问答
  * [Learning Trimodal Relation for AVQA with Missing Modality](http://arxiv.org/abs/2407.16171v1)
* 视频问答
  * [Video Question Answering with Procedural Programs](https://arxiv.org/abs/2312.00937)<br>:house:[project](https://rccchoudhury.github.io/proviq2023/)
  * [TimeCraft: Navigate Weakly-Supervised Temporal Grounded Video Question Answering via Bi-directional Reasoning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00720.pdf)VQA
* 视听问答
  * [Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality](https://arxiv.org/abs/2407.16171)

## Motion Generation(人体运动生成)
* [SMooDi: Stylized Motion Diffusion Model](http://arxiv.org/abs/2407.12783v1)<br>:star:[code](https://neu-vi.github.io/SMooDi/)
* [Length-Aware Motion Synthesis via Latent Diffusion](http://arxiv.org/abs/2407.11532v1)
* [HUMOS: Human Motion Model Conditioned on Body Shape](http://arxiv.org/abs/2409.03944v1)<br>:star:[code](https://CarstenEpic.github.io/humos/)
* [HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance](http://arxiv.org/abs/2407.06937v1)<br>:star:[code](https://github.com/Enderfga/HumanRefiner)
* [Generating Human Interaction Motions in Scenes with Text Control](https://arxiv.org/abs/2404.10685)<br>:house:[project](https://research.nvidia.com/labs/toronto-ai/tesmo/)运动生成
* [Motion Mamba: Efficient and Long Sequence Motion Generation](https://arxiv.org/abs/2403.07487)<br>:star:[code](https://github.com/steve-zeyu-zhang/MotionMamba/)<br>:house:[project](https://steve-zeyu-zhang.github.io/MotionMamba/)
* [Large Motion Model for Unified Multi-Modal Motion Generation](https://arxiv.org/abs/2404.01284)<br>:house:[project](https://mingyuan-zhang.github.io/projects/LMM.html)
* [EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation](https://arxiv.org/abs/2312.02256)<br>:star:[code](https://github.com/Frank-ZY-Dou/EMDM)<br>:house:[project](https://frank-zy-dou.github.io/projects/EMDM/index.html)
* [Bridging the Gap Between Human Motion and Action Semantics via Kinematics Phrases](https://arxiv.org/abs/2310.04189)<br>:house:[project](https://foruck.github.io/KP/)人体运动
* [TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01796.pdf)<br>:house:[project](https://yufu-wang.github.io/tram4d/)人体运动
* [Nymeria: A Massive Collection of Egocentric Multi-modal Human Motion in the Wild](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03541.pdf)人体运动
* [FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models](https://arxiv.org/abs/2406.10740)
* [MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model](https://arxiv.org/abs/2404.19759)<br>:star:[code](https://github.com/Dai-Wenxun/MotionLCM)
* [Realistic Human Motion Generation with Cross-Diffusion Models](https://arxiv.org/abs/2312.10993)<br>:house:[project](https://wonderno.github.io/CrossDiff-webpage/)人体运动
* [CoMo: Controllable Motion Generation through Language Guided Pose Code Editing](https://arxiv.org/abs/2403.13900)<br>:house:[project](https://yh2371.github.io/como/)生成可控运动
* [TLControl: Trajectory and Language Control for Human Motion Synthesis](https://arxiv.org/abs/2311.17135)<br>:house:[project](https://tlcontrol.weilinwl.com/)人体运动合成
* 三维人体运动合成
  * [ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions](https://arxiv.org/pdf/2311.17057.pdf)<br>:house:[project](https://vcai.mpi-inf.mpg.de/projects/remos/)
* 文本-动作合成
  * [FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis](https://arxiv.org/pdf/2405.15763)
  * [Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation](http://arxiv.org/abs/2407.10528v1)<br>:star:[code](https://jpthu17.github.io/GuidedMotion-project/)
  * [Plan, Posture and Go: Towards Open-vocabulary Text-to-Motion Generation](https://arxiv.org/abs/2312.14828)<br>:house:[project](https://moonsliu.github.io/Pro-Motion/)
* 人体运动预测
  * [Human Motion Forecasting in Dynamic Domain Shifts: A Homeostatic Continual Test-time Adaptation Framework](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04599.pdf)人体运动预测
  * [Scene-aware Human Motion Forecasting via Mutual Distance Prediction](https://arxiv.org/abs/2310.00615)
* 人体运动估计
  * [MANIKIN: Biomechanically Accurate Neural Inverse Kinematics for Human Motion Estimation](https://static.siplab.org/papers/eccv2024-manikin.pdf)<br>:house:[project](https://siplab.org/projects/MANIKIN)
* 运动估计
  * [Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation](http://arxiv.org/abs/2407.10802v1)<br>:star:[code](https://github.com/tub-rip/MotionPriorCMax)
  * [COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation](https://arxiv.org/abs/2408.16426)
* 舞蹈生成
  * [Beat-It: Beat-Synchronized Multi-Condition 3D Dance Generation](https://arxiv.org/abs/2407.07554)<br>:house:[project](https://zikaihuangscut.github.io/Beat-It/)
* 行为生成
  * [DIM: Dyadic Interaction Modeling for Social Behavior Generation](https://arxiv.org/abs/2403.09069)<br>:star:[code](https://github.com/Boese0601/Dyadic-Interaction-Modeling)

## Person Re-Identification(人员重识别)
* [Human-in-the-Loop Visual Re-ID for Population Size Estimation](https://arxiv.org/abs/2312.05287)<br>:star:[code](https://github.com/cvl-umass/counting-clusters)
* 行人重识别
  * [Privacy-Preserving Adaptive Re-Identification without Image Transfer](http://arxiv.org/abs/2407.12589v1)
  * VI-ReID
    * [Multi-Memory Matching for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2401.06825)<br>:thumbsup:[无监督可见光-红外行人重识别（USL-VI-ReID）](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
* 人物搜索
  * [PLOT: Text-based Person Search with Part Slot Attention for Corresponding Part Discovery](https://arxiv.org/abs/2409.13475)基于文本的人物搜索
* 步态识别
  * [Camera-LiDAR Cross-modality Gait Recognition](https://arxiv.org/abs/2407.02038)
  * [Free Lunch for Gait Recognition: A Novel Relation Descriptor](https://arxiv.org/abs/2308.11487)
  * [Causality-inspired Discriminative Feature Learning in Triple Domains for Gait Recognition](http://arxiv.org/abs/2407.12519v1)
  * [Cut out the Middleman: Revisiting Pose-based Gait Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04501.pdf)<br>:star:[code](https://github.com/BNU-IVC/FastPoseGait)
* 计数
  * [CountFormer: Multi-View Crowd Counting Transformer](http://arxiv.org/abs/2407.02047v1)
  * [Multi-modal Crowd Counting via a Broker Modality](http://arxiv.org/abs/2407.07518v1)<br>:star:[code](https://github.com/HenryCilence/Broker-Modality-Crowd-Counting)
  * [Improving Point-based Crowd Counting and Localization Based on Auxiliary Point Guidance](https://arxiv.org/abs/2405.10589)<br>:star:[code](https://github.com/AaronCIH/APGCC)

## Point Clouds(点云)
* [SEED: A Simple and Effective 3D DETR in Point Clouds](http://arxiv.org/abs/2407.10749v1)<br>:star:[code](https://github.com/happinesslz/SEED)
* [PointLLM: Empowering Large Language Models to Understand Point Clouds](https://arxiv.org/abs/2308.16911)<br>:star:[code](https://github.com/OpenRobotLab/PointLLM)<br>:house:[project](https://runsenxu.com/projects/PointLLM/)
* [Learning to Adapt SAM for Segmenting Cross-domain Point Clouds](https://arxiv.org/abs/2310.08820)
* [Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time](https://export.arxiv.org/abs/2407.01851)
* [milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing](https://arxiv.org/abs/2306.17010)<br>:star:[code](https://github.com/Toytiny/milliFlow)
* [Fast Point Cloud Geometry Compression with Context-based Residual Coding and INR-based Refinement](http://arxiv.org/abs/2408.02966v1)
* [Learning Local Pattern Modularization for Point Cloud Reconstruction from Unseen Classes](http://arxiv.org/abs/2408.14279v1)<br>:star:[code](https://github.com/chenchao15/Unseen)
* [T-MAE: Temporal Masked Autoencoders for Point Cloud Representation Learning](https://arxiv.org/abs/2312.10217)<br>:star:[code](https://github.com/codename1995/T-MAE)
* [Progressive Classifier and Feature Extractor Adaptation for Unsupervised Domain Adaptation on Point Clouds](https://arxiv.org/abs/2311.16474v2)<br>:star:[code](https://github.com/xiaoyao3302/PCFEA)
* [Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF Decomposition and Ray Tracing](https://arxiv.org/abs/2311.16043)<br>:star:[code](https://github.com/NJU-3DV/Relightable3DGaussian)
* [RangeLDM: Fast Realistic LiDAR Point Cloud Generation](https://arxiv.org/abs/2403.10094)<br>:star:[code](https://github.com/WoodwindHu/RangeLDM)
* 点云完成
  * [Explicitly Guided Information Interaction Network for Cross-modal Point Cloud Completion](http://arxiv.org/abs/2407.02887v1)<br>:star:[code](https://github.com/WHU-USI3DV/EGIInet)
  * [T-CorresNet: Template Guided 3D Point Cloud Completion with Correspondence Pooling Query Generation Strategy](http://arxiv.org/abs/2407.05008v1)<br>:star:[code](https://github.com/df-boy/T-CorresNet)
  * [AEDNet: Adaptive Embedding and Multiview-Aware Disentanglement for Point Cloud Completion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01714.pdf)
  * [EINet: Point Cloud Completion via Extrapolation and Interpolation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05687.pdf)<br>:star:[code](https://github.com/corecai163/EINet)
* 点云重建
  * [DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction](https://arxiv.org/abs/2312.03298)<br>:star:[code](https://github.com/TyraelDLee/DiffPMAE)
* 点云理解
  * [DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding](https://arxiv.org/abs/2407.08801)
* 点云配准
  * [ML-SemReg: Boosting Point Cloud Registration with Multi-level Semantic Consistency](http://arxiv.org/abs/2407.09862v1)<br>:star:[code](https://github.com/Laka-3DV/ML-SemReg)
  * [PointRegGPT: Boosting 3D Point Cloud Registration using Generative Point-Cloud Pairs for Training](http://arxiv.org/abs/2407.14054v1)<br>:star:[code](https://github.com/Chen-Suyi/PointRegGPT)
  * [SemReg: Semantics Constrained Point Cloud Registration](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05759.pdf)<br>:star:[code](https://github.com/SheldonFung98/SemReg.git)
  * [Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning](http://arxiv.org/abs/2407.20223v1)<br>:house:[project](https://sites.google.com/view/eccv24-equivalign)
  * [UMERegRobust -- Universal Manifold Embedding Compatible Features for Robust Point Cloud Registration](http://arxiv.org/abs/2408.12380v1)<br>:star:[code](https://github.com/yuvalH9/UMERegRobust)
  * [Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration](https://arxiv.org/abs/2410.05729)点云配准
* 点云分割
  * [Dual-level Adaptive Self-Labeling for Novel Class Discovery in Point Cloud Segmentation](http://arxiv.org/abs/2407.12489v1)
  * [HGL: Hierarchical Geometry Learning for Test-time Adaptation in 3D Point Cloud Segmentation](http://arxiv.org/abs/2407.12387v1)<br>:star:[code](https://github.com/tpzou/HGL)
  * [SegPoint: Segment Any Point Cloud via Large Language Model](http://arxiv.org/abs/2407.13761v1)<br>:star:[code](https://heshuting555.github.io/SegPoint)
  * [Localization and Expansion: A Decoupled Framework for Point Cloud Few-shot Semantic Segmentation](https://arxiv.org/abs/2408.13752)
  * [Pseudo-Embedding for Generalized Few-Shot Point Cloud Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05346.pdf)<br>:star:[code](https://github.com/jimtsai23/PseudoEmbed)
* 点云理解
  * [GPSFormer: A Global Perception and Local Structure Fitting-based Transformer for Point Cloud Understanding](https://arxiv.org/abs/2407.13519)<br>:star:[code](https://github.com/changshuowang/GPSFormer)
* 3D点云
  * [Implicit Filtering for Learning Neural Signed Distance Functions from 3D Point Clouds](http://arxiv.org/abs/2407.13342v1)<br>:star:[code](https://list17.github.io/ImplicitFilter)
  * [CloudFixer: Test-Time Adaptation for 3D Point Clouds via Diffusion-Guided Geometric Transformation](http://arxiv.org/abs/2407.16193v1)<br>:star:[code](https://github.com/shimazing/CloudFixer)
  * [FLAT: Flux-aware Imperceptible Adversarial Attacks on 3D Point Clouds](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00951.pdf)
  * [RISurConv: Rotation Invariant Surface Attention-Augmented Convolutions for 3D Point Cloud Classification and Segmentation](https://arxiv.org/abs/2408.06110)
  * [P2P-Bridge: Diffusion Bridges for 3D Point Cloud Denoising](http://arxiv.org/abs/2408.16325v1)<br>:star:[code](https://p2p-bridge.github.io)
  * [Heterogeneous Graph Learning for Scene Graph Prediction in 3D Point Clouds](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03785.pdf)
  * [Hiding Imperceptible Noise in Curvature-Aware Patches for 3D Point Cloud Attack](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04444.pdf)3D 点云攻击

## Anomaly Detection(异常检测)
* [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation]
* [GeneralAD: Anomaly Detection Across Domains by Attending to Distorted Features](http://arxiv.org/abs/2407.12427v1)<br>:star:[code](https://github.com/LucStrater/GeneralAD)
* [Learning Diffusion Models for Multi-View Anomaly Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04907.pdf)
* [Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection](https://arxiv.org/abs/2403.13349)<br>:star:[code](https://github.com/xcyao00/HGAD)
* [TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection](https://arxiv.org/abs/2311.09999)<br>:star:[code](https://github.com/MaticFuc/ECCV_TransFusion)
* 缺陷检测
  * [An Incremental Unified Framework for Small Defect Inspection](https://arxiv.org/abs/2312.08917)<br>:star:[code](https://github.com/jqtangust/IUF)
* 故障检测
  * [DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation](http://arxiv.org/abs/2408.00331v1)<br>:star:[code](https://github.com/kowshikthopalli/DECIDER/)
* 3D异常检测
  * [R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection](http://arxiv.org/abs/2407.10862v1)
* 工业异常检测
  * [Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection](https://arxiv.org/abs/2401.03145)
  * [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt]
* 零样本异常检测
  * [AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection](http://arxiv.org/abs/2407.15795v1)<br>:star:[code](https://github.com/caoyunkang/AdaCLIP)
* 多类异常检测
  * [Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection](https://arxiv.org/abs/2403.11561)
* OOD
  * [Gradient-Regularized Out-of-Distribution Detection](https://export.arxiv.org/abs/2404.12368)
  * [PixOOD: Pixel-Level Out-of-Distribution Detection](https://arxiv.org/abs/2405.19882)
  * [Learning Non-Linear Invariants for Unsupervised Out-of-Distribution Detection](http://arxiv.org/abs/2407.04022v1)
  * [LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models](http://arxiv.org/abs/2407.08966v1)<br>:star:[code](https://github.com/YBZh/LAPT)
  * [ProSub: Probabilistic Open-Set Semi-Supervised Learning with Subspace-Based Out-of-Distribution Detection](http://arxiv.org/abs/2407.11735v1)<br>:star:[code](https://github.com/walline/prosub)
  * [Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond](http://arxiv.org/abs/2407.15739v1)<br>:star:[code](https://ade-ood.github.io/)
  * [Can Your Generative Model Detect Out-of-Distribution Covariate Shift?](http://arxiv.org/abs/2409.03043v1)
  * [Gradient-based Out-of-Distribution Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02138.pdf)
* 异常值检测
  * [Rethinking Unsupervised Outlier Detection via Multiple Thresholding](https://arxiv.org/abs/2407.05382)无监督异常值检测

## Semi/self-supervised learning(半/自监督)
* [SweepNet: Unsupervised Learning Shape Abstraction via Neural Sweepers](https://arxiv.org/abs/2407.06305)<br>:house:[project](https://mingrui-zhao.github.io/SweepNet/)
* 自监督
  * [CroMo-Mixup: Augmenting Cross-Model Representations for Continual Self-Supervised Learning](http://arxiv.org/abs/2407.12188v1)<br>:star:[code](https://github.com/ErumMushtaq/CroMo-Mixup)
  * [HPFF: Hierarchical Locally Supervised Learning with Patch Feature Fusion](http://arxiv.org/abs/2407.05638v1)<br>:star:[code](https://github.com/Zeudfish/HPFF)
  * [SCPNet: Unsupervised Cross-modal Homography Estimation via Intra-modal Self-supervised Learning](http://arxiv.org/abs/2407.08148v1)<br>:star:[code](https://github.com/RM-Zhang/SCPNet)
  * [Efficient Unsupervised Visual Representation Learning with Explicit Cluster Balancing](http://arxiv.org/abs/2407.11168v1)
  * [OmniSat: Self-Supervised Modality Fusion for Earth Observation](https://arxiv.org/pdf/2404.08351)<br>:star:[code](https://github.com/gastruc/OmniSat)<br>:house:[project](https://gastruc.github.io/projects/omnisat.html)<br>:sunflower:[dataset](https://huggingface.co/datasets/IGNF/PASTIS-HD)
  * [Decoupling Common and Unique Representations for Multimodal Self-supervised Learning](https://arxiv.org/abs/2309.05300)<br>:star:[code](https://github.com/zhu-xlab/DeCUR)
  * [POA: Pre-training Once for Models of All Sizes](http://arxiv.org/abs/2408.01031v1)<br>:star:[code](https://github.com/Qichuzyy/POA)
  * [ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders](https://arxiv.org/abs/2303.12001)自监督表示学习
  * [Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization](https://arxiv.org/abs/2403.14973)<br>:house:[project](https://pwang.pw/trajSSL/)自监督学习
* 半监督
  * [Image-Feature Weak-to-Strong Consistency: An Enhanced Paradigm for Semi-Supervised Learning](https://arxiv.org/abs/2408.12614)  
  * [Improving 3D Semi-supervised Learning by Effectively Utilizing All Unlabelled Data](http://arxiv.org/abs/2409.13977v1)<br>:star:[code](https://github.com/snehaputul/AllMatch)
  * [SCOMatch: Alleviating Overtrusting in Open-set Semi-supervised Learning](http://arxiv.org/abs/2409.17512v1)
  * [Rebalancing Using Estimated Class Distribution for Imbalanced Semi-Supervised Learning under Class Distribution Mismatch](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03287.pdf)半监督学习
  * [Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning](https://arxiv.org/abs/2407.15837)<br>:star:[code](https://github.com/yibingwei-1/LatentMIM)
 

## Novel Class Discovery(新类发现)
* [Self-Cooperation Knowledge Distillation for Novel Class Discovery](http://arxiv.org/abs/2407.01930v1)

## GNN/GCN
* [GKGNet: Group K-Nearest Neighbor based Graph Convolutional Network for Multi-Label Image Recognition](https://arxiv.org/abs/2308.14378)<br>:star:[code](https://github.com/jin-s13/GKGNet)GNN

## NAS
* [Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00668.pdf)<br>:star:[code](https://github.com/lliai/Auto-GAS)
* [Auto-DAS: Automated Proxy Discovery for Training-free Distillation-aware Architecture Search](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00676.pdf)<br>:star:[code](https://github.com/lliai/Auto-DAS)蒸馏感

## MC/KD/Pruning(模型压缩/知识蒸馏/剪枝)
* [DεpS: Delayed ε-Shrinking for Faster Once-For-All Training](http://arxiv.org/abs/2407.06167v1)
* 剪枝
  * [Straightforward Layer-wise Pruning for More Efficient Visual Adaptation](http://arxiv.org/abs/2407.14330v1)
  * [Isomorphic Pruning for Vision Models](https://arxiv.org/abs/2407.04616)<br>:star:[code](https://github.com/VainF/Isomorphic-Pruning)
  * [PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference](https://arxiv.org/abs/2403.16020)<br>:star:[code](https://github.com/tanvir-utexas/PaPr)剪
* 量化
  * [GenQ: Quantization in Low Data Regimes with Generative Synthetic Data](https://arxiv.org/abs/2312.05272v2)<br>:star:[code](https://github.com/Intelligent-Computing-Lab-Yale/GenQ)
  * [MetaAug: Meta-Data Augmentation for Post-Training Quantization](http://arxiv.org/abs/2407.14726v1)
  * [Toward INT4 Fixed-Point Training via Exploring Quantization Error for Gradients](http://arxiv.org/abs/2407.12637v1)
  * [CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs](http://arxiv.org/abs/2407.05266v1)<br>:star:[code](https://github.com/georgia-tech-synergy-lab/CLAMP-ViT.git)
  * [AdaLog: Post-Training Quantization for Vision Transformers with Adaptive Logarithm Quantizer](http://arxiv.org/abs/2407.12951v1)<br>:star:[code](https://github.com/GoatWu/AdaLog)
  * [POCA: Post-training Quantization with Temporal Alignment for Codec Avatars](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05670.pdf)<br>:house:[project](https://mengjian0502.github.io/poca.github.io/)量化
* KD
  * [Simple Unsupervised Knowledge Distillation With Space Similarity](https://arxiv.org/abs/2409.13939)知识蒸馏
  * [Harmonizing knowledge Transfer in Neural Network with Unified Distillation](https://arxiv.org/abs/2409.18565)
  * [Improving Knowledge Distillation via Regularizing Feature Direction and Norm](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03432.pdf)
  * [Adversarially Robust Distillation by Reducing the Student-Teacher Variance Gap](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00499.pdf)蒸馏
  * [Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation](https://arxiv.org/abs/2407.03056)<br>:star:[code](https://github.com/miccunifi/KDPL)

## Vision Transformer
* [FairViT: Fair Vision Transformer via Adaptive Masking](http://arxiv.org/abs/2407.14799v1)
* [Rotary Position Embedding for Vision Transformer](https://arxiv.org/abs/2403.13298)<br>:star:[code](https://github.com/naver-ai/rope-vit)
* [SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization](https://arxiv.org/abs/2402.03317)
* [PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers](http://arxiv.org/abs/2407.04538v1)
* [OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction](http://arxiv.org/abs/2407.13335v1)
* [AugDETR: Improving Multi-scale Learning for Detection Transformer](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03484.pdf)Transformer
* [AttnZero: Efficient Attention Discovery for Vision Transformers](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00666.pdf)<br>:star:[code](https://github.com/lliai/AttnZero)
* [SpatialFormer: Towards Generalizable Vision Transformers with Explicit Spatial Understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02019.pdf)<br>:star:[code](https://github.com/Euphoria16/SpatialFormer)
* [Stitched ViTs are Flexible Vision Backbones](https://arxiv.org/abs/2307.00154)<br>:star:[code](https://github.com/ziplab/SN-Netv2)
* [Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02429.pdf)
* [Uncertainty-Driven Spectral Compressive Imaging with Spatial-Frequency Transformer](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00861.pdf)<br>:star:[code](https://github.com/bianlab/Specformer)
* [GiT: Towards Generalist Vision Transformer through Universal Language Interface](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04158.pdf)<br>:star:[code](https://github.com/Haiyang-W/GiT)
* [Fairness-aware Vision Transformer via Debiased Self-Attention](https://arxiv.org/abs/2301.13803)<br>:star:[code](https://github.com/qiangyao1988/DSA)
* [ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention](https://arxiv.org/abs/2401.00912)<br>:star:[code](https://github.com/skyhehe123/ScatterFormer)
* [LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors](https://arxiv.org/abs/2403.14625)<br>:house:[project](https://www.cs.umd.edu/~sakshams/LiFT/)
* [Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer: A Disentangled Approach](http://arxiv.org/abs/2407.06964v1)<br>:house:[project](https://synqt.github.io/)
* [LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer](https://arxiv.org/abs/2212.09877)<br>:star:[code](https://github.com/salesforce/LayoutDETR)
* [Efficient Diffusion Transformer with Step-wise Dynamic Attention Mediators](https://arxiv.org/abs/2408.05710)<br>:star:[code](https://github.com/LeapLabTHU/Attention-Mediators)
* [BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal Sentence Grounding in Videos](https://arxiv.org/abs/2312.00083)<br>:star:[code](https://github.com/Pilhyeon/BAM-DETR)
* [An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding](http://arxiv.org/abs/2408.01120v1)<br>:star:[code](https://github.com/chenwei746/EEVG)

## Multimodal Learning(多模态学习)
* [Diagnosing and Re-learning for Balanced Multimodal Learning](http://arxiv.org/abs/2407.09705v1)<br>:star:[code](https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024)
* [Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models](http://arxiv.org/abs/2407.12616v1)
* [HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning](http://arxiv.org/abs/2407.15680v1)<br>:star:[code](https://github.com/google/haloquest)

## (机器学习)
* [Learning to Unlearn for Robust Machine Unlearning](http://arxiv.org/abs/2407.10494v1)
* [Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00022.pdf)<br>:star:[code](https://github.com/yyliu01/IT2)机器学习
* 机器遗忘
  * [MultiDelete for Multimodal Machine Unlearning](https://arxiv.org/abs/2311.12047)<br>:star:[code](https://github.com/CLU-UML/MultiDelete)
  * [Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning](https://arxiv.org/abs/2403.07362)<br>:star:[code](https://github.com/OPTML-Group/Unlearn-WorstCase)
* 对抗
  * [Improving Adversarial Transferability via Model Alignment](https://arxiv.org/abs/2311.18495)
  * [Event Trojan: Asynchronous Event-based Backdoor Attacks](https://arxiv.org/abs/2407.06838)<br>:star:[code](https://github.com/rfww/EventTrojan)
  * [Data Poisoning Quantization Backdoor Attack]
  * [Flatness-aware Sequential Learning Generates Resilient Backdoors]
  * [Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks](http://arxiv.org/abs/2407.20657v1)<br>:star:[code](https://PDCL-Attack.github.io)
  * [Self-Supervised Representation Learning for Adversarial Attack Detection](http://arxiv.org/abs/2407.04382v1)
  * [Prediction Exposes Your Face: Black-box Model Inversion via Prediction Alignment](http://arxiv.org/abs/2407.08127v1)
  * [CLIP-Guided Networks for Transferable Targeted Attacks](http://arxiv.org/abs/2407.10179v1)
  * [CLIP-Guided Generative Networks for Transferable Targeted Adversarial Attacks](https://arxiv.org/abs/2407.10179)
  * [UNIT: Backdoor Mitigation via Automated Neural Distribution Tightening](http://arxiv.org/abs/2407.11372v1)<br>:star:[code](https://github.com/Megum1/UNIT)
  * [Inter-Class Topology Alignment for Efficient Black-Box Substitute Attacks](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05026.pdf)黑盒
  * [Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection](http://arxiv.org/abs/2407.12292v1)<br>:star:[code](https://github.com/VL-Group/GAKer)
  * [AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models](https://arxiv.org/abs/2307.12499)<br>:star:[code](https://github.com/EricDai0/advdiff)
  * [Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks](https://arxiv.org/abs/2402.17976)
* 持续学习
  * [CLEO: Continual Learning of Evolving Ontologies](http://arxiv.org/abs/2407.08411v1)
  * [One-stage Prompt-based Continual Learning](https://arxiv.org/abs/2402.16189)
  * [Exemplar-free Continual Representation Learning via Learnable Drift Compensation](http://arxiv.org/abs/2407.08536v1)<br>:star:[code](https://github.com/alviur/ldc)
  * [Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning](http://arxiv.org/abs/2409.01128v1)<br>:star:[code](https://github.com/jinglin-liang/DDDR)
  * [RCS-Prompt: Learning Prompt to Rearrange Class Space for Prompt-based Continual Learning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06307.pdf)<br>:star:[code](https://github.com/longrongyang/RCS-Prompt)
  * [Revisiting Supervision for Continual Representation Learning](https://arxiv.org/abs/2311.13321)<br>:star:[code](https://github.com/danielm1405/sl-vs-ssl-cl)持续
  * [Anytime Continual Learning for Open Vocabulary Classification](https://arxiv.org/abs/2409.08518)<br>:star:[code](https://github.com/jessemelpolio/AnytimeCL)
  * [MagMax: Leveraging Model Merging for Seamless Continual Learning](http://arxiv.org/abs/2407.06322v1)
  * [Prompt Grouping for Rehearsal-Free Continual Learning]<br>:thumbsup:[在CIFAR-100、ImageNet-R和DomainNet数据集上分别提高了2.31%、3.04%和2.19%的准确率](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [Beyond Prompt Learning: Continual Adapter for Efficient Rehearsal-Free Continual Learning](http://arxiv.org/abs/2407.10281v1)
* 迁移学习
  * [SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning](http://arxiv.org/abs/2407.07523v1)<br>:star:[code](https://github.com/Paranioar/SHERL)
* 主动学习
  * [Dataset Quantization with Active Learning based Adaptive Sampling](http://arxiv.org/abs/2407.07268v1)
  * [Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding](https://arxiv.org/abs/2312.05328)主动学习
  * [Bidirectional Uncertainty-Based Active Learning for Open-Set Annotation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04026.pdf)主动学习
* 强化学习
  * [Reinforcement Learning Meets Visual Odometry](http://arxiv.org/abs/2407.15626v1)
  * [Multimodal Label Relevance Ranking via Reinforcement Learning](http://arxiv.org/abs/2407.13221v1)<br>:star:[code](https://github.com/ChazzyGordon/LR2PPO)
  * [Enhancing Diffusion Models with Text-Encoder Reinforcement Learning](https://arxiv.org/abs/2311.15657)<br>:star:[code](https://github.com/chaofengc/TexForce)
  * [Diffusion Models as Optimizers for Efficient Planning in Offline RL](http://arxiv.org/abs/2407.16142v1)<br>:star:[code](https://github.com/RenMing-Huang/TrajectoryDiffuser)
  * [Unified Local-Cloud Decision-Making via Reinforcement Learning](https://arxiv.org/abs/2409.11403)<br>:house:[project](https://unilcd.github.io/)强化学习
* 联邦学习
  * [Towards Multi-modal Transformers in Federated Learning](https://arxiv.org/abs/2404.12467)<br>:star:[code](https://github.com/imguangyu/FedCola)
  * [Fisher Calibration for Backdoor-Robust Heterogeneous Federated Learning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02353.pdf)<br>:star:[code](https://github.com/WenkeHuang/SDFC)
  * [Federated Learning with Local Openset Noisy Labels](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04952.pdf)<br>:star:[code](https://github.com/UCSC-REAL/FedDPCont)
  * [SkyMask: Attack-agnostic Robust Federated Learning with Fine-grained Learnable Masks](https://arxiv.org/abs/2312.12484)<br>:star:[code](https://github.com/KoalaYan/SkyMask)
* 对比学习
  * [FlowCon: Out-of-Distribution Detection using Flow-based Contrastive Learning](https://arxiv.org/abs/2407.03489)<br>:star:[code](https://github.com/saandeepa93/FlowCon_OOD)
  * [Improving Medical Multi-modal Contrastive Learning with Expert Annotations](https://arxiv.org/abs/2403.10153)
  * [Contrastive Learning with Synthetic Positives](https://arxiv.org/abs/2408.16965)对比学习
  * [CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts](https://arxiv.org/abs/2311.16445)<br>:star:[code](https://github.com/YichaoCai1/CLAP)对比学习
* 类增量
  * [Rethinking Few-shot Class-incremental Learning: Learning from Yourself](http://arxiv.org/abs/2407.07468v1)<br>:star:[code](https://github.com/iSEE-Laboratory/Revisting_FSCIL)
  * [Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and Parameter Fusion](http://arxiv.org/abs/2407.14143v1)<br>:star:[code](https://github.com/linlany/RAPF)
  * [Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning](http://arxiv.org/abs/2409.10956v1)<br>:star:[code](https://github.com/KHU-AGI/VIL)
  * [Confidence Self-Calibration for Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2403.12559)<br>:star:[code](https://github.com/ Kaile-Du/CSC)
  * [Canonical Shape Projection is All You Need for 3D Few-shot Class Incremental Learning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05717.pdf)<br>:star:[code](https://github.com/alichr/C3PR)
  * [Personalized Federated Domain-Incremental Learning based on Adaptive Knowledge Matching](https://arxiv.org/abs/2407.05005)
* 上下文学习
  * [Visual Prompt Selection for In-Context Learning Segmentation](http://arxiv.org/abs/2407.10233v1)


## Few/Zero-Shot Learning/DG/A(小/零样本/域泛化/域适应)
* [Source-Free Domain-Invariant Performance Prediction](https://arxiv.org/abs/2408.02209)
* [The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning](https://arxiv.org/abs/2404.09778)<br>:star:[code](https://github.com/Mark-Sky/KCL)
* DG
  * [Towards Multimodal Open-Set Domain Generalization and Adaptation through Self-supervision](https://arxiv.org/abs/2407.01518)<br>:star:[code](https://github.com/donghao51/MOOSA)
  * [Feature Diversification and Adaptation for Federated Domain Generalization](http://arxiv.org/abs/2407.08245v1)
  * [Soft Prompt Generation for Domain Generalization](https://arxiv.org/abs/2404.19286)<br>:star:[code](https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN)
  * [Integrating Markov Blanket Discovery into Causal Representation Learning for Domain Generalization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01578.pdf)
  * [Rethinking LiDAR Domain Generalization: Single Source as Multiple Density Domains](https://arxiv.org/abs/2312.12098)<br>:star:[code](https://github.com/dgist-cvlab/MultiDensityDG)
  * [Improving Zero-Shot Generalization for CLIP with Variational Adapter](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03044.pdf)
  * [Representation Enhancement-Stabilization: Reducing Bias-Variance of Domain Generalization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05278.pdf)<br>:star:[code](https://github.com/zhu-xlab/DG-RES)
* DA
  * [Training-Free Model Merging for Multi-target Domain Adaptation](http://arxiv.org/abs/2407.13771v1)<br>:star:[code](https://air-discover.github.io/ModelMerging)
  * [MC-PanDA: Mask Confidence for Panoptic Domain Adaptation](http://arxiv.org/abs/2407.14110v1)<br>:star:[code](https://github.com/helen1c/MC-PanDA)
  * [Is user feedback always informative? Retrieval Latent Defending for Semi-Supervised Domain Adaptation without Source Data](http://arxiv.org/abs/2407.15383v1)<br>:house:[project](https://sites.google.com/view/junha/nbf-rld)
  * [Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence](http://arxiv.org/abs/2407.18899v1)<br>:star:[code](https://github.com/lyumengyao/lftl)
  * [Improving Unsupervised Domain Adaptation: A Pseudo-Candidate Set Approach](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04650.pdf)
  * [UDA-Bench: Revisiting Common Assumptions in Unsupervised Domain Adaptation Using a Standardized Framework](http://arxiv.org/abs/2409.15264v1)<br>:star:[code](https://github.com/ViLab-UCSD/UDABench_ECCV2024)
  * [Forget More to Learn More: Domain-specific Feature Unlearning for Semi-supervised and Unsupervised Domain Adaptation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05513.pdf)
  * [Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation](http://arxiv.org/abs/2409.04409v1)
* 零样本
  * [Efficient and Versatile Robust Fine-Tuning of Zero-shot Models](https://arxiv.org/abs/2408.05749)
  * [Prompting Language-Informed Distribution for Compositional Zero-Shot Learning](https://arxiv.org/abs/2305.14428)<br>:star:[code](https://github.com/Cogito2012/PLID)


## Vision-Language(视觉语言)
* [Sapiens: Foundation for Human Vision Models](https://arxiv.org/abs/2408.12569)
* [DEAL: Disentangle and Localize Concept-level Explanations for VLMs](http://arxiv.org/abs/2407.14412v1)
* [FlexAttention for Efficient High-Resolution Vision-Language Models](http://arxiv.org/abs/2407.20228v1)<br>:house:[project](https://vis-www.cs.umass.edu/flexattention)
* [QUAR-VLA: Vision-Language-Action Model for Quadruped Robots](https://arxiv.org/abs/2312.14457)
* [Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models](https://arxiv.org/abs/2312.06109)
* [REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models](https://arxiv.org/abs/2408.02231)<br>:house:[project](https://agneetchatterjee.com/revision/)
* [Octopus: Embodied Vision-Language Programmer from Environmental Feedback](https://arxiv.org/abs/2310.08588)<br>:house:[project](https://choiszt.github.io/Octopus/)
* [GazeXplain: Learning to Predict Natural Language Explanations of Visual Scanpaths](http://arxiv.org/abs/2408.02788v1)
* [Learning Chain of Counterfactual Thought for Bias-Robust Vision-Language Reasoning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01309.pdf)<br>:star:[code](https://github.com/SuperJohnZhang/CoBRa)
* [Cascade Prompt Learning for Vision-Language Model Adaptation](http://arxiv.org/abs/2409.17805v1)<br>:star:[code](https://github.com/megvii-research/CasPL)
* [The Hard Positive Truth about Vision-Language Compositionality](http://arxiv.org/abs/2409.17958v1)
* [Improving 2D Feature Representations by 3D-Aware Fine-Tuning](http://arxiv.org/abs/2407.20229v1)<br>:star:[code](https://ywyue.github.io/FiT3D)
* [Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models](http://arxiv.org/abs/2407.11717v1)
* [ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference](http://arxiv.org/abs/2407.12442v1)<br>:star:[code](https://github.com/mc-)
* [FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance](http://arxiv.org/abs/2407.05578v1)<br>:house:[project](https://pumpkin805.github.io/FALIP/)
* [Deciphering the Role of Representation Disentanglement: Investigating Compositional Generalization in CLIP Models](http://arxiv.org/abs/2407.05897v1)
* [GalLoP: Learning Global and Local Prompts for Vision-Language Models](https://arxiv.org/abs/2407.01400)
* [Quantized Prompt for Efficient Generalization of Vision-Language Models](http://arxiv.org/abs/2407.10704v1)<br>:star:[code](https://github.com/beyondhtx/QPrompt)
* [AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization](http://arxiv.org/abs/2407.08156v1)<br>:star:[code](https://github.com/xsx1001/AddressCLIP)<br>:Thumbsup:[AddressCLIP：一张图实现街道级定位，端到端图像地理定位大模型](https://mp.weixin.qq.com/s/vEdAomq7x22p2HaLfGDCEQ)
* [SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://arxiv.org/abs/2403.11299)
* [Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training](https://arxiv.org/abs/2403.02325)<br>:house:[project](https://contrastive-region-guidance.github.io/)
* [Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models](https://arxiv.org/abs/2403.09296)<br>:house:[project](https://chuyu.org/research/snd)
* [Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model](https://arxiv.org/abs/2402.19150)
* [Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks](https://arxiv.org/abs/2403.09377)
* [Take A Step Back: Rethinking the Two Stages in Visual Reasoning](http://arxiv.org/abs/2407.19666v1)<br>:star:[code](https://mybearyzhang.github.io/projects/TwoStageReason/)
* [HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning](https://arxiv.org/abs/2403.12884)<br>:star:[code](https://github.com/ControlNet/HYDRA)视觉推理
* [Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models](http://arxiv.org/abs/2407.05342v1)<br>:star:[code](https://github.com/lloongx/DIKI)
* [Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models](http://arxiv.org/abs/2407.11422v1)<br>:star:[code](https://zjr2000.github.io/projects/reverie)
* [SDPT: Synchronous Dual Prompt Tuning for Fusion-based Visual-Language Pre-trained Models](http://arxiv.org/abs/2407.11414v1)<br>:star:[code](https://github.com/wuyongjianCODE/SDPT)
* [Robust Calibration of Large Vision-Language Adapters](http://arxiv.org/abs/2407.13588v1)<br>:star:[code](https://github.com/Bala93/CLIPCalib)
* [BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models](http://arxiv.org/abs/2407.13442v1)<br>:star:[code](https://beafbench.github.io/)
* [CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs](https://arxiv.org/abs/2408.10433)
* [MyVLM: Personalizing VLMs for User-Specific Queries](https://arxiv.org/abs/2403.14599)<br>:star:[code](https://github.com/snap-research/MyVLM)
* [BRAVE: Broadening the visual encoding of vision-language models](https://arxiv.org/abs/2404.07204)<br>:house:[project](https://brave-vlms.epfl.ch/)
* [IVTP: Instruction-guided Visual Token Pruning for Large Vision-Language Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02577.pdf)
* [ShareGPT4V: Improving Large Multi-Modal Models with Better Captions](https://arxiv.org/abs/2311.12793)<br>:star:[code](https://github.com/ShareGPT4Omni/ShareGPT4V)<br>:house:[project](https://sharegpt4v.github.io/)
* [Adversarial Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2311.11261)<br>:star:[code](https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning)
* [Language-Image Pre-training with Long Captions](https://arxiv.org/abs/2403.17007)<br>:star:[code](https://github.com/zyf0619sjtu/DreamLIP)
* [CoReS: Orchestrating the Dance of Reasoning and Segmentation](https://arxiv.org/abs/2404.05673)<br>:star:[code](https://github.com/baoxiaoyi/CoReS)<br>:house:[project](https://chain-of-reasoning-and-segmentation.github.io/)
* [Attention Prompting on Image for Large Vision-Language Models](https://arxiv.org/abs/2409.17143)<br>:star:[code](https://github.com/yu-rp/apiprompting)
* [SILC: Improving Vision Language Pretraining with Self-Distillation](https://arxiv.org/abs/2310.13355)
* [SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference](https://arxiv.org/abs/2312.01597)<br>:star:[code](https://github.com/wangf3014/SCLIP)
* [AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale](https://arxiv.org/abs/2404.03482)<br>:star:[code](https://github.com/apardyl/AdaGlimpse)
* Video-Language
  * [Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning](http://arxiv.org/abs/2407.03788v1)
  * [PiTe: Pixel-Temporal Alignment for Large Video-Language Model](https://arxiv.org/abs/2409.07239)
* LLM
  * [BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/abs/2404.12390)<br>:house:[project](https://zeyofu.github.io/blink/)
  * [X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning](https://arxiv.org/abs/2311.18799)<br>:star:[code](https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip)<br>:house:[project](https://artemisp.github.io/X-InstructBLIP-page/)
  * [X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs](http://arxiv.org/abs/2407.13851v1)
  * [LLM as Copilot for Coarse-grained Vision-and-Language Navigation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf)
  * [Instruction Tuning-free Visual Token Complement for Multimodal LLMs](https://arxiv.org/abs/2408.05019)
  * [Merlin: Empowering Multimodal LLMs with Foresight Minds](https://arxiv.org/abs/2312.00589)<br>:house:[project](https://ahnsun.github.io/merlin)
  * [Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models](https://arxiv.org/abs/2404.13013)<br>:house:[project](https://groma-mllm.github.io/)
  * [MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?](https://arxiv.org/abs/2403.14624)<br>:star:[code](https://github.com/ZrrSkywalker/MathVerse)
  * [UniCode: Learning a Unified Codebook for Multimodal Large Language Models](https://arxiv.org/abs/2403.09072)
  * [When Do We Not Need Larger Vision Models?](https://arxiv.org/abs/2403.13043)<br>:star:[code](https://github.com/bfshi/scaling_on_scales)
  * [ControlLLM: Augment Language Models with Tools by Searching on Graphs](https://arxiv.org/abs/2310.17796)<br>:star:[code](https://github.com/OpenGVLab/ControlLLM)
  * [Towards Open-Ended Visual Recognition with Large Language Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02233.pdf)<br>:star:[code](https://github.com/bytedance/OmniScient-Model)
  * [Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models](https://arxiv.org/abs/2311.14552)<br>:star:[code](https://github.com/jefferyZhan/Griffon)
  * [LLMGA: Multimodal Large Language Model based Generation Assistant](https://arxiv.org/abs/2311.16500)<br>:star:[code](https://github.com/dvlab-research/LLMGA)<br>:house:[project](https://llmga.github.io/)
  * [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043)<br>:star:[code](https://github.com/dvlab-research/LLaMA-VID)
  * [LLM as Dataset Analyst: Subpopulation Structure Discovery with Large Language Model](https://arxiv.org/abs/2405.02363)<br>:house:[project](https://llm-as-dataset-analyst.github.io/)
  * [Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization](https://arxiv.org/abs/2403.08730)
  * [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611)
  * [LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models](https://arxiv.org/abs/2312.02949)<br>:star:[code](https://github.com/UX-Decoder/LLaVA-Grounding)
  * [ShapeLLM: Universal 3D Object Understanding for Embodied Interaction](https://arxiv.org/abs/2402.17766)<br>:star:[code](https://github.com/qizekun/ShapeLLM)
  * [Making Large Language Models Better Planners with Reasoning-Decision Alignment](https://arxiv.org/abs/2408.13890)
  * [Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities](https://arxiv.org/abs/2403.04908)
  * [Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos](https://arxiv.org/abs/2409.20557)<br>:house:[project](https://sites.google.com/view/vidassist)
  * [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)<br>:star:[code](https://github.com/SaFoLab-WISC/AdaShield)
  * [DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM](https://arxiv.org/abs/2403.12488)<br>:star:[code](https://github.com/yixuan730/DetToolChain)
  * [GENIXER: Empowering Multimodal Large Language Models as a Powerful Data Generator](https://arxiv.org/abs/2312.06731)<br>:star:[code](https://github.com/zhaohengyuan1/Genixer)
  * [Elysium: Exploring Object-level Perception in Videos through Semantic Integration Using MLLMs](https://arxiv.org/abs/2403.16558)<br>:star:[code](https://github.com/Hon-Wong/Elysium)
* 视觉定位
  * [The Nerfect Match: Exploring NeRF Features for Visual Localization](https://arxiv.org/abs/2403.09577)
  * [MAD-DR: Map Compression for Visual Localization with Matchness Aware Descriptor Dimension Reduction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06052.pdf)
* Visual Grounding
  * [LoA-Trans: Enhancing Visual Grounding by Location-Aware Transformers](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01177.pdf)
  * [Visual Grounding for Object-Level Generalization in Reinforcement Learning](https://arxiv.org/abs/2408.01942)<br>:star:[code](https://github.com/PKU-RL/COPL)
* 视觉意图理解
  * [Synergy of Sight and Semantics: Visual Intention Understanding with CLIP](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01721.pdf)<br>:star:[code](https://github.com/yan9qu/IntCLIP)
* 引用表达理解
  * [APL: Anchor-based Prompt Learning for One-stage Weakly Supervised Referring Expression Comprehension](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02055.pdf)<br>:star:[code](https://github.com/Yaxin9Luo/APL)
* 视觉语言理解
  * [Uni3DL: A Unified Model for 3D Vision-Language Understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03330.pdf)<br>:house:[project](https://uni3dl.github.io/)
  * [Unifying 3D Vision-Language Understanding via Promptable Queries](https://arxiv.org/abs/2405.11442)<br>:house:[project](https://github.com/PQ3D/PQ3D)

## Scene 
* 场景合成
  * [Pyramid Diffusion for Fine 3D Large Scene Generation](https://arxiv.org/abs/2311.12085)<br>:star:[code](https://github.com/yuhengliu02/pyramid-discrete-diffusion)<br>:house:[project](https://yuheng.ink/project-page/pyramid-discrete-diffusion/)<br>:Thumbsup:[西南交大&利兹大学等联合提出金字塔离散扩散模型（PDD），实现了3D户外场景生成的粗到细的策略](https://mp.weixin.qq.com/s/D7y1ci3BH2mbF2_yPPiZTg)
  * [External Knowledge Enhanced 3D Scene Generation from Sketch](https://arxiv.org/abs/2403.14121)3D 场景生成
  * [SceneTeller: Language-to-3D Scene Generation](http://arxiv.org/abs/2407.20727v1)<br>:star:[code](https://sceneteller.github.io/)
  * [Forest2Seq: Revitalizing Order Prior for Sequential Indoor Scene Synthesis](http://arxiv.org/abs/2407.05388v1)
  * [Gaussian Grouping: Segment and Edit Anything in 3D Scenes](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04195.pdf)<br>:star:[code](https://github.com/lkeab/gaussian-grouping)
  * [EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion](https://arxiv.org/abs/2405.00915)<br>:star:[code](https://github.com/ymxlzgy/echoscene)
  * [AnyHome: Open-Vocabulary Large-Scale Indoor Scene Generation with First-Person View Exploration](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05573.pdf)室内场景生成
  * [The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation](http://arxiv.org/abs/2407.12579v1)<br>:star:[code](https://leo81005.github.io/Reality-and-Fantasy/)
  * [Language-Driven Physics-Based Scene Synthesis and Editing via Feature Splatting](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05788.pdf)<br>:house:[project](https://feature-splatting.github.io/)场景合成和编辑
* 场景理解
  * [N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields](https://arxiv.org/pdf/2403.10997v1)
  * [Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding](http://arxiv.org/abs/2407.09781v1)
  * [SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding](https://arxiv.org/abs/2401.09340)<br>:house:[project](https://scene-verse.github.io/)
  * [Shape2Scene: 3D Scene Representation Learning Through Pre-training on Shape Data](http://arxiv.org/abs/2407.10200v1)<br>:star:[code](https://github.com/FengZicai/S2S)
  * [nuCraft: Crafting High Resolution 3D Semantic Occupancy for Unified 3D Scene Understanding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00730.pdf)
  * [Open Vocabulary 3D Scene Understanding via Geometry Guided Self-Distillation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02396.pdf)<br>:star:[code](https://github.com/Wang-pengfei/GGSD)
  * [Agent3D-Zero: An Agent for Zero-shot 3D Understanding](https://arxiv.org/abs/2403.11835)<br>:house:[project](https://zhangsha1024.github.io/Agent3D-Zero/)
* 语义场景完
  * [Hierarchical Temporal Context Learning for Camera-based Semantic Scene Completion](http://arxiv.org/abs/2407.02077v1)<br>:star:[code](https://github.com/Arlo0o/HTCL)
  * [Zero-Shot Multi-Object Scene Completion](https://arxiv.org/abs/2403.14628)<br>:house:[project](https://sh8.io/#/oct_mae)
* 场景图生成
  * [OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models](http://arxiv.org/abs/2407.11213v1)<br>:star:[code](https://github.com/franciszzj/OpenPSG)
  * [Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation](http://arxiv.org/abs/2407.15396v1)
  * [Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction](http://arxiv.org/abs/2407.19259v1)<br>:star:[code](https://github.com/Zhuzi24/SBG)
  * [Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention](https://arxiv.org/pdf/2311.10988)<br>:star:[code](https://github.com/gpt4vision/OvSGTR)<br>:thumbsup:[突破场景图生成的边界：OvSGTR 实现全开放词汇场景图生成](https://mp.weixin.qq.com/s/a6F24PtIzcuSXCrhdsS8tw)
  
## UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)
* [Masked Angle-Aware Autoencoder for Remote Sensing Images](https://arxiv.org/abs/2408.01946)<br>:star:[code](https://github.com/benesakitam/MA3E)
* [Radiance Field Learners As UAV First-Person Viewers](https://arxiv.org/abs/2408.05533)
* [MutDet: Mutually Optimizing Pre-training for Remote Sensing Object Detection](https://arxiv.org/abs/2407.09920)<br>:star:[code](https://github.com/floatingstarZ/MutDet)
* [Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01738.pdf)
* [Free-Viewpoint Video of Outdoor Sports Using a Drone](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02178.pdf)
* [Learning Representations of Satellite Images From Metadata Supervision](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03849.pdf)(https://github.com/preligens-lab/satmip)卫星图像

## Automated Driving(自动驾驶)
* [Continuity Preserving Online CenterLine Graph Learning](http://arxiv.org/abs/2407.11337v1)
* [Accelerating Online Mapping and Behavior Prediction via Direct BEV Feature Attention](http://arxiv.org/abs/2407.06683v1)<br>:star:[code](https://github.com/alfredgu001324/MapBEVPrediction)
* [RepVF: A Unified Vector Fields Representation for Multi-task 3D Perception](http://arxiv.org/abs/2407.10876v1)<br>:star:[code](https://github.com/jbji/RepVF)
* [MapDistill: Boosting Efficient Camera-based HD Map Construction via Camera-LiDAR Fusion Model Distillation](http://arxiv.org/abs/2407.11682v1)
* [Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation](http://arxiv.org/abs/2407.11954v1)<br>:star:[code](https://github.com/olga-zats/GTDA)
* [CarFormer: Self-Driving with Learned Object-Centric Representations](http://arxiv.org/abs/2407.15843v1)<br>:star:[code](https://kuis-ai.github.io/CarFormer/)
* [Image-to-Lidar Relational Distillation for Autonomous Driving Data](http://arxiv.org/abs/2409.00845v1)
* [Modelling Competitive Behaviors in Autonomous Driving Under Generative World Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05085.pdf)<br>:star:[code](https://github.com/qiaoguanren/MARL-CCE)
* [Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438)<br>:house:[project](https://vlm-driver.github.io/)
* [PPAD: Iterative Interactions of Prediction and Planning for End-to-end Autonomous Driving](https://arxiv.org/abs/2311.08100)<br>:star:[code](https://github.com/zlichen/PPAD)
* [Asynchronous Large Language Model Enhanced Planner for Autonomous Driving](https://arxiv.org/abs/2406.14556)<br>:star:[code](https://github.com/memberRE/AsyncDriver)
* [Neural Volumetric World Models for Autonomous Driving](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf)
* [SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving](https://arxiv.org/abs/2407.01702)<br>:star:[code](https://github.com/KTH-RPL/SeFlow)
* [Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex Driving Scenes](https://arxiv.org/abs/2404.17961)<br>:star:[code](https://github.com/zelongzeng/rwpm)自动驾驶
* [SLEDGE: Synthesizing Driving Environments with Generative Models and Rule-Based Traffic](https://arxiv.org/abs/2403.17933)
* [I Can't Believe It's Not Scene Flow!](https://arxiv.org/abs/2403.04739)<br>:star:[code](https://github.com/kylevedder/BucketedSceneFlowEval)场景流
* [Safe-Sim: Safety-Critical Closed-Loop Traffic Simulation with Diffusion-Controllable Adversaries](https://arxiv.org/abs/2401.00391)<br>:house:[project](https://safe-sim.github.io/)交通
* [UniM2AE: Multi-modal Masked Autoencoders with Unified 3D Representation for 3D Perception in Autonomous Driving](https://arxiv.org/abs/2308.10421)<br>:star:[code](https://github.com/hollow-503/UniM2AE)
* [Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)](https://arxiv.org/abs/2402.16720)<br>:house:[project](https://thinklab-sjtu.github.io/CornerCaseRepo/)
* [Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661)<br>:star:[code](https://github.com/fudan-zvg/Reason2Drive)
* [Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving](https://arxiv.org/abs/2409.18343)
* [Lane Graph as Path: Continuity-preserving Path-wise Modeling for Online Lane Graph Construction](https://arxiv.org/abs/2303.08815)<br>:star:[code](https://github.com/hustvl/LaneGAP)
* 轨迹预测
  * [Risk-Aware Self-Consistent Imitation Learning for Trajectory Planning in Autonomous Driving](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02087.pdf)
  * [Progressive Pretext Task Learning for Human Trajectory Prediction](http://arxiv.org/abs/2407.11588v1)<br>:star:[code](https://github.com/iSEE-Laboratory/PPT)
  * [DySeT: a Dynamic Masked Self-distillation Approach for Robust Trajectory Prediction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00414.pdf)
  * [Learning Semantic Latent Directions for Accurate and Controllable Human Motion Prediction](http://arxiv.org/abs/2407.11494v1)<br>:star:[code](https://github.com/GuoweiXu368/SLD-HMP)
  * [NeRMo: Learning Implicit Neural Representations for 3D Human Motion Prediction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06076.pdf)
  * [VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions](http://arxiv.org/abs/2407.12345v1)<br>:star:[code](https://moonseokha.github.io/VisionTrap/)
  * [Optimizing Diffusion Models for Joint Trajectory Prediction and Controllable Generation](http://arxiv.org/abs/2408.00766v1)<br>:star:[code](https://yixiaowang7.github.io/OptTrajDiff_Page/)
  * [Adaptive Human Trajectory Prediction via Latent Corridors](https://arxiv.org/abs/2312.06653)<br>:house:[project](https://neerja.me/atp_latent_corridors/)
  * [NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous Driving](https://arxiv.org/abs/2404.07762)<br>:star:[code](https://github.com/atonderski/neuro-ncap)
  * 车辆轨迹预测
    * [UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction](https://arxiv.org/abs/2403.15098)<br>:star:[code](https://github.com/vita-epfl/UniTraj)
* 占据预测
  * [VEON: Vocabulary-Enhanced Occupancy Prediction](http://arxiv.org/abs/2407.12294v1)
  * [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038)<br>:star:[code](https://github.com/wzzheng/OccWorld)
  * [OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous Driving](https://arxiv.org/abs/2404.15014)<br>:house:[project](https://occgen-ad.github.io/)
  * [Fully Sparse 3D Occupancy Prediction](https://arxiv.org/abs/2312.17118)<br>:star:[code](https://github.com/MCG-NJU/SparseOcc)
  * [Monocular Occupancy Prediction for Scalable Indoor Scenes](https://arxiv.org/abs/2407.11730)<br>:star:[code](https://github.com/hongxiaoy/ISO)
  * [ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D Occupancy Perception via View-Guided Transformers](https://arxiv.org/abs/2405.04299)<br>:star:[code](https://github.com/ViewFormerOcc/ViewFormer-Occ)
  * [GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2405.17429)<br>:star:[code](https://github.com/huang-yh/GaussianFormer)3D 语义占用预测
* 车道线检测
  * [OMR: Occlusion-Aware Memory-Based Refinement for Video Lane Detection](https://arxiv.org/abs/2408.07486)<br>:star:[code](https://github.com/dongkwonjin/OMR)

## Video
* [Stable Video Portraits](http://arxiv.org/abs/2409.18083v1)<br>:house:[project](https://svp.is.tue.mpg.de)
* [Text-Guided Video Masked Autoencoder](http://arxiv.org/abs/2408.00759v1)
* [Multi-Modal Video Dialog State Tracking in the Wild](http://arxiv.org/abs/2407.02218v1)
* [Training-free Video Temporal Grounding using Large-scale Pre-trained Models](http://arxiv.org/abs/2408.16219v1)
* [Rethinking Weakly-supervised Video Temporal Grounding From a Game Perspective](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06164.pdf)
* [Fast Encoding and Decoding for Implicit Video Representation](https://arxiv.org/abs/2409.19429)<br>:star:[code](https://github.com/haochen-rye/FastNeRV)<br>:house:[project](https://haochen-rye.github.io/FastNeRV/)
* VAD
  * [Cross-Domain Learning for Video Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2408.05191)
  * [Learning Anomalies with Normality Prior for Unsupervised Video Anomaly Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00941.pdf)<br>:star:[code](https://github.com/shyern/LANP-UVAD)
  * [Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection](https://arxiv.org/abs/2401.13551)<br>:star:[code](https://github.com/benedictstar/Joint-VAD)视频异常检测
* 视频理解
  * [VideoMamba: Spatio-Temporal Selective State Space Model](http://arxiv.org/abs/2407.08476v1)<br>:star:[code](http://github.com/jinyjelly/VideoMamba)
  * [VideoMamba: State Space Model for Efficient Video Understanding](https://arxiv.org/abs/2403.06977)<br>:star:[code](https://github.com/OpenGVLab/VideoMamba)
  * [Goldfish: Vision-Language Understanding of Arbitrarily Long Videos](http://arxiv.org/abs/2407.12679v1)<br>:star:[code](https://vision-cair.github.io/Goldfish_website/)
  * [Learning Video Context as Interleaved Multimodal Sequences](https://arxiv.org/abs/2407.21757)<br>:star:[code](https://github.com/showlab/MovieSeq)
  * [FunQA: Towards Surprising Video Comprehension](https://arxiv.org/abs/2306.14899)<br>:house:[project](https://funqa-benchmark.github.io/)
  * [Vamos: Versatile Action Models for Video Understanding](https://arxiv.org/abs/2311.13627)<br>:star:[code](https://github.com/brown-palm/Vamos)<br>:house:[project](https://brown-palm.github.io/Vamos/)
  * [LongVLM: Efficient Long Video Understanding via Large Language Models](https://arxiv.org/abs/2404.03384)<br>:star:[code](https://github.com/ziplab/LongVLM)
  * [VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding](https://arxiv.org/abs/2403.11481)<br>:star:[code](https://github.com/YueFan1014/VideoAgent)<br>:house:[project](https://videoagent.github.io/)
* 视频分类
  * [Open Vocabulary Multi-Label Video Classification](http://arxiv.org/abs/2407.09073v1)
* 视频解析
  * [CoLeaF: A Contrastive-Collaborative Learning Framework for Weakly Supervised Audio-Visual Video Parsing](https://arxiv.org/pdf/2405.10690)<br>:star:[code](https://github.com/faeghehsardari/coleaf)
* 视频帧插值
  * [IAM-VFI : Interpolate Any Motion for Video Frame Interpolation with motion complexity map](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02398.pdf)
  * [Clearer Frames, Anytime: Resolving Velocity Ambiguity in Video Frame Interpolation](https://arxiv.org/abs/2311.08007)<br>:house:[project](https://zzh-tech.github.io/InterpAny-Clearer/)
* 视频类增量
  * [STSP: Spatial-Temporal Subspace Projection for Video Class-incremental Learning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04106.pdf)
* 视频抄袭片段定位
  * [Self-Supervised Video Copy Localization with Regional Token Representation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01818.pdf)

## Medical Image Progress(医学影响处理)
* [Adaptive Compressed Sensing with Diffusion-Based Posterior Sampling](http://arxiv.org/abs/2407.08256v1)
* [Co-synthesis of Histopathology Nuclei Image-Label Pairs using a Context-Conditioned Joint Diffusion Model](http://arxiv.org/abs/2407.14434v1)
* [Identity-Consistent Diffusion Network for Grading Knee Osteoarthritis Progression in Radiographic Imaging](http://arxiv.org/abs/2407.21381v1)
* [Multistain Pretraining for Slide Representation Learning in Pathology](http://arxiv.org/abs/2408.02859v1)<br>:star:[code](https://github.com/mahmoodlab/MADELEINE)
* [Energy-induced Explicit quantification for Multi-modality MRI fusion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01183.pdf)<br>:star:[code](https://github.com/JerryQseu/EEPA)
* [Brain-ID: Learning Contrast-agnostic Anatomical Representations for Brain Imaging](https://arxiv.org/abs/2311.16914)<br>:star:[code](https://github.com/peirong26/Brain-ID)
* [CardiacNet: Learning to Reconstruct Abnormalities for Cardiac Disease Assessment from Echocardiogram Videos](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03391.pdf)<br>:star:[code](https://github.com/xmed-lab/CardiacNet)心脏病评估
* [Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer](https://arxiv.org/abs/2404.03819)CT
* 组织病理学图像分类
  * [Test-Time Stain Adaptation with Diffusion Models for Histopathology Image Classification](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05175.pdf)
* 切片图像分类
  * [DGR-MIL: Exploring Diverse Global Representation in Multiple Instance Learning for Whole Slide Image Classification](http://arxiv.org/abs/2407.03575v1)<br>:star:[code](https://github.com/ChongQingNoSubway/DGR-MIL)
  * [Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification](http://arxiv.org/abs/2407.10814v1)
  * [Snuffy: Efficient Whole Slide Image Classifier](https://arxiv.org/abs/2408.08258)
* 医学图像分割
  * [FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification](http://arxiv.org/abs/2407.08813v1)<br>:house:[project](https://ophai.hms.harvard.edu/datasets/harvard-fairdomain20k)<br>:star:[code](https://github.com/Harvard-Ophthalmology-AI-Lab/FairDomain)  
  * [PMT: Progressive Mean Teacher via Exploring Temporal Consistency for Semi-Supervised Medical Image Segmentation](http://arxiv.org/abs/2409.05122v1)<br>:star:[code](https://github.com/Axi404/PMT)
  * [Domesticating SAM for Breast Ultrasound Image Segmentation via Spatial-frequency Fusion and Uncertainty Correction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03309.pdf)<br>:star:[code](https://github.com/dodooo1/SFRecSAM)
  * [Alternate Diverse Teaching for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2311.17325)<br>:star:[code](https://github.com/ZhenZHAO/AD-MT)
  * [I-MedSAM: Implicit Medical Image Segmentation with Segment Anything](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01503.pdf)<br>:star:[code](https://github.com/ucwxb/I-MedSAM)
  * [VP-SAM: Taming Segment Anything Model for Video Polyp Segmentation via Disentanglement and Spatio-temporal Side Network](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03403.pdf)<br>:star:[code](https://github.com/zhixue-fang/VPSAM)息肉分割
* 医学图像配准
  * [Unsupervised Multi-modal Medical Image Registration via Invertible Translation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04467.pdf)<br>:star:[code](https://github.com/MeggieGuo/INNReg)
  * [Adaptive Correspondence Scoring for Unsupervised Medical Image Registration](https://arxiv.org/abs/2312.00837)<br>:house:[project](https://voldemort108x.github.io/AdaCS/)
* 放射学报告生成
  * [Contrastive Learning with Counterfactual Explanations for Radiology Report Generation](http://arxiv.org/abs/2407.14474v1)
  * [HERGen: Elevating Radiology Report Generation with Longitudinal Data](http://arxiv.org/abs/2407.15158v1)
* X 光片
  * [ChEX: Interactive Localization and Region Description in Chest X-rays](https://arxiv.org/abs/2404.15770)<br>:star:[code](https://github.com/philip-mueller/chex)胸部 X 光片
  * [Plug-and-Play Learned Proximal Trajectory for 3D Sparse-View X-Ray Computed Tomography](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05164.pdf)<br>:star:[code](https://github.com/romainvo/pnp-learned-proximal-trajectory)
  * [DiffuX2CT: Diffusion Learning to Reconstruct CT Images from Biplanar X-Rays](https://arxiv.org/abs/2407.13545)
* 医学机器人
  * [Shape-guided Configuration-aware Learning for Endoscopic-image-based Pose Estimation of Flexible Robotic Instruments](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03268.pdf)<br>:house:[project](https://poseflex.github.io/)
* 生物医学图像
  * [ScribblePrompt: Fast and Flexible Interactive Segmentation for Any Biomedical Image](https://arxiv.org/abs/2312.07381)<br>:star:[code](https://github.com/halleewong/ScribblePrompt)


## GAN/Image Synthesis(图像生成)
* [Diffusion Models as Data Mining Tools](http://arxiv.org/abs/2408.02752v1)<br>:star:[code](https://diff-mining.github.io/)
* [ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation](https://arxiv.org/abs/2408.02226)<br>:house:[project](https://procreate-diffusion.github.io/)
* [Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation](https://arxiv.org/abs/2404.00636)<br>:house:[project](https://export3d.github.io/)
* [HiEI: A Universal Framework for Generating High-quality Emerging Images from Natural Images](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04514.pdf)
* [UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models](https://arxiv.org/abs/2312.04884)<br>:star:[code](https://github.com/ZYM-PKU/UDiffText)
* [Free-ATM: Harnessing Free Attention Masks for Representation Learning on Diffusion-Generated Images](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05708.pdf)
* [Idea2Img: Iterative Self-Refinement with GPT-4V for Automatic Image Design and Generation](https://arxiv.org/abs/2310.08541)<br>:house:[project](https://idea2img.github.io/)
* [OMG: Occlusion-friendly Personalized Multi-concept Generation in Diffusion Models](https://arxiv.org/abs/2403.10983)<br>:star:[code](https://github.com/kongzhecn/OMG)
* [DreamDiffusion: High-Quality EEG-to-Image Generation with Temporal Masked Signal Modeling and CLIP Alignment](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04605.pdf)
* GAN
  * [CLR-GAN: Improving GANs Stability and Quality via Consistent Latent Representation and Reconstruction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00072.pdf)<br>:star:[code](https://github.com/Petecheco/CLR-GAN)
  * [A Closer Look at GAN Priors: Exploiting Intermediate Features for Enhanced Model Inversion Attacks](https://arxiv.org/abs/2407.13863)<br>:star:[code](https://github.com/final-solution/IF-GMI)
  * [Distilling Diffusion Models into Conditional GANs](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04129.pdf)
  * [Exploring Guided Sampling of Conditional GANs](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03700.pdf)<br>:star:[code](https://github.com/zyf0619sjtu/GANdance)
  * [Learning 3D-aware GANs from Unposed Images with Template Feature Field](https://arxiv.org/abs/2404.05705)<br>:house:[project](https://xdimlab.github.io/TeFF)
* 扩散
  * [Measuring Style Similarity in Diffusion Models](https://arxiv.org/abs/2404.01292)<br>:star:[code](https://github.com/learn2phoenix/CSD)
  * [Iterative Ensemble Training with Anti-Gradient Control for Mitigating Memorization in Diffusion Models](http://arxiv.org/abs/2407.15328v1)<br>:star:[code](https://github.com/liuxiao-guan/IET_AGC)
  * [Beta-Tuned Timestep Diffusion Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00328.pdf)
  * [ SMooDi: Stylized Motion Diffusion Model](https://arxiv.org/abs/2407.12783)<br>:house:[project](https://neu-vi.github.io/SMooDi/)
  * [Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation](https://arxiv.org/abs/2402.10491)<br>:star:[code](https://github.com/GuoLanqing/Self-Cascade)
  * [Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using Diffusion Models](https://arxiv.org/abs/2311.17050)<br>:star:[code](https://github.com/Yzmblog/SurfD)<br>:house:[project](https://yzmblog.github.io/projects/SurfD/)
  * [Implicit Concept Removal of Diffusion Models](https://arxiv.org/abs/2310.05873)<br>:house:[project](https://kaichen1998.github.io/projects/geom-erasing/)
  * [ZigMa: A DiT-style Zigzag Mamba Diffusion Model](https://arxiv.org/abs/2403.13802)<br>:star:[code](https://github.com/CompVis/zigma)
  * [ColorPeel: Color Prompt Learning with Diffusion Models via Color and Shape Disentanglement](http://arxiv.org/abs/2407.07197v1)<br>:star:[code](https://moatifbutt.github.io/colorpeel/)<br>:house:[project](https://moatifbutt.github.io/colorpeel/)
  * [Timestep-Aware Correction for Quantized Diffusion Models](http://arxiv.org/abs/2407.03917v1)
  * [Shapefusion: 3D localized human diffusion models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02155.pdf)<br>:house:[project](https://rolpotamias.github.io/Shapefusion/)
  * [MVDD: Multi-View Depth Diffusion Models](https://arxiv.org/abs/2312.04875)
  * [SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher](http://arxiv.org/abs/2408.14176v1)<br>:star:[code](https://github.com/vinairesearch/swiftbrushv2)<br>:house:[project](https://swiftbrushv2.github.io/)<br>:tv:[video](https://www.youtube.com/watch?v=xdYWMNJ6_Uo)
  * [Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model](http://arxiv.org/abs/2409.16689v1)<br>:star:[code](https://iwa-shi.github.io/Layout-Corrector-Project-Page/)
  * [Compensation Sampling for Improved Convergence in Diffusion Models](https://arxiv.org/abs/2312.06285)<br>:star:[code](https://github.com/hotfinda/Compensation-sampling)
  * [ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction](http://arxiv.org/abs/2407.07077v1)<br>:star:[code](https://haoosz.github.io/ConceptExpress/)<br>:star:[code](https://github.com/haoosz/ConceptExpress)
  * [Self-Guided Generation of Minority Samples Using Diffusion Models](http://arxiv.org/abs/2407.11555v1)<br>:star:[code](https://github.com/soobin-um/sg-minority)
* [Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems](http://arxiv.org/abs/2407.11288v1)<br>:star:[code](https://github.com/ualcalar17/ZAPS)
* [LogoSticker: Inserting Logos into Diffusion Models for Customized Generation](http://arxiv.org/abs/2407.13752v1)<br>:star:[code](https://mingkangz.github.io/logosticker)
* 纹理合成
  * [VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing](http://arxiv.org/abs/2407.04461v1)
  * [TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling](http://arxiv.org/abs/2408.01291v1)<br>:star:[code](https://dong-huo.github.io/TexGen/)
* 图像合成
  * [Editable Image Elements for Controllable Synthesis](https://arxiv.org/abs/2404.16029)<br>:house:[project](https://jitengmu.github.io/Editable_Image_Elements/)
  * [Assessing Sample Quality via the Latent Space of Generative Models](http://arxiv.org/abs/2407.15171v1)<br>:star:[code](https://github.com/cvlab-stonybrook/LS-sample-quality)
  * [SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior](https://arxiv.org/abs/2403.09638)<br>:house:[project](https://air-discover.github.io/SCP-Diff/)
  * [Zero-shot Text-guided Infinite Image Synthesis with LLM guidance](http://arxiv.org/abs/2407.12642v1)
  * [$\infty$-Brush: Controllable Large Image Synthesis with Diffusion Models in Infinite Dimensions](http://arxiv.org/abs/2407.14709v1)<br>:star:[code](https://github.com/cvlab-stonybrook/infinity-brush)<br>:star:[code](https://histodiffusion.github.io)
  * [SCP-Diff: Spatial-Categorical Joint Prior for Diffusion Based Semantic Image Synthesis](https://arxiv.org/abs/2403.09638)<br>:house:[project](https://air-discover.github.io/SCP-Diff/)
  * [2S-ODIS: Two-Stage Omni-Directional Image Synthesis by Geometric Distortion Correction](https://www.arxiv.org/abs/2409.09969)<br>:star:[code](https://github.com/islab-sophia/2s-odis)
  * [Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis](https://arxiv.org/abs/2409.17439)
  * [FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis](https://arxiv.org/abs/2403.12963)<br>:star:[code](https://github.com/LeonHLJ/FouriScale)
* 图像生成
  * [MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation](https://arxiv.org/abs/2404.05674)<br>:star:[code](https://github.com/bytedance/MoMA/tree/main)<br>:house:[project](https://moma-adapter.github.io/)
  * [Linearly Controllable GAN: Unsupervised Feature Categorization and Decomposition for Image Generation and Manipulation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00540.pdf)
  * [PanoFree: Tuning-Free Holistic Multi-view Image Generation with Cross-view Self-Guidance](https://arxiv.org/abs/2408.02157)
  * [AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation](http://arxiv.org/abs/2409.00342v1)<br>:star:[code](https://github.com/LeapLabTHU/AdaNAT)
  * [AccDiffusion: An Accurate Method for Higher-Resolution Image Generation](https://arxiv.org/abs/2407.10738)<br>:house:[project](https://lzhxmu.github.io/accdiffusion/accdiffusion.html)<br>:thumbsup:[成功地进行无重复高分辨率的图像生成](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [Towards Reliable Advertising Image Generation Using Human Feedback](http://arxiv.org/abs/2408.00418v1)<br>:star:[code](https://github.com/ZhenbangDu/Reliable_AD)
  * [StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion](https://arxiv.org/abs/2404.05979)<br>:star:[code](https://github.com/tobran/StoryImager)
  * [Model-agnostic Origin Attribution of Generated Images with Few-shot Examples](https://arxiv.org/abs/2404.02697)
  * [Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance](https://arxiv.org/abs/2406.04551)<br>:star:[code](https://github.com/facebookresearch/Contextualized-Vendi-Score-Guidance)
  * [Tuning-Free Image Customization with Image and Text Guidance](https://arxiv.org/abs/2403.12658)
  * [Collaborative Control for Geometry-Conditioned PBR Image Generation](https://arxiv.org/abs/2402.05919)<br>:house:[project](https://unity-research.github.io/holo-gen/)
  * [DiffiT: Diffusion Vision Transformers for Image Generation](https://arxiv.org/abs/2312.02139)<br>:star:[code](https://github.com/NVlabs/DiffiT)
  * [MultiGen: Zero-shot Image Generation from Multi-modal Prompts](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01296.pdf)
* 视频生成
  * [FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537)<br>:star:[code](https://github.com/TianxingWu/FreeInit)<br>:house:[project](https://tianxingwu.github.io/pages/FreeInit/)
  * [DragAnything: Motion Control for Anything using Entity Representation](https://arxiv.org/abs/2403.07420/)<br>:star:[code](https://github.com/showlab/DragAnything)<br>:house:[project](https://weijiawu.github.io/draganything_page/)
  * [Physics-Based Interaction with 3D Objects via Video Generation](https://arxiv.org/abs/2404.13026)<br>:star:[code](https://github.com/a1600012888/PhysDreamer)<br>:house:[project](https://physdreamer.github.io/)
  * [DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors](https://arxiv.org/abs/2310.12190)<br>:star:[code](https://github.com/Doubiiu/DynamiCrafter)
  * [PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible Pose Control](https://arxiv.org/abs/2405.14582)<br>:house:[project](https://ml-gsai.github.io/PoseCrafter-demo/)
  * [MoVideo: Motion-Aware Video Generation with Diffusion Models](https://arxiv.org/abs/2311.11325)<br>:house:[project](https://jingyunliang.github.io/MoVideo)
  * [IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation](http://arxiv.org/abs/2407.10937v1)<br>:star:[code](https://yhzhai.github.io/idol/)
  * [MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing](https://arxiv.org/abs/2311.17338)
  * 文本-视频质量评估
    * [Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment](http://arxiv.org/abs/2403.11956v1)<br>:star:[code](https://github.com/QMME/T2VQA)
* 视频编辑
  * [DragVideo: Interactive Drag-style Video Editing](https://arxiv.org/pdf/2312.02216)
  * [Video Editing via Factorized Diffusion Distillation](https://arxiv.org/abs/2403.09334)
  * [DNI: Dilutional Noise Initialization for Diffusion Video Editing](http://arxiv.org/abs/2409.13037v1)
  * [DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion Consistency](https://arxiv.org/abs/2408.07481)
  * [DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video Editing](https://arxiv.org/abs/2403.12002)<br>:house:[project](https://hyeonho99.github.io/dreammotion/)
  * [Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion](https://arxiv.org/abs/2403.14617)<br>:star:[code](https://github.com/sfanxiang/videoshop)
* 图像编辑
  * [ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion](https://export.arxiv.org/abs/2404.17230)
  * [COMPOSE: Comprehensive Portrait Shadow Editing](https://arxiv.org/abs/2408.13922)
  * [FreeDiff: Progressive Frequency Truncation for Image Editing with Diffusion Models](https://arxiv.org/abs/2404.11895)<br>:star:[code](https://github.com/thermal-dynamics/freediff)
  * [ByteEdit: Boost, Comply and Accelerate Generative Image Editing](https://arxiv.org/abs/2404.04860)
  * [RegionDrag: Fast Region-Based Image Editing with Diffusion Models](http://arxiv.org/abs/2407.18247v1)<br>:star:[code](https://visual-ai.github.io/regiondrag)
  * [3DEgo: 3D Editing on the Go!](http://arxiv.org/abs/2407.10102v1)<br>:star:[code](https://3dego.github.io/)
  * [View-Consistent 3D Editing with Gaussian Splatting](https://arxiv.org/abs/2403.11868)<br>:house:[project](http://vcedit.github.io/)
  * [Watch Your Steps: Local Image and Scene Editing by Text Instructions](https://arxiv.org/abs/2308.08947)<br>:house:[project](https://ashmrz.github.io/WatchYourSteps/)
  * [Chat-Edit-3D: Interactive 3D Scene Editing via Text Prompts](http://arxiv.org/abs/2407.06842v1)<br>:house:[project](https://sk-fun.fun/CE3D")
  * [InstructGIE: Towards Generalizable Image Editing](https://arxiv.org/abs/2403.05018)
  * [Lazy Diffusion Transformer for Interactive Image Editing](https://arxiv.org/abs/2404.12382)<br>:house:[project](https://lazydiffusion.github.io/)
  * [DATENeRF: Depth-Aware Text-based Editing of NeRFs](https://arxiv.org/abs/2404.04526)<br>:star:[code](https://github.com/sararoma95/DATENeRF)<br>:house:[project](https://datenerf.github.io/DATENeRF/)
  * [ST-LDM: A Universal Framework for Text-Grounded Object Generation in Real Images](https://arxiv.org/abs/2403.10004)图像编辑
  * [SwapAnything: Enabling Arbitrary Object Swapping in Personalized Image Editing](http://arxiv.org/abs/2404.05717)<br>:house:[project](https://swap-anything.github.io/)
  * [Source Prompt Disentangled Inversion for Boosting Image Editability with Diffusion Models](https://arxiv.org/abs/2403.11105)<br>:star:[code](https://github.com/leeruibin/SPDInv)
  * [Robust-Wide: Robust Watermarking against Instruction-driven Image Editing](https://arxiv.org/abs/2402.12688)<br>:star:[code](https://github.com/hurunyi/Robust-Wide)
  * [FlexiEdit: Frequency-Aware Latent Refinement for Enhanced Non-Rigid Editing](http://arxiv.org/abs/2407.17850v1)
  * [Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing](https://arxiv.org/abs/2403.09468)<br>:star:[code](https://github.com/furiosa-ai/eta-inversion)
  * [RadEdit: stress-testing biomedical vision models via diffusion image editing](https://arxiv.org/abs/2312.12865)
  * [Responsible Visual Editing](https://arxiv.org/abs/2404.05580)<br>:star:[code](https://github.com/kodenii/Responsible-Visual-Editing)
* 图像-视频
   * [Rethinking Image-to-Video Adaptation: An Object-centric Perspective](http://arxiv.org/abs/2407.06871v1)
   * [R2-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding](https://arxiv.org/abs/2404.00801)<br>:star:[code](https://github.com/yeliudev/R2-Tuning)
* 文本-视频
  * [E.T. the Exceptional Trajectories: Text-to-camera-trajectory generation with character awareness](https://arxiv.org/abs/2407.01516)<br>:house:[project](https://www.lix.polytechnique.fr/vista/projects/2024_et_courant/)
  * [MotionDirector: Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2310.08465)<br>:star:[code](https://github.com/showlab/MotionDirector)<br>:house:[project](https://showlab.github.io/MotionDirector/)
  * [SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models](https://arxiv.org/abs/2311.16933)<br>:house:[project](https://guoyww.github.io/projects/SparseCtrl)
  * [MEVG: Multi-event Video Generation with Text-to-Video Models](https://arxiv.org/abs/2312.04086)<br>:house:[project](https://kuai-lab.github.io/eccv2024mevg)
  * [Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2402.14780)<br>:house:[project](https://customize-a-video.github.io/)
  * [xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations](http://arxiv.org/abs/2408.12590v1)
* 文本-3D
  * [Diverse Text-to-3D Synthesis with Augmented Text Embedding](https://arxiv.org/abs/2312.02192v2)<br>:star:[code](https://github.com/VinAIResearch/DiverseDream)<br>:house:[project](https://diversedream.github.io/)
  * [DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for Text-to-3D Generation](http://arxiv.org/abs/2409.07454v1)<br>:star:[code](https://dreammesh.github.io)
  * [DreamDissector: Learning Disentangled Text-to-3D Generation from 2D Diffusion Priors](http://arxiv.org/abs/2407.16260v1)<br>:star:[code](https://chester256.github.io/dreamdissector)
  * [DreamReward: Text-to-3D Generation with Human Preference](https://arxiv.org/abs/2403.14613)<br>:house:[project](https://jamesyjl.github.io/DreamReward)
  * [GVGEN: Text-to-3D Generation with Volumetric Representation](https://arxiv.org/abs/2403.12957)<br>:star:[code](https://github.com/SOTAMak1r/GVGEN)
  * [WordRobe: Text-Guided Generation of Textured 3D Garments](https://arxiv.org/abs/2403.17541)<br>:house:[project](https://wordrobe24.github.io/WordRobe_Page/)
  * [UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation](https://arxiv.org/abs/2312.08754)<br>:star:[code](https://github.com/YG256Li/UniDream)<br>:house:[project](https://yg256li.github.io/UniDream/)
  * [ScaleDreamer: Scalable Text-to-3D Synthesis with Asynchronous Score Distillation](http://arxiv.org/abs/2407.02040v1)<br>:star:[code](https://github.com/theEricMa/ScaleDreamer)
  * [CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model](https://arxiv.org/abs/2403.05034)<br>:house:[project](https://ml.cs.tsinghua.edu.cn/~zhengyi/CRM/)
  * [Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable Repainting](https://arxiv.org/abs/2312.13271)<br>:house:[project](https://pku-yuangroup.github.io/repaint123/)
  * [JointDreamer: Ensuring Geometry Consistency and Text Congruence in Text-to-3D Generation via Joint Score Distillation](http://arxiv.org/abs/2407.12291v1)
  * [Connecting Consistency Distillation to Score Distillation for Text-to-3D Generation](http://arxiv.org/abs/2407.13584v1)<br>:star:[code](https://github.com/LMozart/ECCV2024-GCS-BEG)
  * [TPA3D: Triplane Attention for Fast Text-to-3D Generation](https://arxiv.org/abs/2312.02647)<br>:house:[project](https://redxouls.github.io/TPA3D/)
  * [DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation](https://arxiv.org/abs/2404.06119)<br>:star:[code](https://github.com/iSEE-Laboratory/DreamView)
* 文本-图像
  * [Navigating Text-to-lmage Generative Bias acrossIndic Languages]
  * [Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation](https://arxiv.org/abs/2403.07860)<br>:star:[code](https://github.com/ShihaoZhaoZSH/LaVi-Bridge)<br>:house:[project](https://shihaozhaozsh.github.io/LaVi-Bridge/)
  * [PreciseControl: Enhancing Text-To-Image Diffusion Models with Fine-Grained Attribute Control](https://arxiv.org/abs/2408.05083)<br>:house:[project](https://rishubhpar.github.io/PreciseControl.home/)
  * [IMMA: Immunizing text-to-image Models against Malicious Adaptation](https://arxiv.org/abs/2311.18815)<br>:star:[code](https://github.com/amberyzheng/IMMA)
  * [Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers](https://arxiv.org/abs/2311.17717)<br>:house:[project](https://jasper0314-huang.github.io/receler-concept-erasing/)
  * [Textual-Visual Logic Challenge: Understanding and Reasoning in Text-to-Image Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00796.pdf)<br>:star:[code](https://github.com/IntelLabs/Textual-Visual-Logic-Challenge)
  * [Navigating Text-to-Image Generative Bias across Indic Languages](http://arxiv.org/abs/2408.00283v1)<br>:house:[project](https://iab-rubric.org/resources/other-databases/indictti)
  * [Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion](https://arxiv.org/abs/2407.21032)<br>:star:[code](https://github.com/nannullna/safeguard-hfi)
  * [Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers](http://arxiv.org/abs/2407.08394v1)
  * [DreamDrone: Text-to-Image Diffusion Models are Zero-shot Perpetual View Generators](https://arxiv.org/abs/2312.08746v3)<br>:house:[project](https://hyokong.github.io/dreamdrone-page/)
  * [Harnessing Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02103.pdf)
  * [MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2404.09977)<br>:star:[code](https://github.com/Nithin-GK/MaxFusion)<br>:house:[project](https://nithin-gk.github.io/maxfusion.github.io/)
  * [MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization](https://arxiv.org/abs/2405.17873)<br>:star:[code](https://github.com/A-suozhang/MixDQ)<br>:house:[project](https://a-suozhang.xyz/mixdq.github.io/)
  * [LCM-Lookahead for Encoder-based Text-to-Image Personalization](https://arxiv.org/abs/2404.03620)<br>:star:[code](https://github.com/OrLichter/lcm-lookahead)<br>:house:[project](https://lcm-lookahead.github.io/)
  * [Lego: Learning to Disentangle and Invert Personalized Concepts Beyond Object Appearance in Text-to-Image Diffusion Models](https://arxiv.org/abs/2311.13833)
  * [Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models](http://arxiv.org/abs/2407.12383v1)<br>:star:[code](https://github.com/CharlesGong12/RECE)
  * [Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model](http://arxiv.org/abs/2407.05352v1)<br>:star:[code](https://github.com/nini0919/DiffPNG)<br>:thumbsup:[DiffPNG实现了最佳的性能，证明了T2I扩散模型在短语级理解视觉内容的能力](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [T2IShield: Defending Against Backdoors on Text-to-Image Diffusion Models](http://arxiv.org/abs/2407.04215v1)<br>:star:[code](https://github.com/Robin-WZQ/T2IShield)
  * [Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation](https://arxiv.org/abs/2403.16990)<br>:house:[project](https://omer11a.github.io/bounded-attention/)
  * [Latent Guard: a Safety Framework for Text-to-image Generation](https://arxiv.org/abs/2404.08031)<br>:star:[code](https://github.com/rt219/LatentGuard)
  * [Getting it Right: Improving Spatial Consistency in Text-to-Image Models](https://arxiv.org/abs/2404.01197)<br>:star:[code](https://github.com/SPRIGHT-T2I/SPRIGHT)<br>:house:[project](https://spright-t2i.github.io/)
  * [Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning](http://arxiv.org/abs/2407.06642v1)<br>:star:[code](https://github.com/wfanyue/DPG-T2I-Personalization)
  * [PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion](https://arxiv.org/abs/2312.16486)<br>:house:[project](https://pangu-draw.github.io/)
  * [Text-Anchored Score Composition: Tackling Condition Misalignment in Text-to-Image Diffusion Models](https://arxiv.org/abs/2306.14408)<br>:star:[code](https://github.com/EnVision-Research/Decompose-and-Realign)
  * [Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation](https://arxiv.org/abs/2401.05675)
  * [ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation](https://arxiv.org/abs/2402.15429)
  * [PixArt-Sigma: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04633.pdf)
  * [AnyControl: Create Your Artwork with Versatile Control on Text-to-Image Generation](https://arxiv.org/abs/2406.18958)<br>:star:[code](https://github.com/open-mmlab/AnyControl)<br>:house:[project](https://any-control.github.io/)
  * [Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models](https://arxiv.org/abs/2404.07389)
  * [SpeedUpNet: A Plug-and-Play Adapter Network for Accelerating Text-to-Image Diffusion Models](https://arxiv.org/abs/2312.08887)<br>:house:[project](https://williechai.github.io/speedup-plugin-for-stable-diffusions.github.io)
  * [Stable Preference: Redefining training paradigm of human preference model for Text-to-Image Synthesis](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04077.pdf)
* 图像-文本
  * [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291)<br>:house:[project](https://linzhiqiu.github.io/papers/vqascore/)
  * [Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](https://arxiv.org/abs/2403.09572)<br>:house:[project](https://gyhdog99.github.io/projects/ecso/)
* 文本-视频对齐
  * [Learning to Localize Actions in Instructional Videos with LLM-Based Multi-Pathway Text-Video Alignment](http://arxiv.org/abs/2409.16145v1)
* 图像-文本对齐
  * [Removing Distributional Discrepancies in Captions Improves Image-Text Alignment](https://arxiv.org/abs/2410.00905)<br>:house:[project](https://yuheng-li.github.io/LLaVA-score/)
* 图像-文本
  * [Emergent Visual-Semantic Hierarchies in Image-Text Representations](http://arxiv.org/abs/2407.08521v1)<br>:star:[code](https://hierarcaps.github.io/)<br>:house:[project](https://tau-vailab.github.io/hierarcaps/)
  * [Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment](https://arxiv.org/abs/2312.03766)<br>:star:[code](https://github.com/BrianG13/MismatchQuest)<br>:house:[project](https://mismatch-quest.github.io/)
* 3D(内容)生成
  * [Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation](https://arxiv.org/abs/2403.09625)<br>:house:[project](https://liuff19.github.io/Make-Your-3D/)
  * [LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation](https://arxiv.org/abs/2402.05054)<br>:house:[project](https://me.kiui.moe/lgm/)
  * [LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation](https://arxiv.org/abs/2403.12019)<br>:house:[project](https://nirvanalan.github.io/projects/ln3diff/)
  * [SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion](https://arxiv.org/abs/2403.12008)<br>:house:[project](https://sv3d.github.io/)
  * [VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models](https://arxiv.org/abs/2403.12034)<br>:house:[project](https://junlinhan.github.io/projects/vfusion3d.html)
  * [Compress3D: a Compressed Latent Space for 3D Generation from a Single Image](https://arxiv.org/abs/2403.13524)
  * [AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and Reconstruction with Canonical Score Distillation](https://arxiv.org/abs/2312.03795)<br>:house:[project](https://animatabledreamer.github.io/)
  * [Learn to Optimize Denoising Scores: A Unified and Improved Diffusion Prior for 3D Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06039.pdf)<br>:house:[project](https://yangxiaofeng.github.io/demo_diffusion_prior)
* 视觉文本渲染
  * [Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering](https://arxiv.org/abs/2403.09622)<br>:house:[project](https://glyph-byt5.github.io/)
* GIF 生成
  * [Pix2Gif: Motion-Guided Diffusion for GIF Generation](https://arxiv.org/abs/2403.04634)<br>:house:[project](https://hiteshk03.github.io/Pix2Gif/)
* 布局生成
  * [COHO: Context-Sensitive City-Scale Hierarchical Urban Layout Generation](http://arxiv.org/abs/2407.11294v1)<br>:star:[code](https://github.com/Arking1995/COHO)
  * [Self-training Room Layout Estimation via Geometry-aware Ray-casting](http://arxiv.org/abs/2407.15041v1)
  * [LayoutFlow: Flow Matching for Layout Generation](https://arxiv.org/abs/2403.18187)<br>:house:[project](https://julianguerreiro.github.io/layoutflow/)
* 布局-图像
  * [Training-free Composite Scene Generation for Layout-to-Image Synthesis](http://arxiv.org/abs/2407.13609v1)<br>:star:[code](https://github.com/Papple-F/csg.git)
* 图像-图像翻译
  * [Every Pixel Has its Moments: Ultra-High-Resolution Unpaired Image-to-Image Translation via Dense Normalization](http://arxiv.org/abs/2407.04245v1)<br>:star:[code](https://github.com/Kaminyou/Dense-Normalization)
  * [Diffusion-Based Image-to-Image Translation by Noise Correction via Prompt Interpolation](https://arxiv.org/abs/2409.08077)
  * [EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion Models](https://arxiv.org/abs/2410.09802)
* Text-to-4D
  * [TC4D: Trajectory-Conditioned Text-to-4D Generation](https://arxiv.org/abs/2403.17920)<br>:star:[code](https://github.com/sherwinbahmani/tc4d)<br>:house:[project](https://sherwinbahmani.github.io/tc4d/)
* Video-to-4D
  * [SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer](https://arxiv.org/abs/2404.03736)<br>:star:[code](https://github.com/JarrentWu1031/SC4D)<br>:house:[project](https://sc4d.github.io/)
* 网页设计
  * [WebRPG: Automatic Web Rendering Parameters Generation for Visual Presentation](http://arxiv.org/abs/2407.15502v1)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/WebRPG)
* Text-to-Garment
  * [GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections](https://arxiv.org/abs/2408.12352)
* 图像风格化
  * [StyleTokenizer: Defining Image Style by a Single Instance for Controlling Diffusion Models](http://arxiv.org/abs/2409.02543v1)<br>:star:[code](https://github.com/alipay/style-tokenizer)
  * [ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs](https://arxiv.org/abs/2311.13600)<br>:star:[code](https://github.com/mkshing/ziplora-pytorch)<br>:house:[project](https://ziplora.github.io/)风格
* 图像矢量化
  * [Segmentation-guided Layer-wise Image Vectorization with Gradient Fills](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01546.pdf)
* 视频拼接
  * [Eliminating Warping Shakes for Unsupervised Online Video Stitching](https://arxiv.org/abs/2403.06378)<br>:star:[code](https://github.com/nie-lang/StabStitch)
* 文本到相机轨迹生成
  * [E.T. the Exceptional Trajectory: Text-to-camera-trajectory generation with character awareness](https://arxiv.org/abs/2407.01516)<br>:house:[project](https://www.lix.polytechnique.fr/vista/projects/2024_et_courant/)
* 文本到 3D 场景
  * [DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic Gaussian Splatting](https://arxiv.org/abs/2404.06903)<br>:house:[project](http://dreamscene360.github.io/)
* 身份保留的个性化
  * [Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm](https://arxiv.org/abs/2403.11781)<br>:house:[project](https://infinite-id.github.io/)
  * [ComFusion: Enhancing Personalized Generation by Instance-Scene Compositing and Fusion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06025.pdf)
* 主题驱动生成
  * [HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation](https://arxiv.org/abs/2410.08192)
* 风格内容分离
  * [Implicit Style-Content Separation using B-LoRA](https://arxiv.org/abs/2403.14572)<br>:star:[code](https://github.com/yardenfren1996/B-LoRA)<br>:house:[project](https://b-lora.github.io/B-LoRA/)
* 文本生成多运动
  * [M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models](https://arxiv.org/abs/2407.14502)
* 文本驱动的3D编辑
  * [GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing](https://arxiv.org/abs/2403.08733)<br>:star:[code](https://github.com/ActiveVisionLab/gaussctrl)文本驱动的 3D 高斯泼溅编辑
* 图像插值
  * [DreamMover: Leveraging the Prior of Diffusion Models for Image Interpolation with Large Motion](https://www.arxiv.org/abs/2409.09605)<br>:star:[code](https://github.com/leoShen917/DreamMover)
* 图像合成
  * [FreeCompose: Generic Zero-Shot Image Composition with Diffusion Prior](https://arxiv.org/abs/2407.04947)<br>:star:[code](https://github.com/aim-uofa/FreeCompose)零样本图像合成
* 图像动画
  * [LivePhoto: Real Image Animation with Text-guided Motion Control](https://arxiv.org/abs/2312.02928)<br>:star:[code](https://github.com/XavierCHEN34/LivePhoto)文本引导运动控制的真实图像动画
  * [MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model](https://arxiv.org/abs/2405.20222)<br>:star:[code](https://github.com/MyNiuuu/MOFA-Video)<br>:house:[project](https://myniuuu.github.io/MOFA_Video/)
  * [ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06174.pdf)<br>:house:[project](https://gen-l-2.github.io/)
* 集体照合成
  * [AddMe: Zero-shot Group-photo Synthesis by Inserting People into Scenes](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03028.pdf)集体照合成



## Image Captioning(图像/视频字幕)
* [DECap: Towards Generalized Explicit Caption Editing via Diffusion Mechanism](https://arxiv.org/abs/2311.14920)
* [ControlCap: Controllable Region-level Captioning](https://arxiv.org/abs/2401.17910)<br>:star:[code](https://github.com/callsys/ControlCap)字幕
* [MarineInst: A Foundation Model for Marine Image Analysis with Instance Visual Description](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00223.pdf)<br>:star:[code](https://github.com/zhengziqiang/MarineInst20M)视觉描述
* [Distractors-Immune Representation Learning with Cross-modal Contrastive Regularization for Change Captioning](http://arxiv.org/abs/2407.11683v1)<br>:star:[code](https://github.com/tuyunbin/DIRL)
* [CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation](http://arxiv.org/abs/2407.11393v1)<br>:star:[code](https://github.com/SamsungLabs/CIC-BART-SSA)
* [Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights](http://arxiv.org/abs/2407.11449v1)<br>:star:[code](https://github.com/ShunqiM/Ctrl-CIC)
* [BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues](http://arxiv.org/abs/2407.20341v1)<br>:star:[code](https://github.com/aimagelab/bridge-score)
* [View Selection for 3D Captioning via Diffusion Ranking](https://arxiv.org/abs/2404.07984)<br>:star:[code](https://github.com/tiangeluo/DiffuRank)
* 密集字幕
  * [TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes](https://arxiv.org/abs/2403.19589)<br>:star:[code](https://github.com/jxbbb/TOD3Cap)
  * [Bi-directional Contextual Attention for 3D Dense Captioning](https://arxiv.org/abs/2408.06662)

## Image/video Compression(图像/视频压缩)
* [Adaptive Selection of Sampling-Reconstruction in Fourier Compressed Sensing](http://arxiv.org/abs/2409.11738v1)
* [Image Compression for Machine and Human Vision with Spatial-Frequency Adaptation](http://arxiv.org/abs/2407.09853v1)<br>:star:[code](https://github.com/qingshi9974/ECCV2024-AdpatICMH)
* [Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model](http://arxiv.org/abs/2407.10632v1)
* [Rate-Distortion-Cognition Controllable Versatile Neural Image Compression](http://arxiv.org/abs/2407.11700v1)
* [BaSIC: BayesNet Structure Learning for Computational Scalable Neural Image Compression](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03640.pdf)<br>:star:[code](https://github.com/worldlife123/cbench_BaSIC)
* [Region-Adaptive Transform with Segmentation Prior for Image Compression](https://arxiv.org/abs/2403.00628)<br>:star:[code](https://github.com/GityuxiLiu/SegPIC-for-Image-Compression)
* [EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation](https://arxiv.org/abs/2309.03244)
* [Lagrangian Hashing for Compressed Neural Field Representations](https://arxiv.org/abs/2409.05334)<br>:house:[project](https://theialab.github.io/laghashes/)
* [Latent Diffusion Prior Enhanced Deep Unfolding for Snapshot Spectral Compressive Imaging](https://arxiv.org/abs/2311.14280)<br>:star:[code](https://github.com/Zongliang-Wu/LADE-DUN)快照光谱压缩
* 视频压缩
  * [Hierarchical Separable Video Transformer for Snapshot Compressive Imaging](http://arxiv.org/abs/2407.11946v1)<br>:star:[code](https://github.com/pwangcs/HiSViT)
  * [A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging](http://arxiv.org/abs/2407.21517v1)<br>:star:[code](https://github.com/mcao92/QuantizedSCI)
  * [Free-VSC: Free Semantics from Visual Foundation Models for Unsupervised Video Semantic Compression](http://arxiv.org/abs/2409.11718v1)

## Image Retrieval(图像检索)
* [RGNet: A Unified Clip Retrieval and Grounding Network for Long Videos](https://arxiv.org/pdf/2312.06729)<br>:star:[code](https://github.com/Tanveer81/RGNet)<br>:house:[project](https://sites.google.com/view/rgnet/home)
* [AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval](http://arxiv.org/abs/2408.03282v1)<br>:star:[code](https://github.com/pavelsuma/ames)
* [IRGen: Generative Modeling for Image Retrieval](https://arxiv.org/abs/2303.10126)<br>:star:[code](https://github.com/yakt00/IRGen)
* [FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos](https://arxiv.org/abs/2403.15161)
* [Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval](https://arxiv.org/abs/2405.00571)<br>:star:[code](https://github.com/youngkyunJang/SLERP-TAT)
* [FreestyleRet: Retrieving Images from Style-Diversified Queries](https://arxiv.org/abs/2312.02428)<br>:star:[code](https://github.com/CuriseJia/FreeStyleRet)
* 基于草图的图像检索
  * [Freeview Sketching: View-Aware Fine-Grained Sketch-Based Image Retrieval](http://arxiv.org/abs/2407.01810v1)
  * [Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt Learning](http://arxiv.org/abs/2407.04207v1)<br>:star:[code](https://github.com/mainaksingha01/SpLIP)
* 视频-文本检索
  * [EA-VTR: Event-Aware Video-Text Retrieval](http://arxiv.org/abs/2407.07478v1)
  * [Rethinking Video-Text Understanding: Retrieval from Counterfactually Augmented Data](http://arxiv.org/abs/2407.13094v1)<br>:star:[code](https://feint6k.github.io)
  * [KDProR: A Knowledge-Decoupling Probabilistic Framework for Video-Text Retrieval](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05033.pdf)
* 图像-文本检索
  * [Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval](http://arxiv.org/abs/2407.12346v1)
* 视频检索
  * [EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval](http://arxiv.org/abs/2407.16658v1)<br>:star:[code](https://github.com/ExplainableML/EgoCVR)
  * [Uncertainty-aware sign language video retrieval with probability distribution modeling](https://arxiv.org/abs/2405.19689)<br>:star:[code](https://github.com/xua222/UPRet)
* 近邻搜索
  * [Group Testing for Accurate and Efficient Range-Based Near Neighbor Search for Plagiarism Detection](https://arxiv.org/abs/2311.02573)

## Image Segmentation(图像分割)
* [Occlusion-Aware Seamless Segmentation](http://arxiv.org/abs/2407.02182v1)<br>:star:[code](https://github.com/yihong-97/OASS)
* [SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding](http://arxiv.org/abs/2407.03200v1)<br>:star:[code](https://github.com/WeitaiKang/SegVG)
* [Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts](http://arxiv.org/abs/2407.11382v1)
* [Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively](https://arxiv.org/abs/2401.02955)<br>:star:[code](https://github.com/HarborYuan/ovsam)<br>:house:[project](https://www.mmlab-ntu.com/project/ovsam)
* [Enriching Information and Preserving Semantic Consistency in Expanding Curvilinear Object Segmentation Datasets](http://arxiv.org/abs/2407.08209v1)<br>:star:[code](https://github.com/tanlei0/COSTG)
* [CC-SAM: Enhancing SAM with Cross-feature Attention and Context for Ultrasound Image Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06124.pdf)
* [Unsupervised Moving Object Segmentation with Atmospheric Turbulence](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00851.pdf)
* [Lite-SAM Is Actually What You Need for Segment Everything](http://arxiv.org/abs/2407.08965v1)
* [Textual Query-Driven Mask Transformer for Domain Generalized Segmentation](http://arxiv.org/abs/2407.09033v1)<br>:star:[code](https://byeonghyunpak.github.io/tqdm)
* [Can Textual Semantics Mitigate Sounding Object Segmentation Preference?](http://arxiv.org/abs/2407.10947v1)<br>:star:[code](https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference)
* [RAPiD-Seg: Range-Aware Pointwise Distance Distribution Networks for 3D LiDAR Segmentation](http://arxiv.org/abs/2407.10159v1)<br>:star:[code](https://github.com/l1997i/rapid_seg)
* [SPIN: Hierarchical Segmentation with Subpart Granularity in Natural Images](http://arxiv.org/abs/2407.09686v1)<br>:star:[code](https://joshmyersdean.github.io/spin/index.html)
* [CC-SAM: SAM with Cross-feature Attention and Context for Ultrasound Image Segmentation](http://arxiv.org/abs/2408.00181v1)
* [FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally](http://arxiv.org/abs/2409.08270v1)<br>:star:[code](https://github.com/florinshen/FlashSplat)
* [SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis](https://arxiv.org/abs/2311.03355)<br>:house:[project](https://seggenerator.github.io/)
* [PQ-SAM: Post-training Quantization for Segment Anything Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01627.pdf)
* [Active Coarse-to-Fine Segmentation of Moveable Parts from Real Images](https://arxiv.org/abs/2303.11530)<br>:house:[project](https://suikei-wang.github.io/mvp-seg/)
* [LiteSAM is Actually what you Need for segment Everything](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05077.pdf)
* [SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation](https://arxiv.org/abs/2311.14671)<br>:star:[code](https://github.com/MengLcool/SEGIC)
* [A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties](https://arxiv.org/abs/2312.13764)<br>:star:[code](https://github.com/lambert-x/ProLab)
* [Placing Objects in Context via Inpainting for Out-of-distribution Segmentation](https://arxiv.org/abs/2402.16392)<br>:star:[code](https://github.com/naver/poc)
* [Rethinking and Improving Visual Prompt Selection for In-Context Learning Segmentation Framework](https://arxiv.org/abs/2407.10233v1)<br>:star:[code](https://github.com/LanqingL/SCS)
* [Better Call SAL: Towards Learning to Segment Anything in Lidar](https://arxiv.org/abs/2403.13129)<br>:star:[code](https://github.com/nv-dvl/segment-anything-lidar)
* 抠图
  * [DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation](https://arxiv.org/pdf/2403.06168)<br>:star:[code](https://github.com/HUuxiaobin/DiffuMatting)<br>:house:[project](https://diffumatting.github.io/)
  * [COIN-Matting: Confounder Intervention for Image Matting](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02943.pdf)
* 3D分割
  * [Bayesian Self-Training for Semi-Supervised 3D Segmentation](http://arxiv.org/abs/2409.08102v1)
  * [Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels](https://arxiv.org/abs/2312.17232)<br>:house:[project](http://segment3d.github.io/)
  * [EgoLifter: Open-world 3D Segmentation for Egocentric Perception](https://arxiv.org/abs/2403.18118)<br>:house:[project](https://egolifter.github.io/)<br>🤗[huggingface](https://huggingface.co/papers/2403.18118)
* 视频分割
  * [General and Task-Oriented Video Segmentation](http://arxiv.org/abs/2407.06540v1)<br>:star:[code](https://github.com/kagawa588/GvSeg)
  * [DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries](https://arxiv.org/abs/2404.00086)<br>:star:[code](https://github.com/zhang-tao-whu/DVIS_Plus)<br>:house:[project](https://zhang-tao-whu.github.io/projects/DVIS_DAQ/)
* 实例分割
  * [Unleashing the Power of Prompt-driven Nucleus Instance Segmentation](https://arxiv.org/abs/2311.15939)<br>:star:[code](https://github.com/windygoo/PromptNucSeg)
  * 3D实例分割
    * [Part2Object: Hierarchical Unsupervised 3D Instance Segmentation](http://arxiv.org/abs/2407.10084v1)<br>:star:[code](https://github.com/ChengShiest/Part2Object)
  * 无监督实例分割
    * [ProMerge: Prompt and Merge for Unsupervised Instance Segmentation](https://arxiv.org/abs/2409.18961)
  * 开发世界实例分割
    * [SOS: Segment Object System for Open-World Instance Segmentation With Object Priors](http://arxiv.org/abs/2409.14627v1)<br>:star:[code](https://github.com/chwilms/SOS)
* 全景分割
  * [Open Panoramic Segmentation](http://arxiv.org/abs/2407.02685v1)<br>:star:[code](https://junweizheng93.github.io/publications/OPS/OPS.html)
  * [A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting](https://arxiv.org/abs/2401.10227)<br>:star:[code](https://github.com/segments-ai/latent-diffusion-segmentation)
  * [Point-supervised Panoptic Segmentation via Estimating Pseudo Labels from Learnable Distance](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02430.pdf)
  * [Strike a Balance in Continual Panoptic Segmentation](https://arxiv.org/abs/2407.16354)<br>:star:[code](https://github.com/jinpeng0528/BalConpas)
  * [3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation](https://arxiv.org/abs/2401.02402)
* 语义分割
  * [Rethinking Data Augmentation for Robust LiDAR Semantic Segmentation in Adverse Weather](http://arxiv.org/abs/2407.02286v1)<br>:star:[code](https://github.com/engineerJPark/LiDARWeather)
  * [Exploring Reliable Matching with Phase Enhancement for Night-time Semantic Segmentation](https://arxiv.org/abs/2408.13838)
  * [MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment](http://arxiv.org/abs/2407.21654v1)
  * [Sparse Refinement for Efficient High-Resolution Semantic Segmentation](http://arxiv.org/abs/2407.19014v1)<br>:house:[project](https://sparserefine.mit.edu)
  * [Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation](http://arxiv.org/abs/2407.17261v1)<br>:star:[code](https://github.com/hyunwoo137/EDAFormer)
  * [On the Viability of Monocular Depth Pre-training for Semantic Segmentation](https://arxiv.org/abs/2203.13987)
  * [Efficient Active Domain Adaptation for Semantic Segmentation by Selecting Information-rich Superpixels](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05060.pdf)<br>:star:[code](https://github.com/EdenHazardan/ADA_superpixel)
  * [Distributed Semantic Segmentation with Efficient Joint Source and Task Decoding](http://arxiv.org/abs/2407.11224v1)
  * [FREST: Feature RESToration for Semantic Segmentation under Multiple Adverse Conditions](http://arxiv.org/abs/2407.13437v1)
  * [Make a Strong Teacher with Label Assistance: A Novel Knowledge Distillation Approach for Semantic Segmentation](http://arxiv.org/abs/2407.13254v1)<br>:star:[code](https://github.com/skyshoumeng/Label_Assisted_Distillation)
  * [ItTakesTwo: Leveraging Peer Representations for Semi-supervised LiDAR Semantic Segmentation](http://arxiv.org/abs/2407.07171v1)<br>:star:[code](https://github.com/yyliu01/IT2)
  * [Evaluating the Adversarial Robustness of Semantic Segmentation: Trying Harder Pays Off](http://arxiv.org/abs/2407.09150v1)<br>:star:[code](https://github.com/szegedai/Robust-Segmentation-Evaluation)
  * [Centering the Value of Every Modality: Towards Efficient and Resilient Modality-agnostic Semantic Segmentation](http://arxiv.org/abs/2407.11344v1)
  * [Learning Modality-agnostic Representation for Semantic Segmentation from Any Modalities](http://arxiv.org/abs/2407.11351v1)
  * [SFPNet: Sparse Focal Point Network for Semantic Segmentation on General LiDAR Point Clouds](http://arxiv.org/abs/2407.11569v1)<br>:house:[project](https://www.semanticindustry.top)<br>:star:[code](https://github.com/Cavendish518/SFPNet)
  * [Reliability in Semantic Segmentation: Can We Use Synthetic Data?](https://arxiv.org/abs/2312.09231)<br>:star:[code](https://github.com/valeoai/GenVal)
  * [Cs2K: Class-specific and Class-shared Knowledge Guidance for Incremental Semantic Segmentation](https://arxiv.org/abs/2407.09047)
  * [MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture Synthesis](http://arxiv.org/abs/2407.13675v1)<br>:star:[code](https://github.com/zimingzhong/MeshSegmenter)
  * 3D语义分割
    * [Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models](http://arxiv.org/abs/2407.13642v1)
    * [LASS3D: Language-Assisted Semi-Supervised 3D Semantic Segmentation with Progressive Unreliable Data Exploitation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00378.pdf)
  * 跨域语义分割
    * [Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs](https://arxiv.org/abs/2408.02261)
    * [Attention Decomposition for Cross-Domain Semantic Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02238.pdf)
  * 无监督语义分割
    * [Progressive Proxy Anchor Propagation for Unsupervised Semantic Segmentation](http://arxiv.org/abs/2407.12463v1)
  * 半监督语义分割
    * [Beyond Pixels: Semi-Supervised Semantic Segmentation with a Multi-scale Patch-based Multi-Label Classifier](http://arxiv.org/abs/2407.04036v1)
    * [Weighting Pseudo-Labels via High-Activation Feature Index Similarity and Object Detection for Semi-Supervised Segmentation](http://arxiv.org/abs/2407.12630v1)
    * [SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance](https://arxiv.org/abs/2311.16241)<br>:star:[code](https://github.com/google-research/semivl)
  * 弱监督语义分割
    * [Knowledge Transfer with Simulated Inter-Image Erasing for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2407.02768v1)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/KTSE)<br>通过模拟图像间擦除实现知识转移，弱监督语义分割再也不怕过扩展问题，助力精准目标定位！
    * [Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation](https://arxiv.org/abs/2404.11981)
    * [DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2409.15801v1)
    * [Finding Meaning in Points: Weakly Supervised Semantic Segmentation for Event Cameras](http://arxiv.org/abs/2407.11216v1)<br>:star:[code](https://github.com/Chohoonhee/EV-WSSS)
    * [3D weakly supervised semantic segmentation with 2D vision-language guidance]
    * [Learning from the Web: Language Drives Weakly-Supervised Incremental Learning for Semantic Segmentation](http://arxiv.org/abs/2407.13363v1)
    * [Phase Concentration and Shortcut Suppression for Weakly Supervised Semantic Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04729.pdf)<br>:star:[code](https://github.com/kwonhoyong3/PCSS-WSSS)
  * 域适应语义分割
    * [MICDrop: Masking Image and Depth Features via Complementary Dropout for Domain-Adaptive Semantic Segmentation](https://arxiv.org/abs/2408.16478)
  * 域泛化语义分割
    * [DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control](https://arxiv.org/abs/2312.03048)<br>:house:[project](https://dginstyle.github.io/)
  * 类增量语义分割
    * [Background Adaptation with Residual Modeling for Exemplar-Free Class-Incremental Semantic Segmentation](http://arxiv.org/abs/2407.09838v1)<br>:star:[code](https://andyzaq.github.io/barmsite/)
    * [Mitigating Background Shift in Class-Incremental Semantic Segmentation](http://arxiv.org/abs/2407.11859v1)<br>:star:[code](http://github.com/RoadoneP/ECCV2024_MBS)
    * [Early Preparation Pays Off: New Classifier Pre-tuning for Class Incremental Semantic Segmentation](http://arxiv.org/abs/2407.14142v1)<br>:star:[code](https://github.com/zhengyuan-xie/ECCV24_NeST)
  * 零样本语义分割
    * [AlignZeg: Mitigating Objective Misalignment for Zero-shot Semantic Segmentation](https://arxiv.org/abs/2404.05667)
  * 开放词汇语义分割
    * [CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation](https://arxiv.org/abs/2312.12359)<br>:star:[code](https://github.com/wysoczanska/clip_dinoiser/)<br>:house:[project](https://wysoczanska.github.io/CLIP_DINOiser/)
    * [Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation](http://arxiv.org/abs/2407.08268v1)<br>:star:[code](https://github.com/leaves162/CLIPtrase)
    * [In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2408.04961)
    * [ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation](https://arxiv.org/abs/2408.04883)<br>:star:[code](https://github.com/mc-lan/ProxyCLIP)
* 部分分割
  * [3x2: 3D Object Part Segmentation by 2D Semantic Correspondences](http://arxiv.org/abs/2407.09648v1)<br>:star:[code](https://ngailapdi.github.io/projects/3by2/)
  * [WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models](https://arxiv.org/abs/2407.10131)
  * [PartSTAD: 2D-to-3D Part Segmentation Task Adaptation](https://arxiv.org/abs/2401.05906)<br>:star:[code](https://github.com/KAIST-Visual-AI-Group/PartSTAD)
* 运动分割
  * [Un-EVIMO: Unsupervised Event-based Independent Motion Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02459.pdf)<br>:house:[project](https://www.cis.upenn.edu/~ziyunw/un_evimo/)
  * [Appearance-based Refinement for Object-Centric Motion Segmentation](https://arxiv.org/abs/2312.11463)<br>:house:[project](https://www.robots.ox.ac.uk/vgg/research/appear-refine/)
* 烟雾分割
  * [DSA: Discriminative Scatter Analysis for Early Smoke Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06090.pdf)
* 场景解析
  * [OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part Scene Parsing](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04338.pdf)场景解析
* 交互式分割
  * [Click-Gaussian: Interactive Segmentation to Any 3D Gaussians](http://arxiv.org/abs/2407.11793v1)<br>:star:[code](https://seokhunchoi.github.io/Click-Gaussian)
  * [Click Prompt Learning with Optimal Transport for Interactive Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04816.pdf)<br>:house:[project](https://jliu4ai.github.io/cplot_projectpage/)
* 小样本分割
  * [Eliminating Feature Ambiguity for Few-Shot Segmentation](http://arxiv.org/abs/2407.09842v1)<br>:star:[code](https://github.com/Sam1224/AENet)
  * [CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model](https://arxiv.org/abs/2402.03631)<br>:house:[project](https://xiaoaoran.github.io/projects/CAT-SAM)
  * [AlignDiff: Aligning Diffusion Models for General Few-Shot Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05794.pdf)
* 参考图像分割
  * [Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation](http://arxiv.org/abs/2407.07412v1)
* 指代图像分割
  * [ReMamber: Referring Image Segmentation with Mamba Twister](https://arxiv.org/abs/2403.17839)<br>:star:[code](https://github.com/yyh-rain-song/ReMamber)
* 场景文本分割
  * [EAFormer: Scene Text Segmentation with Edge-Aware Transformers](http://arxiv.org/abs/2407.17020v1)<br>:house:[project](https://hyangyu.github.io/EAFormer/)
* 开放词汇分割
  * [Diffusion Models for Open-Vocabulary Segmentation](https://arxiv.org/abs/2306.09316)
  * [Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation](http://arxiv.org/abs/2408.00744v1)<br>:star:[code](https://github.com/jiaosiyu1999/MAFT-Plus.git)
* 指代表达式分割
  * [SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation](http://arxiv.org/abs/2407.02389v1)
  * [SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation](http://arxiv.org/abs/2409.10542v1)
  * [SAFARI: Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06102.pdf)<br>:house:[project](https://sayannag.github.io/safari_eccv2024/)
* VIS
  * [VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement](https://arxiv.org/abs/2312.04885)<br>:star:[code](https://github.com/KimHanjung/VISAGE)
  * [Unified Embedding Alignment for Open-Vocabulary Video Instance Segmentation](http://arxiv.org/abs/2407.07427v1)<br>:star:[code](https://github.com/fanghaook/OVFormer)
* VOS
  * [ActionVOS: Actions as Prompts for Video Object Segmentation](http://arxiv.org/abs/2407.07402v1)<br>:star:[code](https://github.com/ut-vision/ActionVOS)  
  * [VISA: Reasoning Video Object Segmentation via Large Language Model](https://arxiv.org/abs/2407.11325)<br>:star:[code](https://github.com/cilinyan/VISA)
  * [PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation](https://arxiv.org/abs/2309.12303)<br>:star:[code](https://github.com/shilinyan99/PanoVOS)
  * [Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation](https://arxiv.org/abs/2403.12042)<br>:star:[code](https://github.com/buxiangzhiren/VD-IT)
  * [Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation](https://arxiv.org/abs/2311.17893)<br>:star:[code](https://github.com/shvdiwnkozbw/SSL-UVOS)




## Image Classification(图像分类)
* [Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs](https://arxiv.org/abs/2403.11755)<br>:house:[project](https://jmiemirza.github.io/Meta-Prompting/)
* [Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition](http://arxiv.org/abs/2407.14302v1)
* [Wavelet Convolutions for Large Receptive Fields](http://arxiv.org/abs/2407.05848v1)<br>:star:[code](https://github.com/BGU-CS-VIL/WTConv)
* [Momentum Auxiliary Network for Supervised Local Learning](http://arxiv.org/abs/2407.05623v1)<br>:star:[code](https://github.com/JunhaoSu0/MAN)
* [An accurate detection is not all you need to combat label noise in web-noisy datasets](http://arxiv.org/abs/2407.05528v1)
* [Dual-stage Hyperspectral Image Classification Model with Spectral Supertoken](http://arxiv.org/abs/2407.07307v1)<br>:star:[code](https://github.com/laprf/DSTC)
* [DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks](http://arxiv.org/abs/2407.14509v1)
* [NOVUM: Neural Object Volumes for Robust Object Classification](https://arxiv.org/abs/2305.14668)<br>:star:[code](https://github.com/GenIntel/NOVUM)
* [EntAugment: Entropy-Driven Adaptive Data Augmentation Framework for Image Classification](http://arxiv.org/abs/2409.06290v1)<br>:star:[code](https://github.com/Jackbrocp/EntAugment)
* [Distribution-Aware Robust Learning from Long-Tailed Data with Noisy Labels](https://arxiv.org/abs/2407.16802)<br>:star:[code](https://github.com/JaesoonBaik1213/DaSC)
* 多标签图像分类
  * [Distributionally Robust Loss for Long-Tailed Multi-Label Image Classification](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04926.pdf)<br>:star:[code](https://github.com/Kunmonkey/DR-Loss)
* 小样本分类
  * [Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882v1)<br>:star:[code](https://github.com/gtzheng/FewSTAB)
  * [Learning to Obstruct Few-Shot Image Classification over Restricted Classes](https://arxiv.org/abs/2409.19210)<br>:star:[code](https://github.com/amberyzheng/LTO)
* 零样本分类
  * [Online Zero-Shot Classification with CLIP](http://arxiv.org/abs/2408.13320v1)<br>:star:[code](https://github.com/idstcv/OnZeta)
* 多标签识别
  * [Modeling Label Correlations with Latent Context for Multi-Label Recognition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04855.pdf)
* 长尾识别
  * [LTRL: Boosting Long-tail Recognition via Reflective Learning](http://arxiv.org/abs/2407.12568v1)
* 细粒度
  * [On Learning Discriminative Features from Synthesized Data for Self-Supervised Fine-Grained Visual Recognition](http://arxiv.org/abs/2407.14676v1)
  * [A Rotation-invariant Texture ViT for Fine-Grained Recognition of Esophageal Cancer Endoscopic Ultrasound Images](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04582.pdf)<br>:star:[code](https://github.com/tianyiliu-lab/SRRM-ViT/)
  * [Adapting Fine-Grained Cross-View Localization to Areas without Fine Ground Truth](https://arxiv.org/abs/2406.00474)

## Image Progress(图像/视频处理)
* [ReNoise: Real Image Inversion Through Iterative Noising](https://arxiv.org/abs/2403.14602)<br>:star:[code](https://github.com/garibida/ReNoise-Inversion)<br>:house:[project](https://garibida.github.io/ReNoise-Inversion/)
* 恢复
  * [MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration](http://arxiv.org/abs/2407.10833v1)<br>:star:[code](https://renyulin-f.github.io/MoE-DiffIR.github.io/)
  * [GRIDS: Grouped Multiple-Degradation Restoration with Image Degradation Similarity](https://arxiv.org/abs/2407.12273)
  * [Restoring Images in Adverse Weather Conditions via Histogram Transformer](https://arxiv.org/abs/2407.10172)
  * [InstructIR: High-Quality Image Restoration Following Human Instructions](https://arxiv.org/abs/2401.16468)<br>:star:[code](https://github.com/mv-lab/InstructIR)
  * [Diffusion Prior-Based Amortized Variational Inference for Noisy Inverse Problems](http://arxiv.org/abs/2407.16125v1)<br>:star:[code](https://github.com/mlvlab/DAVI)
  * [Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101v1)
  * [Teaching Tailored to Talent: Adverse Weather Restoration via Prompt Pool and Depth-Anything Constraint](http://arxiv.org/abs/2409.15739v1)
  * [Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration](https://arxiv.org/abs/2404.00288)<br>:star:[code](https://github.com/joshyZhou/FPro)
  * [MambaIR: A Simple Baseline for Image Restoration with State-Space Model](https://arxiv.org/abs/2402.15648)<br>:star:[code](https://github.com/csguoh/MambaIR)<br>:thumbsup:[MambaIR: 基于Mamba的图像复原基准模型](https://zhuanlan.zhihu.com/p/684248751)
  * [AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion](https://arxiv.org/abs/2310.10123)<br>:star:[code](https://github.com/jiangyitong/AutoDIR)
  * [SPIRE: Semantic Prompt-Driven Image Restoration](https://arxiv.org/abs/2312.11595)<br>:house:[project](https://chenyangqiqi.github.io/tip/)
  * [Efficient Cascaded Multiscale Adaptive Network for Image Restoration](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02840.pdf)
  * [Restore Anything with Masks: Leveraging Mask Image Modeling for Blind All-in-One Image Restoration](https://arxiv.org/abs/2409.19403v1)<br>:star:[code](https://github.com/Dragonisss/RAM)
  * [When Fast Fourier Transform Meets Transformer for Image Restoration](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06190.pdf)<br>:star:[code](https://github.com/deng-ai-lab/SFHformer)
* 修补
  * [A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting](https://arxiv.org/abs/2312.03594)<br>:star:[code](https://github.com/open-mmlab/PowerPaint)<br>:house:[project](https://powerpaint.github.io/)
  * [Improving Text-guided Object Inpainting with Semantic Pre-inpainting](http://arxiv.org/abs/2409.08260v1)<br>:star:[code](https://github.com/Nnn-s/CATdiffusion)
  * [BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion](https://arxiv.org/abs/2403.06976)<br>:house:[project](https://tencentarc.github.io/BrushNet/)
* 去雨
  * [Efficient Frequency-Domain Image Deraining with Contrastive Regularization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05751.pdf)<br>:star:[code](https://github.com/deng-ai-lab/FADformer)
* 去噪
  * [TTT-MIM: Test-Time Training with Masked Image Modeling for Denoising Distribution Shifts](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01921.pdf)<br>:star:[code](https://github.com/MLI-lab/TTT_Denoising)
  * [Exploiting Dual-Correlation for Multi-frame Time-of-Flight Denoising](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03302.pdf)<br>:star:[code](https://github.com/gtdong-ustc/multi-frame-tof-denoising)
  * [EDformer: Transformer-Based Event Denoising Across Varied Noise Levels](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03905.pdf)
  * [denoiSplit: a method for joint microscopy image splitting and unsupervised denoising](https://arxiv.org/abs/2403.11854)去噪
  * [Asymmetric Mask Scheme for Self-Supervised Real Image Denoising](https://arxiv.org/abs/2407.06514)<br>:star:[code](https://github.com/lll143653/amsnet)
* 去雾
  * [Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06072.pdf)
* 去模糊
  * [Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions](http://arxiv.org/abs/2409.17988v1)<br>:star:[code](https://wengflow.github.io/deblur-e-nerf)
  * [UniINR: Event-guided Unified Rolling Shutter Correction, Deblurring, and Interpolation](https://arxiv.org/abs/2305.15078)<br>:star:[code](https://github.com/yunfanLu/UniINR)
  * [Blind image deblurring with noise-robust kernel estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03024.pdf)<br>:star:[code](https://github.com/csleemooo/BD_noise_robust_kernel_estimation)
  * [Motion Aware Event Representation-driven Image Deblurring](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06299.pdf)(https://github.com/ZhijingS/DA_event_deblur)
  * [BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting](https://arxiv.org/abs/2403.11831)<br>:star:[code](https://github.com/WU-CVGL/BAD-Gaussians)
* 去卷积
  * [Blind Image Deconvolution by Generative-based Kernel Prior and Initializer via Latent Encoding](http://arxiv.org/abs/2407.14816v1)<br>:star:[code](https://github.com/jtaoz/GKPILE-Deconvolution)
* 去反射
  * [L-DiffER: Single Image Reflection Removal with Language-based Diffusion Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02988.pdf)消除单图像反射
* 去伪影
  * [OAPT: Offset-Aware Partition Transformer for Double JPEG Artifacts Removal](https://arxiv.org/abs/2408.11480)<br>:star:[code](https://github.com/QMoQ/OAPT)
* 去摩尔纹
  * [Image Demoireing in RAW and sRGB Domains](https://arxiv.org/abs/2312.09063)<br>:star:[code](https://github.com/rebeccaeexu/RRID)
* 去马赛克
  * [Wavelength-Embedding-guided Filter-Array Transformer for Spectral Demosaicing](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02182.pdf)去马赛克
* 目标移除
  * [GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal](https://arxiv.org/abs/2404.13679)<br>:house:[project](https://w-ted.github.io/publications/gscream/)
* 扩图
  * [Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation](https://arxiv.org/abs/2403.13745)<br>:star:[code](https://github.com/G-U-N/Be-Your-Outpainter)
* 图像增强
  * [LightenDiffusion: Unsupervised Low-Light Image Enhancement with Latent-Retinex Diffusion Models](http://arxiv.org/abs/2407.08939v1)<br>:star:[code](https://github.com/JianghaiSCU/LightenDiffusion)
  * [NamedCurves: Learned Image Enhancement via Color Naming](http://arxiv.org/abs/2407.09892v1)
  * [Joint RGB-Spectral Decomposition Model Guided Image Enhancement in Mobile Photography](https://arxiv.org/abs/2407.17996)<br>:star:[code](https://github.com/CalayZhou/JDM-HDRNet)
  * [GLARE: Low Light Image Enhancement via Generative Latent Feature based Codebook Retrieval](http://arxiv.org/abs/2407.12431v1)<br>:star:[code](https://github.com/LowLevelAI/GLARE)<br>:thumbsup:[GLARE 利用外部正常光照先验，实现逼真的低光照增强效果！](https://mp.weixin.qq.com/s/aot-cr0fOZTyFFoNuVGi0w)
  * [Fast Context-Based Low-Light Image Enhancement via Neural Implicit Representations](http://arxiv.org/abs/2407.12511v1)<br>:star:[code](https://github.com/ctom2/colie)
  * [Unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement](http://arxiv.org/abs/2409.01641v1)<br>:star:[code](https://github.com/redrock303/ADF-LLIE)
* 图像质量评估
  * [DSMix: Distortion-Induced Sensitivity Map Based Pre-training for No-Reference Image Quality Assessment](http://arxiv.org/abs/2407.03886v1)<br>:star:[code](https://github.com/I2-Multimedia-Lab/DSMix)
  * [Towards Open-ended Visual Quality Comparison](https://arxiv.org/abs/2402.16641)<br>:star:[code](https://huggingface.co/q-future/co-instruct)
  * [PromptIQA: Boosting the Performance and Generalization for No-Reference Image Quality Assessment via Prompts](https://arxiv.org/abs/2403.04993)无参考图像质量评估
  * [Assessing UHD Image Quality from Aesthetics, Distortions, and Saliency](http://arxiv.org/abs/2409.00749v1)<br>:star:[code](https://github.com/sunwei925/UIQA)
* 图像美学质量评价
  * [Scaling Up Personalized Image Aesthetic Assessment via Task Vector Customization](https://arxiv.org/abs/2407.07176)<br>:house:[project](https://yeolj00.github.io/personal-projects/personalized-aesthetics/)图像美学评估
* 视频恢复
  * [Quanta Video Restoration](https://arxiv.org/abs/2410.14994)<br>:star:[code](https://github.com/chennuriprateek/Quanta_Video_Restoration-QUIVER-)
* 视频着色
  * [ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization](https://arxiv.org/abs/2404.06251)<br>:star:[code](https://github.com/yyang181/colormnet)
* 视频增强
  * [Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models](http://arxiv.org/abs/2407.10285v1)<br>:star:[code](https://yangqy1110.github.io/NC-SDEdit/)<br>:star:[code](https://github.com/yangqy1110/NC-SDEdit/)
  * [Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video Enhancement](https://www.arxiv.org/abs/2408.12316)<br>:star:[code](https://github.com/lingyzhu0101/UDU.git)
* 视频去雪
  * [Semi-Supervised Video Desnowing Network via Temporal Decoupling Experts and Distribution-Driven Contrastive Regularization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01500.pdf)<br>:star:[code](https://github.com/TonyHongtaoWu/SemiVDN)去雪
* 视频去模糊
  * [Domain-adaptive Video Deblurring via Test-time Blurring](http://arxiv.org/abs/2407.09059v1)<br>:star:[code](https://github.com/Jin-Ting-He/DADeblur)
  * [Cross-Modal Temporal Alignment for Event-guided Video Deblurring](http://arxiv.org/abs/2408.14930v1)<br>:star:[code](https://github.com/intelpro/CMTA)
  * [Towards Real-world Event-guided Low-light Video Enhancement and Deblurring](http://arxiv.org/abs/2408.14916v1)<br>:star:[code](https://github.com/intelpro/ELEDNet)
  * [Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model](https://arxiv.org/abs/2408.13459)<br>:star:[code](https://github.com/Chen-Rao/VD-Diff)
* 视频去闪烁
  * [BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering](https://arxiv.org/pdf/2403.06243v1)<br>:thumbsup:[盲视频去闪烁通用方法BlazeBVD来了，美图&国科大联合提出](https://mp.weixin.qq.com/s/8i_n-2DTU9UgvKtKG-v-6w)
* 视频重照明
  * [Personalized Video Relighting With an At-Home Light Stage](https://arxiv.org/abs/2311.08843)<br>:star:[code](https://github.com/chedgekorea/relighting)


## Super-Resolution(超分辨率)
* [Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design](http://arxiv.org/abs/2407.02813v1)<br>:star:[code](https://github.com/coulsonlee/Dy-DCA-ECCV2024)
* [BurstM: Deep Burst Multi-scale SR using Fourier Space with Optical Flow](https://www.arxiv.org/abs/2409.15384)<br>:star:[code](https://github.com/Egkang-Luis/burstm)
* [HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution](http://arxiv.org/abs/2407.05878v1)
* [Pairwise Distance Distillation for Unsupervised Real-World Image Super-Resolution](http://arxiv.org/abs/2407.07302v1)<br>:star:[code](https://github.com/Yuehan717/PDD)
* [UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt](http://arxiv.org/abs/2407.13108v1)<br>:star:[code](https://lixinustc.github.io/UCIP.github.io)
* [Accelerating Image Super-Resolution Networks with Pixel-Level Classification](http://arxiv.org/abs/2407.21448v1)<br>:star:[code](https://github.com/3587jjh/PCSR)
* [Rethinking Image Super-Resolution from Training Data Perspectives](http://arxiv.org/abs/2409.00768v1)
* [Spatially-Variant Degradation Model for Dataset-free Super-resolution](https://arxiv.org/abs/2407.08252)<br>:star:[code](https://github.com/shaojieguoECNU/SVDSR)
* [Learning Exhaustive Correlation for Spectral Super-Resolution: Where Spatial-Spectral Attention Meets Linear Dependence](https://arxiv.org/abs/2312.12833)
* [Contourlet Residual for Prompt Learning Enhanced Infrared Image Super-Resolution](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00391.pdf)<br>:star:[code](https://github.com/hey-it-s-me/CoRPLE)
* [Pixel-Aware Stable Diffusion for Realistic Image Super-Resolution and Personalized Stylization](https://arxiv.org/abs/2308.14469)<br>:star:[code](https://github.com/yangxy/PASD)
* [XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution](https://arxiv.org/abs/2403.05049)<br>:star:[code](https://github.com/qyp2000/XPSR)
* [AdaDiffSR: Adaptive Region-aware Dynamic acceleration Diffusion Model for Real-World Image Super-Resolution](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01944.pdf)
* [Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks](https://arxiv.org/abs/2307.13337)<br>:star:[code](https://github.com/Cheeun/ODM)
* [Rethinking Image Super Resolution from Training Data Perspectives](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02525.pdf)<br>:star:[code](https://github.com/gohtanii/DiverSeg-dataset)
* [MTKD: Multi-Teacher Knowledge Distillation for Image Super-Resolution](https://arxiv.org/abs/2404.09571)<br>:star:[code](https://github.com/YuxuanJJ/MTKD)
* [OmniSSR: Zero-shot Omnidirectional Image Super-Resolution using Stable Diffusion Model](https://arxiv.org/abs/2404.10312)
* [You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation](https://arxiv.org/abs/2401.17258)<br>:star:[code](https://github.com/SamsungLabs/yonos)
* [A New Dataset and Framework for Real-World Blurred Images Super-Resolution](https://arxiv.org/abs/2407.14880)<br>:star:[code](https://github.com/Imalne/PBaSR)
* 场景文本图像超分辨率
  * [DCDM: Diffusion-Conditioned-Diffusion Model for Scene Text Image Super-Resolution](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02357.pdf)<br>:star:[code](https://github.com/shreygithub/DCDM)
* VSR
  * [Arbitrary-Scale Video Super-Resolution with Structural and Textural Priors](http://arxiv.org/abs/2407.09919v1)<br>:star:[code](https://github.com/shangwei5/ST-AVSR)
  * [RealViformer: Investigating Attention for Real-World Video Super-Resolution](http://arxiv.org/abs/2407.13987v1)<br>:star:[code](https://github.com/Yuehan717/RealViformer)
  * [Enhancing Perceptual Quality in Video Super-Resolution through Temporally-Consistent Detail Synthesis using Diffusion Models](https://arxiv.org/abs/2311.15908)<br>:star:[code](https://github.com/claudiom4sir/StableVSR)
  * [SuperGaussian: Repurposing Video Models for 3D Super Resolution](https://arxiv.org/abs/2406.00609)<br>:house:[project](http://supergaussian.github.io/)
  * [Event-Adapted Video Super-Resolution](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05857.pdf)
  * [Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution](https://arxiv.org/abs/2312.00853)<br>:star:[code](https://github.com/IanYeung/MGLD-VSR)

## Object Detection(目标检测)
* [Can OOD Object Detectors Learn from Foundation Models?](http://arxiv.org/abs/2409.05162v1)<br>:star:[code](https://github.com/CVMI-Lab/SyncOOD)
* [LEROjD: Lidar Extended Radar-Only Object Detection](http://arxiv.org/abs/2409.05564v1)<br>:star:[code](https://github.com/rst-tu-dortmund/lerojd)
* [Bucketed Ranking-based Losses for Efficient Training of Object Detectors](http://arxiv.org/abs/2407.14204v1)<br>:star:[code](https://github.com/blisgard/BucketedRankingBasedLosses)
* [Plain-Det: A Plain Multi-Dataset Object Detector](http://arxiv.org/abs/2407.10083v1)<br>:star:[code](https://github.com/ChengShiest/Plain-Det)
* [On Calibration of Object Detectors: Pitfalls, Evaluation and Baselines](https://arxiv.org/abs/2405.20459)<br>:star:[code](https://github.com/fiveai/detection_calibration)
* [Towards Open-World Object-based Anomaly Detection via Self-Supervised Outlier Synthesis](http://arxiv.org/abs/2407.15763v1)<br>:star:[code](https://github.com/KostadinovShalon/oln-ssos)
* [Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection](http://arxiv.org/abs/2407.15296v1)
* [PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects](http://arxiv.org/abs/2407.16696v1)<br>:star:[code](https://provencestar.github.io/PartGLEE-Vision/)
* [Crowd-SAM: SAM as a Smart Annotator for Object Detection in Crowded Scenes](http://arxiv.org/abs/2407.11464v1)<br>:star:[code](https://github.com/FelixCaae/CrowdSAM)
* [Relation DETR: Exploring Explicit Position Relation Prior for Object Detection](http://arxiv.org/abs/2407.11699v1)<br>:star:[code](https://github.com/xiuqhou/Relation-DETR)
* [Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection](http://arxiv.org/abs/2407.11499v1)<br>:star:[code](https://github.com/iSEE-Laboratory/BPF)
* [Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge](https://arxiv.org/abs/2404.01492)<br>:star:[code](https://github.com/heitorrapela/ModTr)
* [T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy](https://arxiv.org/pdf/2403.14610)<br>:star:[code](https://github.com/IDEA-Research/T-Rex?tab=readme-ov-file#news-)
* [Fine-grained Dynamic Network for Generic Event Boundary Detection](http://arxiv.org/abs/2407.04274v1)<br>:star:[code](https://github.com/Ziwei-Zheng/DyBDet)
* [CSOT: Cross-Scan Object Transfer for Semi-Supervised LiDAR Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02623.pdf)
* [Bayesian Detector Combination for Object Detection with Crowdsourced Annotations](http://arxiv.org/abs/2407.07958v1)<br>:star:[code](https://github.com/zhiqin1998/bdc)
* [Simplifying Source-Free Domain Adaptation for Object Detection: Effective Self-Training Strategies and Performance Insights](http://arxiv.org/abs/2407.07586v1)<br>:star:[code](https://github.com/EPFL-IMOS/simple-SFOD)
* [Benchmarking Object Detectors with COCO: A New Path Forward](https://arxiv.org/abs/2403.18819)<br>:sunflower:[dataset](https://cocorem.xyz/)
* [DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion](https://arxiv.org/abs/2403.00326)<br>:star:[code](https://github.com/gjj45/DAMSDet)目标检测
* [Integer-Valued Training and Spike-driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection](https://arxiv.org/abs/2407.20708)<br>:star:[code](https://github.com/BICLab/SpikeYOLO)
* [YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information](https://arxiv.org/abs/2402.13616)<br>:star:[code](https://github.com/WongKinYiu/yolov9)
* [Equivariant Spatio-Temporal Self-Supervision for LiDAR Object Detection](https://arxiv.org/abs/2404.11737)
* [Projecting Points to Axes: Oriented Object Detection via Point-Axis Representation](http://arxiv.org/abs/2407.08489v1)
* [GRA: Detecting Oriented Objects through Group-wise Rotating and Attention](https://arxiv.org/abs/2403.11127)
* [Embracing Events and Frames with Hierarchical Feature Refinement Network for Object Detection](http://arxiv.org/abs/2407.12582v1)<br>:star:[code](https://github.com/HuCaoFighting/FRN)
* [Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection](http://arxiv.org/abs/2407.16497v1)<br>:star:[code](https://github.com/lbktrinh/DRU)
* [Zero-Shot Detection of AI-Generated Images](https://arxiv.org/abs/2409.15875)<br>:star:[code](https://github.com/grip-unina/ZED/)<br>:house:[project](https://grip-unina.github.io/ZED/)
* [MOD-UV: Learning Mobile Object Detectors from Unlabeled Videos](https://arxiv.org/abs/2405.14841)<br>:star:[code](https://github.com/YihongSun/MOD-UV)
* [Category-level Object Detection, Pose Estimation and Reconstruction from Stereo Images](https://www.arxiv.org/abs/2407.06984)<br>:house:[project](https://xingyoujun.github.io/coders)
* 3D目标检测
  * [Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression](http://arxiv.org/abs/2409.00633v1)<br>:star:[code](https://github.com/DYZhang09/ToC3D)
  * [Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D Scene](http://arxiv.org/abs/2407.08569v1)
  * [SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object Detection](https://arxiv.org/abs/2403.07284)
  * [MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection](http://arxiv.org/abs/2407.16448v1)<br>:star:[code](https://github.com/VisualAIKHU/MonoWAD)
  * [Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection](http://arxiv.org/abs/2408.15637v1)<br>:star:[code](https://roadsense3d.github.io)
  * [Learning High-resolution Vector Representation from Multi-Camera Images for 3D Object Detection](http://arxiv.org/abs/2407.15354v1)<br>:star:[code](https://github.com/zlichen/VectorFormer)
  * [OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation](https://arxiv.org/abs/2403.19580)<br>:star:[code](https://github.com/zhenyuw16/Uni3DETR)
  * [Better Regression Makes Better Test-time Adaptive 3D Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05971.pdf)
  * [Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments](https://arxiv.org/abs/2403.13556)<br>:star:[code](https://github.com/djamahl99/findnpropagate)
  * [Diff3DETR: Agent-based Diffusion Model for Semi-supervised 3D Object Detection](https://arxiv.org/abs/2408.00286)
  * [Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance](https://arxiv.org/abs/2312.07530)<br>:star:[code](https://github.com/kuanchihhuang/VG-W3D)
  * [SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras](https://arxiv.org/abs/2403.10353)<br>:star:[code](https://github.com/nullmax-vision/SimPB)
  * [CMD: A Cross Mechanism Domain Adaptation Dataset for 3D Object Detection]<br>:thumbsup:[DIG从密度、强度和几何三方面缓和传感器体制带来的点云数据差异，显著提升了域自适应算法的性能。](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view 3D Object Detection](https://arxiv.org/abs/2402.03634)<br>:star:[code](https://github.com/LiewFeng/RayDN)
  * [OPEN: Object-wise Position Embedding for Multi-view 3D Object Detection](http://arxiv.org/abs/2407.10753v1)<br>:star:[code](https://github.com/AlmoonYsl/OPEN)
  * [LabelDistill: Label-guided Cross-modal Knowledge Distillation for Camera-based 3D Object Detection](http://arxiv.org/abs/2407.10164v1)<br>:star:[code](https://github.com/sanmin0312/LabelDistill)
  * [MonoTTA: Fully Test-Time Adaptation for Monocular 3D Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06035.pdf)<br>:star:[code](https://github.com/Hongbin98/MonoTTA)
  * [FSD-BEV: Foreground Self-Distillation for Multi-view 3D Object Detection](http://arxiv.org/abs/2407.10135v1)<br>:star:[code](https://github.com/CocoBoom/fsd-bev)
  * [General Geometry-aware Weakly Supervised 3D Object Detection](http://arxiv.org/abs/2407.13748v1)<br>:star:[code](https://github.com/gwenzhang/GGA)
  * [Interactive 3D Object Detection with Prompts](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02556.pdf)
  * [Beyond Viewpoint: Robust 3D Object Recognition under Arbitrary Views through Joint Multi-Part Representation](http://arxiv.org/abs/2407.03842v1)
  * [Detecting As Labeling: Rethinking LiDAR-camera Fusion in 3D Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03298.pdf)<br>:star:[code](https://github.com/HuangJunJie2017/BEVDet)
  * [TCC-Det: Temporarily consistent cues for weakly-supervised 3D detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03732.pdf)<br>:star:[code](https://github.com/jskvrna/TCC-Det)
  * [GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection](https://arxiv.org/abs/2403.11848)<br>:star:[code](https://github.com/adept-thu/GraphBEV)
* 小目标检测
  * [IRSAM: Advancing Segment Anything Model for Infrared Small Target Detection](http://arxiv.org/abs/2407.07520v1)
  * [Visible and Clear: Finding Tiny Objects in Difference Map](https://arxiv.org/abs/2405.11276)
  * [3D Small Object Detection with Dynamic Spatial Pruning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04105.pdf)<br>:star:[code](https://github.com/xuxw98/DSPDet3D)<br>:thumbsup:[DSPDet3D：基于动态空间剪枝的高效率3D小目标检测](https://zhuanlan.zhihu.com/p/714402773)
* 伪装目标检测
  * [CamoTeacher: Dual-Rotation Consistency Learning for Semi-Supervised Camouflaged Object Detection](https://arxiv.org/abs/2408.08050)<br>:thumbsup:[有效减少了像素级、实例级噪声问题](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [Learning Camouflaged Object Detection from Noisy Pseudo Label](http://arxiv.org/abs/2407.13157v1)<br>:star:[code](https://github.com/zhangjinCV/Noisy-COD)
  * [Just a Hint: Point-Supervised Camouflaged Object Detection](https://arxiv.org/abs/2408.10777)
  * [SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection](https://arxiv.org/abs/2408.10760)
  * [Frequency-Spatial Entanglement Learning for Camouflaged Object Detection](http://arxiv.org/abs/2409.01686v1)<br>:star:[code](https://github.com/CSYSI/FSEL)
* 长尾目标检测
  * [Rectify the Regression Bias in Long-Tailed Object Detection](https://arxiv.org/abs/2401.15885)
* 显著目标检测
  * [CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](https://www.arxiv.org/abs/2407.06780)<br>:star:[code](https://github.com/ssecv/CoLA)
* 域适应目标检测
  * [Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation](http://arxiv.org/abs/2407.13524v1)<br>:star:[code](https://github.com/junia3/LPLD)
  * [Revisiting Domain-Adaptive Object Detection in Adverse Weather by the Generation and Composition of High-Quality Pseudo-Labels](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05312.pdf)<br>:star:[code](https://github.com/iu110/GCHQ/)
* 小样本目标检测
  * [SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot Object Detection](http://arxiv.org/abs/2407.02665v1)<br>:house:[project](https://anaymajee.me/assets/project_pages/smile.html)
  * [Adaptive Multi-task Learning for Few-shot Object Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01149.pdf)<br>:star:[code](https://github.com/RY-Paper/MTL-FSOD)
  * [Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector](https://arxiv.org/pdf/2402.03094)<br>:house:[project](http://yuqianfu.com/CDFSOD-benchmark/)<br>:thumbsup:[跨域小样本物体检测CD-FSOD新数据集、CD-ViTO新方法（数据代码均已开源）](https://mp.weixin.qq.com/s/hsF95e5urjkEAc0dggFMFQ)
* 共同显著目标检测
  * [CONDA: Condensed Deep Association Learning for Co-Salient Object Detection](http://arxiv.org/abs/2409.01021v1)
  * [Self-supervised co-salient object detection via feature correspondences at multiple scales](https://arxiv.org/abs/2403.11107)<br>:star:[code](https://github.com/sourachakra/SCoSPARC)
* 开放词汇目标检测
  * [Global-Local Collaborative Inference with LLM for Lidar-Based Open-Vocabulary Detection](http://arxiv.org/abs/2407.08931v1)<br>:star:[code](https://github.com/GradiusTwinbee/GLIS)
  * [LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction](http://arxiv.org/abs/2407.11335v1)
  * [MarvelOVD: Marrying Object Recognition and Vision-Language Models for Robust Open-Vocabulary Object Detection](https://arxiv.org/abs/2407.21465)<br>:star:[code](https://github.com/wkfdb/MarvelOVD)
* 水印检测
  * [Finding a needle in a haystack: A Black-Box Approach to Invisible Watermark Detection](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04872.pdf)
* 阴影检测
  * [RenDetNet: Weakly-supervised Shadow Detection with Shadow Caster Verification](http://arxiv.org/abs/2408.17143v1)<br>:star:[code](https://github.com/n-kubiak/RenDetNet)
* 开集识别
  * [Operational Open-Set Recognition and PostMax Refinement](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01043.pdf)
  * [Open-Set Recognition in the Age of Vision-Language Models](https://arxiv.org/abs/2403.16528)
  * [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)<br>:star:[code](https://github.com/IDEA-Research/GroundingDINO)
* 目标定位
  * [WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language](https://arxiv.org/abs/2304.05645)<br>:star:[code](https://github.com/4DVLab/WildRefer)


## Object Tracking(目标跟踪)
* [Local All-Pair Correspondence for Point Tracking](http://arxiv.org/abs/2407.15420v1)<br>:star:[code](https://ku-cvlab.github.io/locotrack)<br>:star:[code](https://github.com/KU-CVLAB/locotrack)
* [Track Everything Everywhere Fast and Robustly](https://arxiv.org/abs/2403.17931)<br>:star:[code](https://github.com/TimSong412/OmniTrackFast/)<br>:house:[project](https://timsong412.github.io/FastOmniTrack/)
* [DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video](http://arxiv.org/abs/2403.14548)<br>:star:[code](https://github.com/AssafSinger94/dino-tracker)<br>:house:[project](https://dino-tracker.github.io/)
* [Decomposition Betters Tracking Everything Everywhere](http://arxiv.org/abs/2407.06531v1)<br>:star:[code](https://github.com/qianduoduolr/DecoMotion)
* [Self-Supervised Any-Point Tracking by Contrastive Random Walks](http://arxiv.org/abs/2409.16288v1)<br>:house:[project](https://ayshrv.com/gmrw)<br>:star:[code](https://github.com/ayshrv/gmrw/)
* [TAPTR: Tracking Any Point with Transformers as Detection](https://arxiv.org/abs/2403.13042)<br>:star:[code](https://github.com/IDEA-Research/TAPTR)
* [MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping](https://arxiv.org/abs/2403.15951)<br>:house:[project](https://map-tracker.github.io/)
* [SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow](https://arxiv.org/abs/2404.11426)
* [SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking](http://arxiv.org/abs/2409.11235v1)<br>:star:[code](https://github.com/siyuanliii/SLAck)
* [OneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01174.pdf)
* [Exploring the Feature Extraction and Relation Modeling For Light-Weight Transformer Tracking](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04168.pdf)<br>:star:[code](https://github.com/KarlesZheng/FERMT)
* 3D目标跟踪
  * [3D Single-object Tracking in Point Clouds with High Temporal Variation](https://arxiv.org/abs/2408.02049)
  * [Boosting 3D Single Object Tracking with 2D Matching Distillation and 3D Pre-training](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01900.pdf)
* 多目标跟踪
  * [Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking](http://arxiv.org/abs/2407.10151v1)<br>:star:[code](https://github.com/lorenzovaquero/BUSCA)
  * [Walker: Self-supervised Multiple Object Tracking by Walking on Temporal Object Appearance Graphs](https://arxiv.org/abs/2409.17221)
  * [Beyond MOT: Semantic Multi-Object Tracking](https://arxiv.org/abs/2403.05021)<br>:star:[code](https://github.com/HengLan/SMOT)

## OCR
* [Parrot Captions Teach CLIP to Spot Text](https://arxiv.org/abs/2312.14232)<br>:star:[code](https://github.com/opendatalab/CLIP-Parrot-Bias)
* [WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting](http://arxiv.org/abs/2407.19507v1)
* [FineMatch: Aspect-based Fine-grained Image and Text Mismatch Detection and Correction](https://arxiv.org/abs/2404.14715)<br>:house:[project](https://hanghuacs.github.io/finematch/)
* [Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors](https://arxiv.org/abs/2312.05286)<br>:star:[code](https://github.com/SJTU-DeepVisionLab/FreeReal)
* 手写文本检测
  * [Align, Minimize and Diversify: A Source-Free Unsupervised Domain Adaptation Method for Handwritten Text Recognition](https://export.arxiv.org/abs/2404.18260)
  * [PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer](http://arxiv.org/abs/2407.07764v1)<br>:star:[code](https://github.com/SJTU-DeepVisionLab/PosFormer)<br>:Thumbsup:[上交推出 PosFormer！优化位置识别任务来辅助表达式识别，复杂公式识别能力再创新SOTA！](https://mp.weixin.qq.com/s/XXKlGA-8yQoPcaWK8G1_5Q)
  * [Elegantly Written: Disentangling Writer and Character Styles for Enhancing Online Chinese Handwriting](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01329.pdf)
  * [NAMER: Non-Autoregressive Modeling for Handwritten Mathematical Expression Recognition](http://arxiv.org/abs/2407.11380v1)
* 手写文本合成
  * [One-Shot Diffusion Mimicker for Handwritten Text Generation](http://arxiv.org/abs/2409.04004v1)<br>:star:[code](https://github.com/dailenson/One-DM)
* 场景文本删除  
  * [Leveraging Text Localization for Scene Text Removal via Text-aware Masked Image Modeling](http://arxiv.org/abs/2409.13431v1)<br>:star:[code](https://github.com/wzx99/TMIM)
* 文档理解
  * [Textual Grounding for Open-vocabulary Visual Information Extraction in Layout-Diversified Documents]<br>:thumbsup:[结合版式感知上下文学习和适用于文档的两阶段预训练，显著提高了模型对文档的理解能力](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [VisFocus: Prompt-Guided Vision Encoders for OCR-Free Dense Document Understanding](https://arxiv.org/abs/2407.12594)<br>🤗[huggingface](https://huggingface.co/papers/2407.12594)密集文档理解
* 文本分割
  * [WAS: Dataset and Methods for Artistic Text Segmentation](http://arxiv.org/abs/2408.00106v1)




## Pose(姿态估计)
* [X-Pose: Detecting Any Keypoints](https://arxiv.org/abs/2310.08530)<br>:star:[code](https://github.com/IDEA-Research/X-Pose)
* [VQ-HPS: Human Pose and Shape Estimation in a Vector-Quantized Latent Space]<br>:house:[project](https://g-fiche.github.io/research-pages/vqhps/static/data/VQ-HPS_arxiv.pdf)
* [Expressive Whole-Body 3D Gaussian Avatar](http://arxiv.org/abs/2407.21686v1)<br>:star:[code](https://mks0601.github.io/ExAvatar/)
* [GTPT: Group-based Token Pruning Transformer for Efficient Human Pose Estimation](http://arxiv.org/abs/2407.10756v1)
* [PoseSOR: Human Pose Can Guide Our Attention](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02792.pdf)<br>:star:[code](https://github.com/guanhuankang/ECCV24PoseSOR)
* [COSMU: Complete 3D human shape from monocular unconstrained images](http://arxiv.org/abs/2407.10586v1)
* [Modeling and Driving Human Body Soundfields through Acoustic Primitives](http://arxiv.org/abs/2407.13083v1)<br>:house:[project](https://wikichao.github.io/Acoustic-Primitives/)
* [Domain-Adaptive 2D Human Pose Estimation via Dual Teachers in Extremely Low-Light Conditions](http://arxiv.org/abs/2407.15451v1)<br>:star:[code](https://github.com/ayh015-dev/DA-LLPose)
* [SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views](https://arxiv.org/abs/2408.10195)
* [PoseEmbroider: Towards a 3D, Visual, Semantic-aware Human Pose Representation](http://arxiv.org/abs/2409.06535v1)
* [PoseAugment: Generative Human Pose Data Augmentation with Physical Plausibility for IMU-based Motion Capture](http://arxiv.org/abs/2409.14101v1)<br>:star:[code](https://github.com/CaveSpiderLZJ/PoseAugment-ECCV2024)
* [HPE-Li: WiFi-enabled Lightweight Dual Selective Kernel Convolution for Human Pose Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04496.pdf)人体姿势估计
* [EgoPoser: Robust Real-Time Egocentric Pose Estimation from Sparse and Intermittent Observations Everywhere](https://arxiv.org/abs/2308.06493)<br>:house:[project](https://siplab.org/projects/EgoPoser)
* [You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception](https://arxiv.org/abs/2312.05525)<br>:star:[code](https://github.com/lishuhuai527/COCO-UniHuman)
* [Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects](https://arxiv.org/abs/2403.16428)
* 多人姿势预测
  * [Multi-Person Pose Forecasting with Individual Interaction Perceptron and Prior Learning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02808.pdf)<br>:star:[code](https://github.com/ArcticPole/IAFormer)
* 3D人体姿态估计
  * [MPL: Lifting 3D Human Pose from Multi-view 2D Poses](http://arxiv.org/abs/2408.10805v1)<br>:star:[code](https://github.com/aghasemzadeh/OpenMPL)
  * [RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark](http://arxiv.org/abs/2407.13930v1)<br>:house:[project](https://huggingface.co/datasets/uwipl/RT-Pose)
  * [Mask as Supervision: Leveraging Unified Mask Information for Unsupervised 3D Pose Estimation](https://arxiv.org/abs/2312.07051)<br>:star:[code](https://github.com/Charrrrrlie/Mask-as-Supervision)
  * [3D Human Pose Estimation via Non-Causal Retentive Networks](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04820.pdf)<br>:star:[code](https://github.com/Kelly510/PoseRetN)
  * [UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues](https://arxiv.org/abs/2404.14634)
  * [Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional Encoding](https://arxiv.org/abs/2405.17397)
  * [RePOSE: 3D Human Pose Estimation via Spatio-Temporal Depth Relational Consistency](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02925.pdf)<br>:star:[code](https://github.com/StupidAdam/RePOSE)
  * [3DSA:Multi-View 3D Human Pose Estimation With 3D Space Attention Mechanisms](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03937.pdf)
  * [WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02927.pdf)
* 人体网格恢复
  * [Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot](https://arxiv.org/abs/2402.14654)<br>:star:[code](https://github.com/naver/multi-hmr)
  * [Divide and Fuse: Body Part Mesh Recovery from Partially Visible Human Images](http://arxiv.org/abs/2407.09694v1)
  * [Global-to-Pixel Regression for Human Mesh Recovery](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02513.pdf)
* 3D人体纹理生成
  * [TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation](https://arxiv.org/abs/2403.12906)<br>:house:[project](https://ggxxii.github.io/texdreamer/)
* 3D人体生成
  * [StructLDM: Structured Latent Diffusion for 3D Human Generation](https://arxiv.org/pdf/2404.01241.pdf)<br>:star:[code](https://github.com/TaoHuUMD/StructLDM)<br>:house:[project](https://taohuumd.github.io/projects/StructLDM/)<br>:thumbsup:[南洋理工三维数字人生成新范式：结构扩散模型](https://mp.weixin.qq.com/s/hPPMxq2ogrchbCE3CdI1ow)
  * [Text to Layer-wise 3D Clothed Human Generation](https://arxiv.org/abs/2404.16748)<br>:house:[project](http://jtdong.com/tela_layer/)
  * [SemanticHuman-HD: High Resolution Semantic disentangled 3D Human Generation](https://arxiv.org/abs/2403.10166)<br>:house:[project](https://pengzheng0707.github.io/SemanticHuman-HD/)3D 人类生成
* 人体重建
  * [ReLoo: Reconstructing Humans Dressed in Loose Garments from Monocular Video in the Wild](https://arxiv.org/abs/2409.15269)<br>:house:[project](https://moygcc.github.io/ReLoo/)
* 动作捕捉
  * [LiveHPS++: Robust and Coherent Motion Capture in Dynamic Free Environment](http://arxiv.org/abs/2407.09833v1)<br>:house:[project](https://4dvlab.github.io/project_page/LiveHPS2.html)
* 手语识别
  * [EvSign: Sign Language Recognition and Translation with Streaming Events](http://arxiv.org/abs/2407.12593v1)<br>:star:[code](https://zhang-pengyu.github.io/EVSign)
  * [Pose-Guided Fine-Grained Sign Language Video Generation](http://arxiv.org/abs/2409.16709v1)
  * [Visual Alignment Pre-training for Sign Language Translation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05894.pdf)
* 手部网格
  * [HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning](http://arxiv.org/abs/2407.15844v1)
* 3D手部重建
  * [Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance](http://arxiv.org/abs/2407.12307v1)
* 手部姿态估计
  * [HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation](http://arxiv.org/abs/2407.20542v1)<br>:star:[code](https://github.com/cwc1260/HandDAGT)
* 手部运动预测
  * [Prompting Future Driven Diffusion Model for Hand Motion Prediction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01102.pdf)
* 头部姿态估计
  * [Event-based Head Pose Estimation: Benchmark and Method](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02323.pdf)<br>:star:[code](https://github.com/Jiahui-Yuan-1/EVHPE)
* 手持物体重建
  * [D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction](https://arxiv.org/abs/2311.14189)手持物体重建
* 头部姿态估计
  * [6DoF Head Pose Estimation through Explicit Bidirectional Interaction with Face Geometry](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04824.pdf)<br>:star:[code](https://github.com/asw91666/TRG-Release)
* 4D 头部捕获
  * [Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head Capture](https://arxiv.org/abs/2406.00440)<br>:star:[code](https://github.com/XuanchenLi/Topo4D)<br>:house:[project](https://xuanchenli.github.io/Topo4D/)4D 头部捕获
* 动作捕捉  
  * [MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering](https://arxiv.org/abs/2403.18820)<br>:star:[code](https://github.com/sunshinnnn/metacap)

## Face(人脸)
* [Task-adaptive Q-Face](https://arxiv.org/abs/2405.09059)
* [Faceptor: A Generalist Model for Face Perception](https://arxiv.org/abs/2403.09500)<br>:star:[code](https://github.com/lxq1000/Faceptor)
* [A Light Stage on Every Desk](https://arxiv.org/abs/2105.08051)<br>:house:[project](https://grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/)
* [Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control](https://arxiv.org/abs/2405.12970)<br>:star:[code](https://github.com/FaceAdapter/Face-Adapter)<br>:house:[project](https://faceadapter.github.io/face-adapter.github.io/)
* [ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer](http://arxiv.org/abs/2408.03284v1)<br>:star:[code](https://guanjz20.github.io/projects/ReSyncer)
* [Facial Affective Behavior Analysis with Instruction Tuning](https://arxiv.org/abs/2404.05052)<br>:star:[code](https://github.com/JackYFL/EmoLA)<br>:house:[project](https://johnx69.github.io/FABA/)
* [Arc2Face: A Foundation Model for ID-Consistent Human Faces](https://arxiv.org/abs/2403.11641)<br>:star:[code](https://github.com/foivospar/Arc2Face)<br>:house:[project](https://arc2face.github.io/)
* 人脸模糊
  * [Forbes: Face Obfuscation Rendering via Backpropagation Refinement Scheme](http://arxiv.org/abs/2407.14170v1)<br>:star:[code](https://github.com/mcljtkim/Forbes)
* 人脸识别
  * [AdaDistill: Adaptive Knowledge Distillation for Deep Face Recognition](https://arxiv.org/abs/2407.01332)
  * [ARoFace: Alignment Robustness to Improve Low-Quality Face Recognition](http://arxiv.org/abs/2407.14972v1)<br>:star:[code](https://github.com/msed-Ebrahimi/ARoFace)
  * [Personalized Privacy Protection Mask Against Unauthorized Facial Recognition](http://arxiv.org/abs/2407.13975v1)
  * [MST-KD: Multiple Specialized Teachers Knowledge Distillation for Fair Face Recognition](http://arxiv.org/abs/2408.16563v1)
* 人脸聚类
  * [VideoClusterNet: Self-Supervised and Adaptive Face Clustering for Videos](https://arxiv.org/abs/2407.12214)人脸聚类
* 人脸重建
  * [Face Reconstruction Transfer Attack as Out-of-Distribution Generalization](http://arxiv.org/abs/2407.02403v1)
* 人脸表情
  * [Norface: Improving Facial Expression Analysis by Identity Normalization](http://arxiv.org/abs/2407.15617v1)<br>:star:[code](https://norface-fea.github.io/)
  * [Generalizable Facial Expression Recognition](http://arxiv.org/abs/2408.10614v1)<br>:star:[code](https://github.com/zyh-uaiaaaa/Generalizable-FER)
  * [How Video Meetings Change Your Expression](https://arxiv.org/abs/2406.00955)<br>:house:[project](https://facet.cs.columbia.edu/)人脸
* 人脸编辑
  * [GroupDiff: Diffusion-based Group Portrait Editing](https://arxiv.org/abs/2409.14379)<br>:star:[code](https://github.com/yumingj/GroupDiff)
* 三维人脸动画
  * [KMTalk: Speech-Driven 3D Facial Animation with Key Motion Embedding](http://arxiv.org/abs/2409.01113v1)<br>:star:[code](https://github.com/ffxzh/KMTalk)
  * [UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model](https://arxiv.org/abs/2408.00762)<br>🤗[huggingface](https://huggingface.co/papers/2408.00762)
* 说话头合成
  * [ScanTalk: 3D Talking Heads from Unregistered Scans](https://arxiv.org/abs/2403.10942)<br>:star:[code](https://github.com/miccunifi/ScanTalk)
  * [EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis](https://arxiv.org/abs/2404.01647)<br>:house:[project](https://tanshuai0219.github.io/EDTalk/)头部合成
  * [EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head](http://arxiv.org/abs/2408.00297v1)<br>:star:[code](https://nju-3dv.github.io/projects/EmoTalk3D)
  * [Audio-driven Talking Face Generation with Stabilized Synchronization Loss](https://arxiv.org/abs/2307.09368)
  * [Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in 360°](http://arxiv.org/abs/2408.00296v1)<br>:star:[code](https://nju-3dv.github.io/projects/Head360)
  * [S^3D-NeRF: Single-Shot Speech-Driven Neural Radiance Field for High Fidelity Talking Head Synthesis](http://arxiv.org/abs/2408.09347v1)
  * [Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing](https://arxiv.org/abs/2312.03763)<br>:house:[project](https://nirvanalan.github.io/projects/gaussian3diff/)头部合成
  * [Tri2-plane: Thinking Head Avatar via Feature Pyramid](https://arxiv.org/abs/2401.09386)<br>:house:[project](https://songluchuan.github.io/Tri2Plane.github.io/)
  * [Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02330.pdf)<br>:house:[project](https://research.nvidia.com/labs/nxp/avatar-fingerprinting/)
  * [TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting](https://arxiv.org/abs/2404.15264)<br>:star:[code](https://github.com/Fictionarry/TalkingGaussian)3D 说话头合成
* 动画头部头像 
  * [HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting](https://arxiv.org/abs/2402.06149)<br>:star:[code](https://github.com/ZhenglinZhou/HeadStudio)动画头部头像  
  * [HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting](https://arxiv.org/abs/2312.02902)
  * [3D Gaussian Parametric Head Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05133.pdf)<br>:star:[code](https://github.com/YuelangX/GPHM)头部
* 人脸超分辨
  * [Kalman-Inspired Feature Propagation for Video Face Super-Resolution](https://arxiv.org/abs/2408.05205)<br>:house:[project](https://jnjaby.github.io/projects/KEEP/)
* 人脸活体检测
  * [TF-FAS: Twofold-Element Fine-Grained Semantic Guidance for Generalizable Face Anti-Spoofing](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01098.pdf)<br>:star:[code](https://github.com/xudongww/TF-FAS)<br>:thumbsup:[通过双重元素细粒度语义指导来增强泛化能力](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [Towards Unified Representation of Invariant-Specific Features in Missing Modality Face Anti-Spoofing](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03248.pdf)
* 头部合成  
  * [Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer](https://arxiv.org/abs/2403.13570)<br>:star:[code](https://github.com/YuDeng/Portrait-4D)<br>:house:[project](https://yudeng.github.io/Portrait4D-v2/)
* 情绪识别
  * [Upper-body Hierarchical Graph for Skeleton Based Emotion Recognition in Assistive Driving](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03697.pdf)<br>:star:[code](https://github.com/jerry-wjh/UbH-GCN)基于骨骼的情绪识别

## 3D Visual
* [GroundUp: Rapid Sketch-Based 3D City Massing](http://arxiv.org/abs/2407.12739v1)<br>:house:[project](http://visual.cs.ucl.ac.uk/pubs/groundup/index.html)
* [Ray-Distance Volume Rendering for Neural Scene Reconstruction](https://arxiv.org/abs/2408.15524)
* [Decomposition of Neural Discrete Representations for Large-Scale 3D Mapping](http://arxiv.org/abs/2407.15554v1)<br>:star:[code](https://github.com/minseong-p/dnmap)
* [BlenderAlchemy: Editing 3D Graphics with Vision-Language Models](https://arxiv.org/abs/2404.17672)<br>:house:[project](https://ianhuang0630.github.io/BlenderAlchemyWeb/)
* [Temporal Event Stereo via Joint Learning with Stereoscopic Flow](http://arxiv.org/abs/2407.10831v1)<br>:star:[code](https://github.com/mickeykang16/TemporalEventStereo)
* [GenRC: Generative 3D Room Completion from Sparse Image Collections](http://arxiv.org/abs/2407.12939v1)<br>:star:[code](https://minfenli.github.io/GenRC)
* [SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided Geometric Linearization](http://arxiv.org/abs/2407.14257v1)<br>:star:[code](https://sparsecraft.github.io)
* [3D Congealing: 3D-Aware Image Alignment in the Wild](https://arxiv.org/abs/2404.02125)<br>:house:[project](https://ai.stanford.edu/~yzzhang/projects/3d-congealing/)
* [ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07447.pdf)<br>:house:[project](https://sm0kywu.github.io/ClusteringSDF/)
* [An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes](https://arxiv.org/abs/2403.15559)<br>:house:[project](https://aigc3d.github.io/ConsistenTex)
* [Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models](https://arxiv.org/abs/2404.12139)<br>:house:[project](https://omniview-tuning.github.io/)
* [Diffusion Model is a Good Pose Estimator from 3D RF-Vision](https://arxiv.org/abs/2403.16198)<br>:house:[project](https://fanjunqiao.github.io/mmDiff-site/)
* [Nuvo: Neural UV Mapping for Unruly 3D Representations](https://arxiv.org/abs/2312.05283)<br>:house:[project](https://pratulsrinivasan.github.io/nuvo/)
* [MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps](https://arxiv.org/abs/2406.05849)<br>:house:[project](https://map-adapt.github.io/)
* 3D Visual Grounding
  * [Empowering 3D Visual Grounding with Reasoning Capabilities](https://arxiv.org/abs/2407.01525)<br>:house:[project](https://zcmax.github.io/projects/ScanReason)
  * [Multi-branch Collaborative Learning Network for 3D Visual Grounding](http://arxiv.org/abs/2407.05363v1)<br>:star:[code](https://github.com/qzp2018/MCLN)<br>:thumbsup:[3DREC的Acc@0.5提高了 3.27%，3DRES的mIOU 提高了5.22%](https://std.xmu.edu.cn/2024/0710/c4739a488273/page.htm)
  * [ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities](https://arxiv.org/abs/2407.01525)<br>:star:[code](https://github.com/ZCMax/ScanReason)
* Stereo Matching
  * [Temporally Consistent Stereo Matching](http://arxiv.org/abs/2407.11950v1)<br>:star:[code](https://github.com/jiaxiZeng/Temporally-Consistent-Stereo-Matching)
  * [Learning Representations from Foundation Models for Domain Generalized Stereo Matching](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05841.pdf)立体匹配
* 3DGS
  * [GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254v1)
  * [3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting](https://arxiv.org/abs/2408.03753)
  * [Texture-GS: Disentangle the Geometry and Texture for 3D Gaussian Splatting Editing](https://arxiv.org/abs/2403.10050)<br>:star:[code](https://github.com/slothfulxtx/Texture-GS)
  * [Compact3D: Smaller and Faster Gaussian Splatting with Vector Quantization](https://arxiv.org/abs/2311.18159)<br>:star:[code](https://github.com/UCDvision/compact3d)<br>:house:[project](https://ucdvision.github.io/compact3d/)
  * [CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization](https://arxiv.org/abs/2405.12110)<br>:star:[code](https://github.com/jiaw-z/CoR-GS)<br>:house:[project](https://jiaw-z.github.io/CoR-GS/)3DGS
  * [Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting](https://arxiv.org/abs/2404.03613)<br>:house:[project](https://jeongminb.github.io/e-d3dgs/)
  * [HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2403.14530)<br>:house:[project](https://yihangchen-ee.github.io/project_hac/)
  * [On the Error Analysis of 3D Gaussian Splatting and an Optimal Projection Strategy](https://arxiv.org/abs/2402.00752)<br>:star:[code](https://github.com/LetianHuang/op43dgs)
  * [Analytic-Splatting: Anti-Aliased 3D Gaussian Splatting via Analytic Integration](https://arxiv.org/abs/2403.11056)<br>:star:[code](https://github.com/lzhnb/Analytic-Splatting)
  * [MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo](https://arxiv.org/abs/2405.12218)<br>:star:[code](https://github.com/TQTQliu/MVSGaussian)<br>:house:[project](https://mvsgaussian.github.io/)
  * [MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation](https://www.arxiv.org/abs/2409.09756)
  * [MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition](http://arxiv.org/abs/2407.07284v1)<br>:star:[code](https://aggelinacha.github.io/MIGS/)
  * [SAGS: Structure-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2404.19149)<br>:house:[project](https://eververas.github.io/SAGS/)
  * [Pixel-GS Density Control with Pixel-aware Gradient for 3D Gaussian Splatting](https://arxiv.org/abs/2403.15530)<br>:star:[code](https://github.com/zhengzhang01/Pixel-GS)
  * [WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D Gaussians](https://arxiv.org/abs/2409.17917)<br>:house:[project](https://compvis.github.io/wast3d/)
  * [MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2403.14627)<br>:star:[code](https://github.com/donydchen/mvsplat)
  * [GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting](https://arxiv.org/abs/2404.19702)<br>:house:[project](https://sai-bi.github.io/project/gs-lrm/)
* 深度估计
  * [Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos](https://arxiv.org/abs/2403.17915)<br>:star:[code](https://github.com/Roni-Lab/PPSNet)<br>:house:[project](https://ppsnet.github.io/)
  * [FutureDepth: Learning to Predict the Future Improves Video Depth Estimation](https://arxiv.org/abs/2403.12953)
  * [ProDepth: Boosting Self-Supervised Multi-Frame Monocular Depth with Probabilistic Fusion](http://arxiv.org/abs/2407.09303v1)<br>:star:[code](https://sungmin-woo.github.io/prodepth/)
  * [Mono-ViFI: A Unified Learning Framework for Self-supervised Single- and Multi-frame Monocular Depth Estimation](http://arxiv.org/abs/2407.14126v1)<br>:star:[code](https://github.com/LiuJF1226/Mono-ViFI)
  * [Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions](http://arxiv.org/abs/2407.16698v1)<br>:star:[code](https://diffusion4robustdepth.github.io/)<br>:star:[code](https://github.com/fabiotosi92/Diffusion4RobustDepth)
  * [High-Precision Self-Supervised Monocular Depth Estimation with Rich-Resource Prior](http://arxiv.org/abs/2408.00361v1)
  * [DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation](https://arxiv.org/abs/2303.05021)<br>:star:[code](https://github.com/duanyiqun/DiffusionDepth)
  * [SEDiff: Structure Extraction for Domain Adaptive Depth Estimation via Denoising Diffusion Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02829.pdf)
  * [Camera Height Doesn't Change: Unsupervised Training for Metric Monocular Road-Scene Depth Estimation](https://arxiv.org/abs/2312.04530)<br>:house:[project](https://vision.ist.i.kyoto-u.ac.jp/research/fumet/)
  * [M2Depth: Self-supervised Two-Frame Multi-camera Metric Depth Estimation](https://arxiv.org/abs/2405.02004)<br>:house:[project](https://heiheishuang.xyz/M2Depth)
  * [Improving Domain Generalization in Self-Supervised Monocular Depth Estimation via Stabilized Adversarial Training](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03467.pdf)
* 深度补全
  * [Deep Cost Ray Fusion for Sparse Depth Video Completion](http://arxiv.org/abs/2409.14935v1)
  * [OGNI-DC: Robust Depth Completion with Optimization-Guided Neural Iterations](https://arxiv.org/abs/2406.11711)<br>:star:[code](https://github.com/princeton-vl/OGNI-DC)
* 表面重建
  * [Surface Reconstruction from Gaussian Splatting via Novel Stereo Views](https://arxiv.org/abs/2404.01810)<br>:house:[project](https://gs2mesh.github.io/)
  * [SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization](http://arxiv.org/abs/2407.12667v1)<br>:star:[code](https://github.com/Iris-cyy/SG-NeRF)
  * [Improving Neural Surface Reconstruction with Feature Priors from Multi-View Image](https://arxiv.org/abs/2408.02079)
  * [DiffSurf: A Transformer-based Diffusion Model for Generating and Reconstructing 3D Surfaces in Pose](http://arxiv.org/abs/2408.14860v1)
  * [Rethinking Directional Parameterization in Neural Implicit Surface Reconstruction](http://arxiv.org/abs/2409.06923v1)
  * [PISR: Polarimetric Neural Implicit Surface Reconstruction for Textureless and Specular Objects](http://arxiv.org/abs/2409.14331v1)<br>:star:[code](https://github.com/GCChen97/PISR)
  * [Surface Reconstruction for 3D Gaussian Splatting via Local Structural Hints](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00274.pdf)<br>:house:[project](https://qianyiwu.github.io/gsrec)
  * [DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction](https://arxiv.org/abs/2403.11586)<br>:star:[code](https://github.com/yaoyx689/DynoSurf)
  * [Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction](https://arxiv.org/abs/2409.03634)<br>:star:[code](https://github.com/prstrive/SuRF)
  * [Parameterization-driven Neural Surface Reconstruction for Object-oriented Editing in Neural Rendering](https://arxiv.org/abs/2310.05524)<br>:house:[project](https://xubaixinxbx.github.io/neuparam)
* 三维重建
  * [GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction](http://arxiv.org/abs/2407.04237v1)<br>:house:[project](https://yxmu.foo/GSD/)
  * [InfoNorm: Mutual Information Shaping of Normals for Sparse-View Reconstruction](http://arxiv.org/abs/2407.12661v1)<br>:star:[code](https://github.com/Muliphein/InfoNorm)
  * [fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction](http://arxiv.org/abs/2409.11315v1)<br>:star:[code](https://jianxgao.github.io/MinD-3D)<br>:house:[project](https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse)<br>:house:[project](https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape)
  * [Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians](https://arxiv.org/abs/2403.09434)<br>:star:[code](https://github.com/Colmar-zlicheng/Spring-Gaus)
  * [GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation](https://arxiv.org/abs/2403.14621)<br>:star:[code](https://github.com/justimyhxu/grm)<br>:house:[project](https://justimyhxu.github.io/projects/grm/)
  * [Human Hair Reconstruction with Strand-Aligned 3D Gaussians](https://arxiv.org/abs/2409.14778)<br>:house:[project](https://eth-ait.github.io/GaussianHaircut)
  * [MVDiffHD: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02446.pdf)(https://github.com/Tangshitao/MVDiffusion_plusplus)
  * [NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation](https://arxiv.org/abs/2403.18241)<br>:house:[project](https://weizheliu.github.io/NeuSDFusion/)
  * [Analysis-by-Synthesis Transformer for Single-View 3D Reconstruction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03170.pdf)<br>:star:[code](https://github.com/DianJJ/AST)
* 三维形状
  * [Synchronous Diffusion for Unsupervised Smooth Non-Rigid 3D Shape Matching](http://arxiv.org/abs/2407.08244v1)
  * [Transferable 3D Adversarial Shape Completion using Diffusion Models](http://arxiv.org/abs/2407.10077v1)
  * [Self-supervised Shape Completion via Involution and Implicit Correspondences](http://arxiv.org/abs/2409.15939v1)
* 视频重建
  * [Temporal Residual Guided Diffusion Framework for Event-Driven Video Reconstruction](http://arxiv.org/abs/2407.10636v1)
* 四维重建
  * [SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction](http://arxiv.org/abs/2409.11211v1)<br>:star:[code](https://markomih.github.io/SplatFields/)
* 3D 纹理形状
  * [High-Fidelity 3D Textured Shapes Generation by Sparse Encoding and Adversarial Decoding](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01495.pdf)<br>:star:[code](https://github.com/aigc3d/Sparse3D)<br>:house:[project](https://aigc3d.github.io/gobjaverse/) 

## Other(其它)
* [Dataset Growth](https://arxiv.org/abs/2405.18347)<br>:star:[code](https://github.com/NUS-HPC-AI-Lab/InfoGrowth)
* [Adaptive Parametric Activation](https://arxiv.org/abs/2407.08567)<br>:star:[code](https://github.com/kostas1515/AGLU)
* [Nonverbal Interaction Detection](http://arxiv.org/abs/2407.08133v1)<br>:star:[code](https://github.com/weijianan1/NVI)
* [Situated Instruction Following](https://arxiv.org/abs/2407.12061)<br>:house:[project](https://soyeonm.github.io/SIF_webpage/)
* [Optimizing Illuminant Estimation in Dual-Exposure HDR Imaging](https://arxiv.org/abs/2403.02449)
* [Unsupervised Exposure Correction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00986.pdf)<br>:star:[code](https://github.com/BeyondHeaven/uec_code)
* [Global Structure-from-Motion Revisited](http://arxiv.org/abs/2407.20219v1)<br>:star:[code](https://github.com/colmap/glomap)
* [Fast Sprite Decomposition from Animated Graphics](https://arxiv.org/abs/2408.03923)<br>:house:[project](https://cyberagentailab.github.io/sprite-decompose/)
* [Sync from the Sea: Retrieving Alignable Videos from Large-Scale Datasets](http://arxiv.org/abs/2409.01445v1)<br>:house:[project](https://daveishan.github.io/avr-webpage/)
* [MERLiN: Single-Shot Material Estimation and Relighting for Photometric Stereo](http://arxiv.org/abs/2409.00674v1)
* [Enhancing Vectorized Map Perception with Historical Rasterized Maps](http://arxiv.org/abs/2409.00620v1)<br>:star:[code](https://github.com/HXMap/HRMapNet)
* [Align before Collaborate: Mitigating Feature Misalignment for Robust Multi-Agent Perception](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00560.pdf)
* [DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation](http://arxiv.org/abs/2409.03755v1)<br>:star:[code](https://github.com/wl-zhao/DC-Solver)
* [Weight Conditioning for Smooth Optimization of Neural Networks](http://arxiv.org/abs/2409.03424v1)
* [Bones Can't Be Triangles: Accurate and Efficient Vertebrae Keypoint Estimation through Collaborative Error Revision](http://arxiv.org/abs/2409.03261v1)<br>:star:[code](https://ts-kim.github.io/KeyBot/)
* [Textual Grounding for Open-vocabulary Visual Information Extraction in Layout-diversified Documents](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06219.pdf)
* [Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset](https://arxiv.org/abs/2407.12330)<br>:star:[code](https://github.com/mijoo308/Energy-Calibration)
* [Learning to Enhance Aperture Phasor Field for Non-Line-of-Sight Imaging](https://arxiv.org/abs/2407.18574)<br>:star:[code](https://github.com/join16/LEAP)
* [UniFS: Universal Few-shot Instance Perception with Point Representations](https://arxiv.org/abs/2404.19401)<br>:star:[code](https://github.com/jin-s13/UniFS)
* [Combining Generative and Geometry Priors for Wide-Angle Portrait Correction](https://arxiv.org/abs/2410.09911)<br>:star:[code](https://github.com/Dev-Mrha/DualPriorsCorrection)
* [FlashTex: Fast Relightable Mesh Texturing with LightControlNet](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03874.pdf)<br>:house:[project](https://flashtex.github.io/)重新照明
* [SIGMA: Sinkhorn-Guided Masked Video Modeling](https://arxiv.org/abs/2407.15447)<br>:house:[project](https://quva-lab.github.io/SIGMA/)
* [LiDAR-Event Stereo Fusion with Hallucinations](https://arxiv.org/abs/2408.04633)<br>:star:[code](https://github.com/bartn8/eventvppstereo/)<br>:house:[project](https://eventvppstereo.github.io/)
* [Temporal Residual Guided Diffusion Framework for Event-Driven Video Reconstruction](https://arxiv.org/abs/2407.10636)视频重建
* [Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted Nearby Views](https://arxiv.org/abs/2312.04424)<br>:house:[project](https://cascadezero123.github.io/)
* [Controlling the World by Sleight of Hand](https://arxiv.org/abs/2408.07147)
* [Probabilistic Weather Forecasting with Deterministic Guidance-based Diffusion Model](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04326.pdf)<br>:star:[code](https://github.com/DongGeun-Yoon/DGDM)概率天气预报
* [Representing Topological Self-Similarity Using Fractal Feature Maps for Accurate Segmentation of Tubular Structures](https://arxiv.org/abs/2407.14754)<br>:star:[code](https://github.com/cbmi-group/FFM-Multi-Decoder-Network)
* [Functional Transform-Based Low-Rank Tensor Factorization for Multi-Dimensional Data Recovery](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04468.pdf)
* [G3R: Gradient Guided Generalizable Reconstruction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00992.pdf)<br>:house:[project](https://waabi.ai/g3r/)
* [SAIR: Learning Semantic-aware Implicit Representation](https://arxiv.org/abs/2310.09285)
* [Spectral Subsurface Scattering for Material Classification](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00725.pdf)
* [Instance-dependent Noisy-label Learning with Graphical Model Based Noise-rate Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00589.pdf)
* [Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance](https://arxiv.org/abs/2403.05231)<br>:star:[code](https://github.com/LitingLin/LoRAT)
* [A Direct Approach to Viewing Graph Solvability](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00126.pdf)
* [Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training](https://arxiv.org/abs/2311.14109)<br>:star:[code](https://github.com/chengtan9907/mc-cot)
* [ZeST: Zero-Shot Material Transfer from a Single Image](https://arxiv.org/abs/2404.06425)<br>:star:[code](https://github.com/ttchengab/zest_code)
* [PCF-Lift: Panoptic Lifting by Probabilistic Contrastive Fusion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00187.pdf)
* [SemGrasp: Semantic Grasp Generation via Language Aligned Discretization](https://arxiv.org/abs/2404.03590)<br>:house:[project](https://kailinli.github.io/SemGrasp)
* [DragAPart: Learning a Part-Level Motion Prior for Articulated Objects](https://arxiv.org/abs/2403.15382)<br>:house:[project](https://dragapart.github.io/)
* [Superpixel-informed Implicit Neural Representation for Multi-Dimensional Data](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00234.pdf)
* [Physics-Free Spectrally Multiplexed Photometric Stereo under Unknown Spectral Composition](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00252.pdf)
* [Robust Fitting on a Gate Quantum Computer](http://arxiv.org/abs/2409.02006v1)
* [Mahalanobis Distance-based Multi-view Optimal Transport for Multi-view Crowd Localization](http://arxiv.org/abs/2409.01726v1)<br>:house:[project](https://vcc.tech/research/2024/MVOT)
* [On the Vulnerability of Skip Connections to Model Inversion Attacks](http://arxiv.org/abs/2409.01696v1)<br>:star:[code](https://Pillowkoh.github.io/projects/RoLSS/)
* [Taming CLIP for Fine-grained and Structured Visual Understanding of Museum Exhibits](http://arxiv.org/abs/2409.01690v1)<br>:star:[code](https://github.com/insait-institute/MUZE)
* [GMM-IKRS: Gaussian Mixture Models for Interpretable Keypoint Refinement and Scoring](http://arxiv.org/abs/2408.17149v1)
* [ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features from Multi-View Images](http://arxiv.org/abs/2408.17027v1)
* [Does Data-Efficient Generalization Exacerbate Bias in Foundation Models?](http://arxiv.org/abs/2408.16154v1)
* [InfMAE: A Foundation Model in The Infrared Modality](https://arxiv.org/abs/2402.00407)红外
* [Teach CLIP to Develop a Number Sense for Ordinal Regression](https://arxiv.org/abs/2408.03574)
* [GlobalPointer: Large-Scale Plane Adjustment with Bi-Convex Relaxation](http://arxiv.org/abs/2407.13537v1)<br>:star:[code](https://github.com/wu-cvgl/GlobalPointer)
* [ColorMAE: Exploring data-independent masking strategies in Masked AutoEncoders](http://arxiv.org/abs/2407.13036v1)
* [Scalar Function Topology Divergence: Comparing Topology of 3D Objects](http://arxiv.org/abs/2407.08364v1)
* [OneRestore: A Universal Restoration Framework for Composite Degradation](https://arxiv.org/abs/2407.04621)<br>:star:[code](https://github.com/gy65896/OneRestore)
* [RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via Diffusion](https://arxiv.org/abs/2404.09290)
* [Binomial Self-compensation for Motion Error in Dynamic 3D Scanning](https://arxiv.org/abs/2404.06693)
* [Encapsulating Knowledge in One Prompt]
* [iMatching: Imperative Correspondence Learning](https://arxiv.org/abs/2312.02141)
* [An Adaptive Screen-Space Meshing Approach for Normal Integration](https://arxiv.org/abs/2409.16907)
* [Efficient Pre-training for Localized Instruction Generation of Procedural Videos](https://arxiv.org/abs/2311.15964)<br>:star:[code](https://github.com/anilbatra2185/sns_procx)
* [Shape from Heat Conduction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05559.pdf)
* [Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos](https://arxiv.org/abs/2403.06351)
* [Optimal Transport of Diverse Unsupervised Tasks for Robust Learning from Noisy Few-Shot Data](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05600.pdf)
* [Finding Visual Task Vectors](https://arxiv.org/abs/2404.05729)<br>:star:[code](https://github.com/alhojel/visual_task_vectors)
* [Occupancy as Set of Points](http://arxiv.org/abs/2407.04049v1)<br>:star:[code](https://github.com/hustvl/osp)
* [Learning to Robustly Reconstruct Dynamic Scenes from Low-light Spike Streams](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02547.pdf)
* [AID-AppEAL: Automatic Image Dataset and Algorithm for Content Appeal Enhancement and Assessment Labeling](https://arxiv.org/abs/2407.05546)<br>:star:[code](https://github.com/SherryXTChen/AID-Appeal)
* [Physical-Based Event Camera Simulator](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06110.pdf)<br>:star:[code](https://github.com/lanpokn/PECS_trail_version)
* [REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices](https://arxiv.org/abs/2403.16481)<br>:star:[code](https://github.com/MARVELOUSJI/REFRAME)
* [Self-Training Room Layout via Geometry-aware Ray-casting](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06150.pdf)
* [Closed-Loop Unsupervised Representation Disentanglement with β-VAE Distillation and Diffusion Probabilistic Feedback](https://arxiv.org/abs/2402.02346)
* [UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding](https://arxiv.org/abs/2401.06397)<br>:star:[code](https://github.com/lygsbw/UMG-CLIP)
* [Where am I? Scene Retrieval with Language](https://arxiv.org/abs/2404.14565)
* [Event Camera Data Dense Pre-training](https://arxiv.org/abs/2311.11533)
* [Unsqueeze [CLS] Bottleneck to Learn Rich Representations](https://arxiv.org/abs/2407.17671)<br>:star:[code](https://github.com/ISL-CV/udi)
* [VeCLIP: Improving CLIP Training via Visual-enriched Captions](https://arxiv.org/abs/2310.07699)<br>:star:[code](https://github.com/apple/ml-veclip)
* [Spike-Temporal Latent Representation for Energy-Efficient Event-to-Video Reconstruction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05843.pdf)
* [Catastrophic Overfitting: A Potential Blessing in Disguise](https://arxiv.org/abs/2402.18211)
* [Diffusion Reward: Learning Rewards via Conditional Video Diffusion](https://arxiv.org/abs/2312.14134)<br>:house:[project](https://diffusion-reward.github.io/)
* [Data-to-Model Distillation: Data-Efficient Learning Framework](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06020.pdf)
* [Neural graphics texture compression supporting random access](https://arxiv.org/abs/2407.00021)
* [ReMatching: Low-Resolution Representations for Scalable Shape Correspondence](https://arxiv.org/abs/2305.09274)
* [EgoPet: Egomotion and Interaction Data from an Animal's Perspective](https://arxiv.org/abs/2404.09991)<br>:house:[project](https://www.amirbar.net/egopet/)
* [This Probably Looks Exactly Like That: An Invertible Prototypical Network](https://arxiv.org/abs/2407.12200)<br>:star:[code](https://github.com/craymichael/ProtoFlow)
* [Revisiting Feature Disentanglement Strategy in Diffusion Training and Breaking Conditional Independence Assumption in Sampling](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05452.pdf)
* [ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling](https://arxiv.org/abs/2408.04102)<br>:star:[code](https://github.com/google-research/google-research/tree/master/attribute_with_prefixlm)属性识别
* [Stream Query Denoising for Vectorized HD-Map Construction](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02879.pdf)
* [PartCraft: Crafting Creative Objects by Parts](http://arxiv.org/abs/2407.04604v1)<br>:star:[code](https://github.com/kamwoh/partcraft)
* [ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback](https://arxiv.org/abs/2404.07987)<br>:star:[code](https://github.com/liming-ai/ControlNet_Plus_Plus)
* [Dropout Mixture Low-Rank Adaptation for Visual Parameters-Efficient Fine-Tuning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01163.pdf)
* [UNIC: Universal Classification Models via Multi-teacher Distillation](https://arxiv.org/abs/2408.05088)
* [Efficient Training of Spiking Neural Networks with Multi-Parallel Implicit Stream Architecture](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05068.pdf)<br>:star:[code](https://github.com/kiritozc/MPIS-SNNs)尖峰神经网络
* [IGNORE: Information Gap-based False Negative Loss Rejection for Single Positive Multi-Label Learning](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05081.pdf)
* [Visual Prompting via Partial Optimal Transport](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05082.pdf)
* [E3V-K5: An Authentic Benchmark for Redefining Video-Based Energy Expenditure Estimation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05216.pdf)<br>:star:[code](https://github.com/zsxm1998/E3V)
* [Understanding Physical Dynamics with Counterfactual World Modeling](https://arxiv.org/abs/2312.06721)<br>:house:[project](https://neuroailab.github.io/cwm-physics/)
* [4Diff: 3D-Aware Diffusion Model for Third-to-First Viewpoint Translation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03536.pdf)<br>:house:[project](https://klauscc.github.io/4diff)
* [Revisiting Calibration of Wide-Angle Radially Symmetric Cameras](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05302.pdf)<br>:star:[code](http://github.com/andreadalcin/RadiallySymmetricCalib)相机校准
* [STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians](https://arxiv.org/abs/2403.14939)<br>:star:[code](https://github.com/zeng-yifei/STAG4D)
* [Synchronization of Projective Transformations](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05369.pdf)
* [UniCal: Unified Neural Sensor Calibration](https://arxiv.org/abs/2409.18953)<br>:house:[project](https://waabi.ai/unical/)
* [Rawformer: Unpaired Raw-to-Raw Translation for Learnable Camera ISPs](https://arxiv.org/abs/2404.10700)<br>:star:[code](https://github.com/gosha20777/rawformer)
* [Robust Incremental Structure-from-Motion with Hybrid Features](https://arxiv.org/abs/2409.19811)
* [Any2Point: Empowering Any-modality Transformers for Efficient 3D Understanding](https://arxiv.org/abs/2404.07989)<br>:star:[code](https://github.com/Ivan-Tang-3D/Any2Point)
* [CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization](https://arxiv.org/abs/2311.18159)<br>:star:[code](https://github.com/UCDvision/compact3d)
* [Multiscale Graph Texture Network](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04702.pdf)<br>:star:[code](https://github.com/RavishankarEvani/GTN)
* [Enhancing Optimization Robustness in 1-bit Neural Networks through Stochastic Sign Descent](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04573.pdf)<br>:star:[code](https://github.com/GreenBitAI/bitorch-engine)
* [Domain Reduction Strategy for Non-Line-of-Sight Imaging](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04491.pdf)<br>:star:[code](https://github.com/hyunbo9/domain-reduction-strategy)
* [BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation](https://arxiv.org/abs/2408.05926)<br>:star:[code](https://github.com/hee-suk-yoon/bi-mdrg)
* [Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data](https://arxiv.org/abs/2402.05892)<br>:star:[code](https://github.com/jacklishufan/mamba-nd)
* [Model Stock: All we need is just a few fine-tuned models](https://arxiv.org/abs/2403.19522)<br>:star:[code](https://github.com/naver-ai/model-stock)
* [DreamStruct: Understanding Slides and User Interfaces via Synthetic Data Generation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03543.pdf)
* [DetailSemNet: Elevating Signature Verification through Detail-Semantic Integration](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03694.pdf)<br>:star:[code](https://github.com/nycu-acm/DetailSemNet_OSV)
* [SLIM: Spuriousness Mitigation with Minimal Human Annotations](http://arxiv.org/abs/2407.05594v1)<br>:star:[code](https://github.com/xiweix/SLIM.git/)
* [Scaling Backwards: Minimal Synthetic Pre-training?](http://arxiv.org/abs/2408.00677v1)<br>:star:[code](https://github.com/SUPER-TADORY/1p-frac)
* [Forecasting Future Videos from Novel Views via Disentangled 3D Scene Representation](http://arxiv.org/abs/2407.21450v1)<br>:star:[code](https://skrya.github.io/projects/ffn-dsr/)
* [On the Evaluation Consistency of Attribution-based Explanations](http://arxiv.org/abs/2407.19471v1)<br>:star:[code](https://github.com/TreeThree-R/Meta-Rank)
* [GTP-4o: Modality-prompted Heterogeneous Graph Learning for Omni-modal Biomedical Representation](http://arxiv.org/abs/2407.05540v1)<br>:star:[code](https://gtp-4-o.github.io/)
* [OvSW: Overcoming Silent Weights for Accurate Binary Neural Networks](http://arxiv.org/abs/2407.05257v1)<br>:star:[code](https://github.com/JingyangXiang/OvSW)
* [SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding](http://arxiv.org/abs/2407.05118v1)<br>:star:[code](https://github.com/zxccade/SHINE)
* [Robust Multimodal Learning via Representation Decoupling](http://arxiv.org/abs/2407.04458v1)
* [A High-quality Robust Diffusion Framework for Corrupted Dataset]
* [ReGround: Improving Textual and Spatial Grounding at No Cost](https://arxiv.org/abs/2403.13589)<br>:house:[project](https://re-ground.github.io/)
* [ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware Diffusion Guidance](https://arxiv.org/abs/2403.12409)<br>:house:[project](https://cyw-3d.github.io/ComboVerse/)
* [WHAC: World-grounded Humans and Cameras](https://arxiv.org/abs/2403.12959)<br>:house:[project](https://wqyin.github.io/projects/WHAC/)
* [Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and VisualAnalysis Strategy](http://arxiv.org/abs/2408.12086v1)<br>:star:[code](https://github.com/lyu-yx/ACUMEN)
* [Neural Metamorphosis](https://arxiv.org/abs/2410.11878)<br>:house:[project](https://adamdad.github.io/neumeta/)
* [Light-in-Flight for a World-in-Motion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05009.pdf)
* [Learning with Unmasked Tokens Drives Stronger Vision Learners](https://arxiv.org/abs/2310.13593)<br>:star:[code](https://github.com/naver-ai/lut)
* [PSALM: Pixelwise Segmentation with Large Multi-modal Model](https://arxiv.org/abs/2403.14598)<br>:star:[code](https://github.com/zamling/PSALM)
* [InsMapper: Exploring Inner-instance Information for Vectorized HD Mapping](https://arxiv.org/abs/2308.08543)<br>:star:[code](https://github.com/TonyXuQAQ/InsMapper)
* [The All-Seeing Project V2: Towards General Relation Comprehension of the Open World](https://arxiv.org/abs/2402.19474)<br>:star:[code](https://github.com/OpenGVLab/all-seeing)
* [Refine, Discriminate and Align: Stealing Encoders via Sample-Wise Prototypes and Multi-Relational Extraction](https://arxiv.org/abs/2312.00855)
* [Multi-Task Domain Adaptation for Language Grounding with 3D Objects](https://arxiv.org/abs/2407.02846)<br>:house:[project](https://sites.google.com/view/da4lg)
* [QueryCDR: Query-based Controllable Distortion Rectification Network for Fisheye Images](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02355.pdf)<br>:star:[code](https://github.com/PbGuo/QueryCDR)鱼眼图像
* [BAMM: Bidirectional Autoregressive Motion Model](https://arxiv.org/abs/2403.19435)<br>:house:[project](https://exitudio.github.io/BAMM-page/)
* [Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02249.pdf)
* [Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics](https://arxiv.org/abs/2408.02672)
* [RPBG: Towards Robust Neural Point-based Graphics in the Wild](https://arxiv.org/abs/2405.05663)<br>:star:[code](https://github.com/QT-Zhu/RPBG)
* [Memory-Efficient Fine-Tuning for Quantized Diffusion Model](https://arxiv.org/abs/2401.04339)<br>:star:[code](https://github.com/ugonfor/TuneQDM)
* [Towards Architecture-Agnostic Untrained Networks Priors for Image Reconstruction with Frequency Regularization](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02230.pdf)<br>:star:[code](https://github.com/YilinLiu97/Untrained-Recon)
* [Test-time Model Adaptation for Image Reconstruction Using Self-supervised Adaptive Layers](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05391.pdf)图像重建
* [Unveiling Privacy Risks in Stochastic Neural Networks Training: Effective Image Reconstruction from Gradients](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04439.pdf)图像重建
* [CrossScore: A Multi-View Approach to Image Evaluation and Scoring](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01470.pdf)
* [ADMap: Anti-disturbance Framework for Vectorized HD Map Construction](https://arxiv.org/abs/2401.13172)
* [GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting](https://arxiv.org/abs/2403.08551)<br>:star:[code](https://github.com/Xinjie-Q/GaussianImage)
* [PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01465.pdf)<br>:star:[code](https://github.com/THU-MIG/PYRA)
* [ReALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01610.pdf)<br>:star:[code](https://github.com/snumprlab/realfred)
* [DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01652.pdf)<br>:star:[code](https://github.com/IRMVLab/DVLO)
* [Real-data-driven 2000 FPS Color Video from Mosaicked Chromatic Spikes](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01910.pdf)<br>🤗[huggingface](https://huggingface.co/datasets/YOUSIKI/chromatic-spikes)
* [RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF](https://arxiv.org/abs/2403.11909)<br>:house:[project](https://sib1.github.io/projects/roguenerf/)
* [LaRa: Efficient Large-Baseline Radiance Fields](https://arxiv.org/abs/2407.04699)<br>:star:[code](https://github.com/autonomousvision/LaRa)
* [Category Adaptation Meets Projected Distillation in Generalized Continual Category Discovery](https://arxiv.org/abs/2308.12112)<br>:star:[code](https://github.com/grypesc/CAMP)
* [Bi-TTA: Bidirectional Test-Time Adapter for Remote Physiological Measurement](https://arxiv.org/abs/2409.17316)<br>:house:[project](https://bi-tta.github.io/)
* [ELSE: Efficient Deep Neural Network Inference through Line-based Sparsity Exploration](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01791.pdf)
* [Open-World Dynamic Prompt and Continual Visual Representation Learning](http://arxiv.org/abs/2409.05312v1)
* [GeoCalib: Learning Single-image Calibration with Geometric Optimization](http://arxiv.org/abs/2409.06704v1)<br>:star:[code](https://github.com/cvg/GeoCalib)
* [LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation](http://arxiv.org/abs/2409.06703v1)<br>:star:[code](https://archana1998.github.io/leia/)
* [Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences](http://arxiv.org/abs/2409.06683v1)
* [Weakly-supervised Camera Localization by Ground-to-satellite Image Registration](http://arxiv.org/abs/2409.06471v1)
* [Learning Neural Volumetric Pose Features for Camera Localization](https://arxiv.org/abs/2403.12800)<br>:house:[project](https://gujiaqivadin.github.io/posemap/)
* [DECOLLAGE: 3D Detailization by Controllable, Localized, and Learned Geometry Enhancement](http://arxiv.org/abs/2409.06129v1)<br>:star:[code](https://qiminchen.github.io/decollage/)
* [Event-based Mosaicing Bundle Adjustment](http://arxiv.org/abs/2409.07365v1)<br>:star:[code](https://github.com/tub-rip/emba)
* [Reprojection Errors as Prompts for Efficient Scene Coordinate Regression](http://arxiv.org/abs/2409.04178v1)
* [Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor](http://arxiv.org/abs/2409.08277v1)
* [AMEGO: Active Memory from long EGOcentric videos](http://arxiv.org/abs/2409.10917v1)<br>:star:[code](https://gabrielegoletto.github.io/AMEGO/)
* [Vista3D: Unravel the 3D Darkside of a Single Image](http://arxiv.org/abs/2409.12193v1)<br>:star:[code](https://github.com/florinshen/Vista3D)
* [Agglomerative Token Clustering](http://arxiv.org/abs/2409.11923v1)<br>:house:[project](https://vap.aau.dk/atc/)
* [Formula-Supervised Visual-Geometric Pre-training](http://arxiv.org/abs/2409.13535v1)<br>:star:[code](https://ryosuke-yamada.github.io/fdsl-fsvgp/)
* [Interpretability-Guided Test-Time Adversarial Defense](http://arxiv.org/abs/2409.15190v1)<br>:star:[code](https://lilywenglab.github.io/Interpretability-Guided-Defense/)
* [Towards Model-Agnostic Dataset Condensation by Heterogeneous Models](http://arxiv.org/abs/2409.14538v1)<br>:star:[code](https://github.com/KHU-AGI/HMDC)
* [MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views](http://arxiv.org/abs/2409.14316v1)<br>:star:[code](https://zezeaaa.github.io/projects/MVPGS/)
* [Intrinsic Single-Image HDR Reconstruction](http://arxiv.org/abs/2409.13803v1)
* [Disentangled Generation and Aggregation for Robust Radiance Fields](http://arxiv.org/abs/2409.15715v1)<br>:star:[code](https://gaohchen.github.io/DiGARR/)
* [Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection](http://arxiv.org/abs/2409.15557v1)
* [Commonly Interesting Images](http://arxiv.org/abs/2409.16736v1)
* [Sequential Representation Learning via Static-Dynamic Conditional Disentanglement](https://arxiv.org/abs/2408.05599)
* [QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer](https://arxiv.org/abs/2404.07988)<br>:star:[code](https://github.com/Meowuu7/QuasiSim)<br>:house:[project](https://meowuu7.github.io/QuasiSim/)
* [Dataset Distillation by Automatic Training Trajectories](http://arxiv.org/abs/2407.14245v1)
* [Neural Graphics Texture Compression Supporting Random Acces](https://arxiv.org/abs/2407.00021)  
* [Rasterized Edge Gradients: Handling Discontinuities Differentiably](https://arxiv.org/abs/2405.02508)
* [LookupViT: Compressing visual information to a limited number of tokens](http://arxiv.org/abs/2407.12753v1)
* [PromptCCD: Learning Gaussian Mixture Prompt Pool for Continual Category Discovery](http://arxiv.org/abs/2407.19001v1)<br>:star:[code](https://visual-ai.github.io/promptccd)
* [Bridging the Gap: Studio-like Avatar Creation from a Monocular Phone Capture](https://arxiv.org/abs/2407.19593)<br>:house:[project](http://shahrukhathar.github.io/2024/07/22/Bridging.html)
* [Generating 3D House Wireframes with Semantics](http://arxiv.org/abs/2407.12267v1)<br>:star:[code](https://github.com/3d-house-wireframe/3d-house-wireframe-dataset)<br>:house:[project](https://vcc.tech/research/2024/3DWire)
* [Flying with Photons: Rendering Novel Views of Propagating Light](https://arxiv.org/abs/2404.06493)<br>:star:[code](https://github.com/anaghmalik/FlyingWithPhotons)
* [Deep Nets with Subsampling Layers Unwittingly Discard Useful Activations at Test-Time](https://arxiv.org/abs/2410.01083)<br>:star:[code](https://github.com/ca-joe-yang/discard-in-subsampling)
* [MobileNetV4: Universal Models for the Mobile Ecosystem](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05647.pdf)
* [Gravity-aligned Rotation Averaging with Circular Regression](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05651.pdf)<br>:star:[code](https://github.com/colmap/glomap)
* [Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models](https://arxiv.org/abs/2311.12092)<br>:house:[project](https://sliders.baulab.info/)
* [HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts](https://arxiv.org/abs/2404.17507)
* [DoughNet: A Visual Predictive Model for Topological Manipulation of Deformable Objects](https://arxiv.org/abs/2404.12524)<br>:house:[project](https://dough-net.github.io/)
* [TrajPrompt: Aligning Color Trajectory with Vision-Language Representations](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05758.pdf)<br>:star:[code](https://github.com/basiclab/TrajPrompt)
* [DomainFusion: Generalizing To Unseen Domains with Latent Diffusion Models](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05806.pdf)
* [Toward Tiny and High-quality Facial Makeup with Data Amplify Learning](https://arxiv.org/abs/2403.15033)<br>:star:[code](https://github.com/TinyBeauty)
* [Multi-Label Cluster Discrimination for Visual Representation Learning](https://arxiv.org/abs/2407.17331)
* [Preventing Catastrophic Overfitting in Fast Adversarial Training: A Bi-level Optimization Perspective](https://arxiv.org/abs/2407.12443)
* [MemBN: Robust Test-Time Adaptation via Batch Norm with Statistics Memory](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04143.pdf)
* [SeiT++: Masked Token Modeling Improves Storage-efficient Training](https://arxiv.org/abs/2312.10105)<br>:star:[code](https://github.com/naver-ai/seit)
* [MagicEraser: Erasing Any Objects via Semantics-Aware Control](https://arxiv.org/abs/2410.10207)
* [Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation](https://arxiv.org/abs/2403.06461)<br>:house:[project](https://sites.google.com/view/eccv24-latte)
* [A Cephalometric Landmark Regression Method based on Dual-encoder for High-resolution X-ray Image](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04167.pdf)<br>:star:[code](https://github.com/huang229/D-CeLR)
* [Resilience of Entropy Model in Distributed Neural Networks](https://arxiv.org/abs/2403.00942)<br>:star:[code](https://github.com/Restuccia-Group/EntropyR)
* [G2fR: Frequency Regularization in Grid-based Feature Encoding Neural Radiance Fields](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03259.pdf)
* [GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image](https://arxiv.org/abs/2403.12013)<br>:house:[project](https://fuxiao0719.github.io/projects/geowizard/)
* [MotionChain: Conversational Motion Controllers via Multimodal Prompts](https://arxiv.org/abs/2404.01700)
* [MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion](https://www.arxiv.org/abs/2409.10473)<br>:house:[project](https://lehongwu.github.io/ECCV24MacDiff/)
* [Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models](https://arxiv.org/abs/2405.01531)<br>:star:[code](https://github.com/ExplainableML/concept_realignment)
* [Brain Netflix: Scaling Data to Reconstruct Videos from Brain Signals](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03816.pdf)
* [Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics](https://arxiv.org/abs/2408.02672)
* [Tensorial template matching for fast cross-correlation with rotations and its application for tomography](https://arxiv.org/abs/2408.02398)
* [SelfGeo: Self-supervised and Geodesic-consistent Estimation of Keypoints on Deformable Shapes](https://arxiv.org/abs/2408.02291)
* [Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts](https://arxiv.org/abs/2408.02265)
* [Motion and Structure from Event-based Normal Flow](http://arxiv.org/abs/2407.12239v1)
* [SENC: Handling Self-collision in Neural Cloth Simulation](http://arxiv.org/abs/2407.12479v1)
* [Distribution Alignment for Fully Test-Time Adaptation with Dynamic Online Data Streams](http://arxiv.org/abs/2407.12128v1)
* [Animate Your Motion: Turning Still Images into Dynamic Videos](https://arxiv.org/pdf/2403.10179)<br>:house:[project](https://mingxiao-li.github.io/smcd/)
* [Gaussian Splatting on the Move:Blur and Rolling Shutter Compensation for Natural Camera Motion](https://arxiv.org/pdf/2403.13327)<br>:star:[code](https://github.com/SpectacularAI/3dgs-deblur)<br>:house:[project](https://spectacularai.github.io/3dgs-deblur/)
* [Relightable Neural Actor with Intrinsic Decomposition and Pose Control](https://arxiv.org/abs/2312.11587)<br>:house:[project](https://people.mpi-inf.mpg.de/~dluvizon/relightable-neural-actor/)
* [Layer-Wise Relevance Propagation with Conservation Property for ResNet](https://arxiv.org/abs/2407.09115)<br>:house:[project](https://lrp-for-resnet-project-page-pvgmt.kinsta.page/)
* [Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance](https://arxiv.org/abs/2403.17377)<br>:house:[project](https://ku-cvlab.github.io/Perturbed-Attention-Guidance)
* [SparseSSP: 3D Subcellular Structure Prediction from Sparse-View Transmitted Light Images](http://arxiv.org/abs/2407.02159v1)
* [ViG-Bias: Visually Grounded Bias Discovery and Mitigation](http://arxiv.org/abs/2407.01996v1)
* [DOCCI: Descriptions of Connected and Contrasting Images](https://arxiv.org/pdf/2404.19753)<br>:house:[project](https://google.github.io/docci)
* [Geometry Fidelity for Spherical Images](http://arxiv.org/abs/2407.18207v1)
* [Efficient Inference of Vision Instruction-Following Models with Elastic Cache](http://arxiv.org/abs/2407.18121v1)<br>:star:[code](https://github.com/liuzuyan/ElasticCache)
* [AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild](http://arxiv.org/abs/2407.18034v1)
* [Mew: Multiplexed Immunofluorescence Image Analysis through an Efficient Multiplex Network](http://arxiv.org/abs/2407.17857v1)<br>:star:[code](https://github.com/UNITES-Lab/Mew)
* [Topology-Preserving Downsampling of Binary Images](http://arxiv.org/abs/2407.17786v1)
* [Quality Assured: Rethinking Annotation Strategies in Imaging AI](http://arxiv.org/abs/2407.17596v1)
* [Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models](http://arxiv.org/abs/2407.15408v1)<br>:house:[project](https://kfworks.com/CAR-WP/)
* [Data Collection-free Masked Video Modeling](http://arxiv.org/abs/2409.06665v1)
* [An Implicit Solution to Inverse Scattering Problems]
* [Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders](https://arxiv.org/abs/2403.17823)<br>:star:[code](https://github.com/alexandre-eymael/CropMAE)
* [Möbius Transform for Mitigating Perspective Distortions in Representation Learning](https://arxiv.org/abs/2405.02296)
* [Foster Adaptivity and Balance in Learning with Noisy Labels](http://arxiv.org/abs/2407.02778v1)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/SED)<br>无需先验知识即可高效解决深度学习中的噪声标签问题，让模型性能和鲁棒性大幅提升！
* [Solving Motion Planning Tasks with a Scalable Generative Model](http://arxiv.org/abs/2407.02797v1)<br>:star:[code](https://github.com/HorizonRobotics/GUMP/)
* [4D Contrastive Superflows are Dense 3D Representation Learners](http://arxiv.org/abs/2407.06190v1)<br>:star:[code](https://github.com/Xiangxu-0103/SuperFlow)
* [Learning to Complement and to Defer to Multiple Users](http://arxiv.org/abs/2407.07003v1)
* [Shedding More Light on Robust Classifiers under the lens of Energy-based Models](http://arxiv.org/abs/2407.06315v1)<br>:star:[code](http://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/)
* [TIP: Tabular-Image Pre-training for Multimodal Classification with Incomplete Data](http://arxiv.org/abs/2407.07582v1)<br>:star:[code](https://github.com/siyi-wind/TIP)
* [UMBRAE: Unified Multimodal Brain Decoding](https://arxiv.org/abs/2404.07202)<br>:star:[code](https://github.com/weihaox/UMBRAE)<br>:house:[project](https://weihaox.github.io/UMBRAE/)
* [Trainable Highly-expressive Activation Functions](http://arxiv.org/abs/2407.07564v1)<br>:star:[code](https://github.com/BGU-CS-VIL/DiTAC)
* [Controllable Navigation Instruction Generation with Chain of Thought Prompting](http://arxiv.org/abs/2407.07433v1)
* [Recursive Visual Programming](https://arxiv.org/abs/2312.02249)<br>:star:[code](https://github.com/para-lost/RVP)
* [Reshaping the Online Data Buffering and Organizing Mechanism for Continual Test-Time Adaptation](http://arxiv.org/abs/2407.09367v1)<br>:star:[code](https://github.com/z1358/OBAO)
* [Imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems](http://arxiv.org/abs/2407.09352v1)<br>:star:[code](https://luo-ziyuan.github.io/Imaging-Interiors)
* [The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable Model with Counterfactual Explanations](http://arxiv.org/abs/2409.12952v1)<br>:star:[code](https://github.com/trustinai/gdvaecode)<br>:house:[project](https://trustinai.github.io/gdvae/)
* [HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions]
* [DataDream: Few-shot Guided Dataset Generation](http://arxiv.org/abs/2407.10910v1)<br>:star:[code](https://github.com/ExplainableML/DataDream)
* [Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models](http://arxiv.org/abs/2407.10737v1)<br>:star:[code](https://github.com/wurining/Vi-ST)<br>:house:[project](https://eccv2024.ecva.net/Conferences/2024/AcceptedPapers)
* [Rethinking Data Bias: Dataset Copyright Protection via Embedding Class-wise Hidden Bias](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03084.pdf)<br>:star:[code](https://github.com/jjh6297/UndercoverBias)保护数据集版权
* [Towards Robust Event-based Networks for Nighttime via Unpaired Day-to-Night Event Translation](http://arxiv.org/abs/2407.10703v1)<br>:star:[code](https://github.com/jeongyh98/UDNET)
* [FRI-Net: Floorplan Reconstruction via Room-wise Implicit Representation](http://arxiv.org/abs/2407.10687v1)
* [Hierarchically Structured Neural Bones for Reconstructing Animatable Objects from Casual Videos](https://arxiv.org/abs/2408.00351)<br>:star:[code](https://github.com/subin6/HSNB)
* [Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse Problems](http://arxiv.org/abs/2407.10641v1)<br>:star:[code](https://github.com/HJ-harry/DDIP3D)
* [Pathformer3D: A 3D Scanpath Transformer for 360° Images](http://arxiv.org/abs/2407.10563v1)<br>:star:[code](https://github.com/lsztzp/Pathformer3D)
* [Kinetic Typography Diffusion Model](http://arxiv.org/abs/2407.10476v1)<br>:star:[code](https://seonmip.github.io/kinety)
* [PolyRoom: Room-aware Transformer for Floorplan Reconstruction](http://arxiv.org/abs/2407.10439v1)<br>:star:[code](https://github.com/3dv-casia/PolyRoom/)
* [Tree-D Fusion: Simulation-Ready Tree Dataset from Single Images with Diffusion Priors](http://arxiv.org/abs/2407.10330v1)
* [Multiscale Sliced Wasserstein Distances as Perceptual Color Difference Measures](http://arxiv.org/abs/2407.10181v1)<br>:star:[code](https://github.com/real-hjq/MS-SWD)
* [Augmented Neural Fine-Tuning for Efficient Backdoor Purification](http://arxiv.org/abs/2407.10052v1)
* [Improving Hyperbolic Representations via Gromov-Wasserstein Regularization](http://arxiv.org/abs/2407.10495v1)
* [Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion](http://arxiv.org/abs/2407.10373v1)<br>:star:[code](https://hechang25.github.io/MVSD)
* [Efficient Training with Denoised Neural Weights](http://arxiv.org/abs/2407.11966v1)<br>:star:[code](https://yifanfanfanfan.github.io/denoised-weights/)
* [SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint Alignment of Images](http://arxiv.org/abs/2407.11850v1)<br>:star:[code](https://bgu-cs-vil.github.io/SpaceJAM/)
* [Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery](http://arxiv.org/abs/2407.14499v1)<br>:star:[code](https://github.com/neuroexplicit-saar/discover-then-name)
* [Multi-modal Relation Distillation for Unified 3D Representation Learning](http://arxiv.org/abs/2407.14007v1)
* [Continual Learning for Remote Physiological Measurement: Minimize Forgetting and Simplify Inference](http://arxiv.org/abs/2407.13974v1)<br>:star:[code](https://github.com/MayYoY/rPPGDIL)
* [TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly](http://arxiv.org/abs/2407.15648v1)
* [SIGMA:Sinkhorn-Guided Masked Video Modeling](http://arxiv.org/abs/2407.15447v1)<br>:star:[code](https://quva-lab.github.io/SIGMA)
* [Attention Beats Linear for Fast Implicit Neural Representation Generation](http://arxiv.org/abs/2407.15355v1)<br>:star:[code](https://github.com/Roninton/ANR)
* [Text2Place: Affordance-aware Text Guided Human Placement](http://arxiv.org/abs/2407.15446v1)<br>:star:[code](https://rishubhpar.github.io/Text2Place/)
* [RoadPainter: Points Are Ideal Navigators for Topology transformER](http://arxiv.org/abs/2407.15349v1)
* [STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay](http://arxiv.org/abs/2407.15773v1)<br>:star:[code](https://github.com/yuyongcan/STAMP)
* [Differentiable Convex Polyhedra Optimization from Multi-view Images](http://arxiv.org/abs/2407.15686v1)<br>:star:[code](https://github.com/kimren227/DiffConvex)
* [A Diffusion Model for Simulation Ready Coronary Anatomy with Morpho-skeletal Control](http://arxiv.org/abs/2407.15631v1)
* [Power Variable Projection for Initialization-Free Large-Scale Bundle Adjustment](https://arxiv.org/abs/2405.05079)
* [Multi-label Cluster Discrimination for Visual Representation Learning](http://arxiv.org/abs/2407.17331v1)
* [SINDER: Repairing the Singular Defects of DINOv2](http://arxiv.org/abs/2407.16826v1)<br>:star:[code](https://github.com/haoqiwang/sinder)
* [SHIC: Shape-Image Correspondences with no Keypoint Supervision](http://arxiv.org/abs/2407.18907v1)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/shic/)
* [Semicalibrated Relative Pose from an Affine Correspondence and Monodepth](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05645.pdf)相对位姿半校准
* [Scalable Group Choreography via Variational Phase Manifold Learning](http://arxiv.org/abs/2407.18839v1)
* [Deep Companion Learning: Enhancing Generalization Through Historical Consistency](http://arxiv.org/abs/2407.18821v1)
* [Revisit Event Generation Model: Self-Supervised Learning of Event-to-Video Reconstruction with Implicit Neural Representations](http://arxiv.org/abs/2407.18500v1)<br>:star:[code](https://vlislab22.github.io/EvINR/)
* [Neural Surface Detection for Unsigned Distance Fields](http://arxiv.org/abs/2407.18381v1)
* [Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas](http://arxiv.org/abs/2408.15660v1)<br>:star:[code](https://github.com/aimagelab/MAD)
* [Platypus: A Generalized Specialist Model for Reading Text in Various Forms](http://arxiv.org/abs/2408.14805v1)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/Platypus)
* [RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images](https://arxiv.org/abs/2408.14802)<br>:star:[code](https://github.com/cuiziteng/ECCV_RAW_Adapter)
* [Learning Differentially Private Diffusion Models via Stochastic Adversarial Distillation](https://arxiv.org/abs/2408.14738)
* [SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery](http://arxiv.org/abs/2408.14371v1)<br>:star:[code](https://github.com/SarahRastegar/SelEx)
* [Affine steerers for structured keypoint description](https://arxiv.org/abs/2408.14186)
* [SeA: Semantic Adversarial Augmentation for Last Layer Features from Unsupervised Representation Learning](http://arxiv.org/abs/2408.13351v1)<br>:star:[code](https://github.com/idstcv/SeA)
* [MMBench: Is Your Multi-modal Model an All-around Player?](https://arxiv.org/abs/2307.06281)<br>:star:[code](https://github.com/open-compass/VLMEvalKit)<br>:house:[project](https://opencompass.org.cn/mmbench)
* [DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs](https://arxiv.org/abs/2403.19588)<br>:star:[code](https://github.com/naver-ai/rdnet)
* [3DFG-PIFu: 3D Feature Grids for Human Digitization from Sparse Views](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03433.pdf)
* [AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation](https://arxiv.org/abs/2406.01194)
* [PreLAR: World Model Pre-training with Learnable Action Representation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03363.pdf)<br>:star:[code](https://github.com/zhanglixuan0720/PreLAR)
* [Dataset Enhancement with Instance-Level Augmentations](https://arxiv.org/abs/2406.08249)<br>:star:[code](https://github.com/KupynOrest/instance_augmentation)
* [Non-parametric Sensor Noise Modeling and Synthesis](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03438.pdf)
* [Stripe Observation Guided Inference Cost-free Attention Mechanism](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03451.pdf)<br>:star:[code](https://github.com/zhongshsh/ASR)
* [Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation](https://arxiv.org/abs/2310.07506)
* [Object-Aware NIR-to-Visible Translation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03338.pdf)<br>:star:[code](https://github.com/Yiiclass/Sherry)<br>:sunflower:[dataset](https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy9lOTc2YWNjYTdiOWZjZDFmL0VpRHlibTZ0aF9kQ21mN3YwSERNLWhZQmp1SGNPc1ZrakNhMjA2N3BnemFVeFE%5FZT1lVmlzVlg&id=E976ACCA7B9FCD1F%21s6e6ef22087ad42f799feefd070ccfa16&cid=E976ACCA7B9FCD1F)Low-level Vision


<a name="0"/>

## 2020 年论文分类汇总戳这里
↘️[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers) 
↘️[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

<a name="00"/>

## 2021 年论文分类汇总戳这里
↘️[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
↘️[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

<a name="000"/>

## 2022 年论文分类汇总戳这里
↘️[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers/blob/main/README.md)
↘️[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)
↘️[ECCV-2022-Papers](https://github.com/52CV/ECCV-2022-Papers/blob/main/README.md)

### 扫码CV君微信(注明：CVPR)入微信交流群：
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)# ECCV-2024-Papers